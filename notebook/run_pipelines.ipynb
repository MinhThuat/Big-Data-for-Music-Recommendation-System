{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkConf\n",
    "from contextlib import contextmanager\n",
    "import pyspark.sql\n",
    "\n",
    "\"\"\" Context manager for creating Spark Session. \"\"\"\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    #declare sparkconf\n",
    "    conf = SparkConf()\n",
    "\n",
    "    #set config\n",
    "    conf = conf.setAppName(appName) \\\n",
    "               .setMaster(master) \\\n",
    "               .set(\"spark.executor.memory\", \"4g\") \\\n",
    "               .set(\"spark.executor.cores\", \"2\") \\\n",
    "               .set(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "               .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "               .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\") \\\n",
    "            #    .set(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4\")\n",
    "    \n",
    "    #create Spark Session\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully created Spark Session with app name: {appName} and master: {master}!\")\n",
    "\n",
    "    #yield spark\n",
    "    try:\n",
    "        yield spark\n",
    "\n",
    "    finally:\n",
    "        #must stop Spark Session\n",
    "        spark.stop()\n",
    "        print(\"Successfully stopped Spark Session!\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from mongoDB. \"\"\"\n",
    "def read_mongoDB(spark: SparkSession, database_name: str, collection_name: str, query: dict = None,\n",
    "                 schema: StructType = None, username: str = 'huynhthuan', password: str = 'password', \n",
    "                 host: str = 'mongo', port: str = 27017) -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if query is not None and not isinstance(query, dict):\n",
    "        raise TypeError(\"query must be a dict!\")\n",
    "    \n",
    "    if schema is not None and not isinstance(schema, StructType):\n",
    "        raise TypeError(\"schema must be a StructType!\")\n",
    "    \n",
    "    #uri mongoDB \n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "\n",
    "    print(f\"Starting to read data from database '{database_name}' and collection '{collection_name}'...\")\n",
    "  \n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format('mongodb') \\\n",
    "                         .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                         .option('header', 'true')\n",
    "        \n",
    "        data = data.schema(schema).load() if schema is not None else data.load()\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from mongoDB: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from HDFS. \"\"\"\n",
    "def read_HDFS(spark: SparkSession, HDFS_dir: str, file_type: str) -> pyspark.sql.DataFrame:\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    #set HDFS path\n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{HDFS_dir}\"\n",
    "\n",
    "    print(f\"Starting to read data from {HDFS_path}...\")\n",
    "\n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format(file_type).option('header', 'true').load(HDFS_path)\n",
    "        #return data\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from HDFS: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Write data into HDFS. \"\"\"\n",
    "def write_HDFS(spark: SparkSession, data: pyspark.sql.DataFrame, direct: str, \n",
    "               file_type: str, mode: str = 'overwrite', partition: str = None):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "\n",
    "    #set HDFS path  \n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{direct}\"\n",
    "    table_name = direct.split('/')[-1]\n",
    "\n",
    "    print(f\"Starting to upload '{table_name}' into {HDFS_path}...\")\n",
    "    \n",
    "    #write data\n",
    "    try:\n",
    "        if partition is not None:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode(mode) \\\n",
    "                      .partitionBy('Execution_date') \\\n",
    "                      .save(HDFS_path)\n",
    "        else:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode(mode) \\\n",
    "                      .save(HDFS_path)\n",
    "        \n",
    "        print(f\"Successfully uploaded '{table_name}' into HDFS.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "\n",
    "\"\"\" Write data into SnowFlake Data Warehouse. \"\"\"\n",
    "def write_SnowFlake(spark: SparkSession, data: pyspark.sql.DataFrame, table_name: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    \n",
    "    snowflake_connection_options = {\n",
    "        \"sfURL\": \"https://sl70006.southeast-asia.azure.snowflakecomputing.com\",\n",
    "        \"sfUser\": \"HUYNHTHUAN\", \n",
    "        \"sfPassword\": \"Thuan123456\",\n",
    "        \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "        \"sfDatabase\": \"SPOTIFY_MUSIC_DB\" \n",
    "    }\n",
    "\n",
    "    print(f\"Starting to upload {table_name.split('.')[-1]} into SnowFlake...\")\n",
    "    try:\n",
    "        data.write.format(\"snowflake\") \\\n",
    "                .options(**snowflake_connection_options) \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .mode('overwrite') \\\n",
    "                .save()\n",
    "        print(f\"Successfully uploaded '{table_name}' into SnowFlake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Initial load task starts! -------------------------------\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9cdb7693-043a-44ff-96bb-de2add6e49b7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 2083ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9cdb7693-043a-44ff-96bb-de2add6e49b7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n",
      "24/12/16 13:43:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/16 13:43:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/16 13:43:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: init_load and master: local!\n",
      "Starting load csv files into MongoDB...\n",
      "An error occured while loading data: An error occurred while calling o77.save.\n",
      ": com.mongodb.MongoTimeoutException: Timed out while waiting for a server that matches WritableServerSelector. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongo:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: mongo}, caused by {java.net.UnknownHostException: mongo}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.createAndLogTimeoutException(BaseCluster.java:392)\n",
      "\tat com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:148)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.selectServer(SingleServerCluster.java:46)\n",
      "\tat com.mongodb.internal.binding.ClusterBinding.getWriteConnectionSource(ClusterBinding.java:126)\n",
      "\tat com.mongodb.client.internal.ClientSessionBinding.getConnectionSource(ClientSessionBinding.java:128)\n",
      "\tat com.mongodb.client.internal.ClientSessionBinding.getWriteConnectionSource(ClientSessionBinding.java:102)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.withConnection(SyncOperationHelper.java:103)\n",
      "\tat com.mongodb.internal.operation.DropCollectionOperation.execute(DropCollectionOperation.java:95)\n",
      "\tat com.mongodb.internal.operation.DropCollectionOperation.execute(DropCollectionOperation.java:61)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:173)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeDrop(MongoCollectionImpl.java:865)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.drop(MongoCollectionImpl.java:797)\n",
      "\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.lambda$doWithCollection$4(AbstractMongoConfig.java:223)\n",
      "\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withCollection(AbstractMongoConfig.java:210)\n",
      "\tat com.mongodb.spark.sql.connector.config.WriteConfig.withCollection(WriteConfig.java:39)\n",
      "\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.doWithCollection(AbstractMongoConfig.java:222)\n",
      "\tat com.mongodb.spark.sql.connector.config.WriteConfig.doWithCollection(WriteConfig.java:39)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoBatchWrite.createBatchWriterFactory(MongoBatchWrite.java:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:396)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:382)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Successfully stopped Spark Session!\n",
      "------------------------------- Initial load task finished! -------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\"\"\" Load all csv files into mongoDB.\"\"\"\n",
    "def initial_load(Execution_date: str):\n",
    "    with get_sparkSession(appName = \"init_load\") as spark:\n",
    "        #uri\n",
    "        uri_artist_name = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_name_collection?authSource=admin\"\n",
    "        uri_artist = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_collection?authSource=admin\"\n",
    "        uri_album = \"mongodb://huynhthuan:password@mongo:27017/music_database.album_collection?authSource=admin\"\n",
    "        uri_track = \"mongodb://huynhthuan:password@mongo:27017/music_database.track_collection?authSource=admin\"\n",
    "        uri_trackfeature = \"mongodb://huynhthuan:password@mongo:27017/music_database.trackfeature_collection?authSource=admin\"\n",
    "\n",
    "        # read\n",
    "        df_ArtistName = spark.read.option('header', 'true').csv(\"/opt/data/ArtistName.csv\")\n",
    "        df_ArtistName = df_ArtistName.withColumn('Execution_date', lit(Execution_date))\n",
    "        \n",
    "        df_Artist = spark.read.option('header', 'true').csv(\"/opt/data/Artist.csv\")\n",
    "        df_Artist = df_Artist.withColumn('Execution_date', lit(Execution_date))\n",
    "\n",
    "        df_Album = spark.read.option('header', 'true').csv(\"/opt/data/Album.csv\")\n",
    "        df_Album = df_Album.withColumn('Execution_date', lit(Execution_date))\n",
    "\n",
    "        df_Track = spark.read.option('header', 'true').csv(\"/opt/data/Track.csv\")\n",
    "        df_Track = df_Track.withColumn('Execution_date', lit(Execution_date))\n",
    "        \n",
    "        df_TrackFeature = spark.read.option('header', 'true').csv(\"/opt/data/TrackFeature.csv\")\n",
    "        df_TrackFeature = df_TrackFeature.withColumn('Execution_date', lit(Execution_date))\n",
    "\n",
    "        #write\n",
    "        try:\n",
    "            print(\"Starting load csv files into MongoDB...\")\n",
    "            df_ArtistName.write.format('mongoDB') \\\n",
    "                            .option(\"spark.mongodb.write.connection.uri\", uri_artist_name) \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .save()\n",
    "            \n",
    "            df_Artist.write.format('mongoDB') \\\n",
    "                        .option(\"spark.mongodb.write.connection.uri\", uri_artist) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save()\n",
    "            \n",
    "            df_Album.write.format('mongoDB') \\\n",
    "                        .option(\"spark.mongodb.write.connection.uri\", uri_album) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save()\n",
    "            \n",
    "            df_Track.write.format('mongoDB') \\\n",
    "                        .option(\"spark.mongodb.write.connection.uri\", uri_track) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save()\n",
    "            \n",
    "            df_TrackFeature.write.format('mongoDB') \\\n",
    "                                .option(\"spark.mongodb.write.connection.uri\", uri_trackfeature) \\\n",
    "                                .mode(\"overwrite\") \\\n",
    "                                .save()\n",
    "            print(\"Successfully uploaded data into mongoDB.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured while loading data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"------------------------------- Initial load task starts! -------------------------------\")\n",
    "    initial_load(\"01-12-2004\")\n",
    "    print(\"------------------------------- Initial load task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DateType, FloatType\n",
    "\n",
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',      StringType(), True),\n",
    "                     StructField('Artist_Name',    StringType(), True),\n",
    "                     StructField('Genres',         ArrayType(StringType(), True), True),\n",
    "                     StructField('Followers',      IntegerType(), True),\n",
    "                     StructField('Popularity',     IntegerType(), True),\n",
    "                     StructField('Artist_Image',   StringType(), True),\n",
    "                     StructField('Artist_Type',    StringType(), True),\n",
    "                     StructField('External_Url',   StringType(), True),\n",
    "                     StructField('Href',           StringType(), True),\n",
    "                     StructField('Artist_Uri',     StringType(), True),\n",
    "                     StructField('Execution_date', DateType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)\n",
    "    \n",
    "    \"\"\" Album schema. \"\"\"\n",
    "    album_schema = [StructField('Artist',               StringType(), True),\n",
    "                    StructField('Artist_ID',            StringType(), True),\n",
    "                    StructField('Album_ID',             StringType(), True),\n",
    "                    StructField('Name',                 StringType(), True),\n",
    "                    StructField('Type',                 StringType(), True),\n",
    "                    StructField('Genres',               ArrayType(StringType(), True), True),\n",
    "                    StructField('Label',                StringType(), True),\n",
    "                    StructField('Popularity',           IntegerType(), True),\n",
    "                    StructField('Available_Markets',    StringType(), True),\n",
    "                    StructField('Release_Date',         DateType(), True),\n",
    "                    StructField('ReleaseDatePrecision', StringType(), True),\n",
    "                    StructField('TotalTracks',          IntegerType(), True),\n",
    "                    StructField('Copyrights',           StringType(), True),\n",
    "                    StructField('Restrictions',         StringType(), True),\n",
    "                    StructField('External_URL',         StringType(), True),\n",
    "                    StructField('Href',                 StringType(), True),\n",
    "                    StructField('Image',                StringType(), True),\n",
    "                    StructField('Uri',                  StringType(), True),\n",
    "                    StructField('Execution_date',       DateType(), True)]\n",
    "    #Applying struct type\n",
    "    album_schema = StructType(album_schema)\n",
    "\n",
    "    \"\"\" Track schema. \"\"\"\n",
    "    track_schema = [StructField(\"Artists\",          StringType(), True),\n",
    "                    StructField(\"Album_ID\",         StringType(), True),\n",
    "                    StructField(\"Album_Name\",       StringType(), True),\n",
    "                    StructField(\"Track_ID\",         StringType(), True),\n",
    "                    StructField(\"Name\",             StringType(), True),\n",
    "                    StructField(\"Track_Number\",     IntegerType(), True),\n",
    "                    StructField(\"Type\",             StringType(), True),\n",
    "                    StructField(\"AvailableMarkets\", StringType(), True),\n",
    "                    StructField(\"Disc_Number\",      IntegerType(), True),\n",
    "                    StructField(\"Duration_ms\",      IntegerType(), True),\n",
    "                    StructField(\"Explicit\",         StringType(), True),\n",
    "                    StructField(\"External_urls\",    StringType(), True),\n",
    "                    StructField(\"Href\",             StringType(), True),\n",
    "                    StructField(\"Restrictions\",     StringType(), True),\n",
    "                    StructField(\"Preview_url\",      StringType(), True),\n",
    "                    StructField(\"Uri\",              StringType(), True),\n",
    "                    StructField(\"Is_Local\",         StringType(), True),\n",
    "                    StructField('Execution_date',   StringType(), True)]\n",
    "    #Applying struct type\n",
    "    track_schema = StructType(track_schema)\n",
    "    \n",
    "    \"\"\" TrackFeature schema. \"\"\"\n",
    "    trackfeature_schema = [StructField(\"Track_ID\",         StringType(), True),\n",
    "                           StructField(\"Danceability\",     FloatType(), True),\n",
    "                           StructField(\"Energy\",           FloatType(), True),\n",
    "                           StructField(\"Key\",              IntegerType(), True),\n",
    "                           StructField(\"Loudness\",         FloatType(), True),\n",
    "                           StructField(\"Mode\",             IntegerType(), True),\n",
    "                           StructField(\"Speechiness\",      FloatType(), True),\n",
    "                           StructField(\"Acousticness\",     FloatType(), True),\n",
    "                           StructField(\"Instrumentalness\", FloatType(), True),\n",
    "                           StructField(\"Liveness\",         FloatType(), True),\n",
    "                           StructField(\"Valence\",          FloatType(), True),\n",
    "                           StructField(\"Tempo\",            FloatType(), True),\n",
    "                           StructField(\"Time_signature\",   IntegerType(), True),\n",
    "                           StructField(\"Track_href\",       StringType(), True),\n",
    "                           StructField(\"Type_Feature\",     StringType(), True),\n",
    "                           StructField(\"Analysis_Url\",     StringType(), True),\n",
    "                           StructField('Execution_date',   StringType(), True)]\n",
    "    #Applying struct type\n",
    "    trackfeature_schema = StructType(trackfeature_schema)\n",
    "\n",
    "    #mapping\n",
    "    mapping = {\n",
    "        'artist': artist_schema,\n",
    "        'album': album_schema,\n",
    "        'track': track_schema,\n",
    "        'trackfeature': trackfeature_schema\n",
    "    }\n",
    "    \n",
    "    #return schema\n",
    "    return mapping[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Bronze task starts! -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 13:43:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/16 13:43:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: Bronze_task_spark and master: local!\n",
      "Starting to read data from database 'music_database' and collection 'artist_collection'...\n",
      "An error occurred while reading data from mongoDB: An error occurred while calling o151.load.\n",
      ": com.mongodb.MongoTimeoutException: Timed out while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongo:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: mongo}, caused by {java.net.UnknownHostException: mongo}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.createAndLogTimeoutException(BaseCluster.java:392)\n",
      "\tat com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:148)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.selectServer(SingleServerCluster.java:46)\n",
      "\tat com.mongodb.internal.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:108)\n",
      "\tat com.mongodb.client.internal.ClientSessionBinding.getConnectionSource(ClientSessionBinding.java:128)\n",
      "\tat com.mongodb.client.internal.ClientSessionBinding.getReadConnectionSource(ClientSessionBinding.java:92)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.withSuppliedResource(SyncOperationHelper.java:141)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.withSourceAndConnection(SyncOperationHelper.java:122)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.lambda$executeRetryableRead$4(SyncOperationHelper.java:186)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.lambda$decorateReadWithRetries$12(SyncOperationHelper.java:289)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:67)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.executeRetryableRead(SyncOperationHelper.java:191)\n",
      "\tat com.mongodb.internal.operation.SyncOperationHelper.executeRetryableRead(SyncOperationHelper.java:173)\n",
      "\tat com.mongodb.internal.operation.AggregateOperationImpl.execute(AggregateOperationImpl.java:189)\n",
      "\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:153)\n",
      "\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:44)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:153)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:130)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.forEach(MongoIterableImpl.java:116)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.into(MongoIterableImpl.java:125)\n",
      "\tat com.mongodb.spark.sql.connector.schema.InferSchema.lambda$inferSchema$0(InferSchema.java:106)\n",
      "\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withClient(AbstractMongoConfig.java:182)\n",
      "\tat com.mongodb.spark.sql.connector.config.ReadConfig.withClient(ReadConfig.java:47)\n",
      "\tat com.mongodb.spark.sql.connector.schema.InferSchema.inferSchema(InferSchema.java:83)\n",
      "\tat com.mongodb.spark.sql.connector.MongoTableProvider.inferSchema(MongoTableProvider.java:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n",
      "\tat scala.Option.flatMap(Option.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Successfully stopped Spark Session!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------- Bronze task starts! -------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mbronze_layer_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m01-12-2004\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------ Bronze task finished! -------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mbronze_layer_processing\u001b[0;34m(Execution_date)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"------------------------ BRONZE ARTIST ------------------------\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m artist_data \u001b[38;5;241m=\u001b[39m read_mongoDB(spark, database_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic_database\u001b[39m\u001b[38;5;124m'\u001b[39m, collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist_collection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m artist_data \u001b[38;5;241m=\u001b[39m \u001b[43martist_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m(artist_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m Execution_date)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting bronze preprocessing for artist data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#preprocessing before loading data\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, get_json_object, to_date, regexp_replace, length\n",
    "\n",
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_layer_processing(Execution_date: str):\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task_spark') as spark:\n",
    "        \"\"\"------------------------ BRONZE ARTIST ------------------------\"\"\"\n",
    "        artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "        artist_data = artist_data.filter(artist_data['Execution_date'] == Execution_date)\n",
    "\n",
    "        print(\"Starting bronze preprocessing for artist data...\")\n",
    "        #preprocessing before loading data\n",
    "        try:\n",
    "            artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                     .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "                                     .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                     .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "                                     .withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "            #reorder columns after reading \n",
    "            artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "                                            'Followers', 'Popularity', 'Artist_Image', \n",
    "                                            'Artist_Type', 'External_Url', 'Href', 'Artist_Uri', 'Execution_date')\n",
    "            #applying schema        \n",
    "            artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "\n",
    "            print(\"Finished bronze preprocessing for artist data.\")\n",
    "\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = artist_data, direct = 'bronze_data/bronze_artist', \n",
    "                       file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "        \"\"\"------------------------ BRONE ALBUM ------------------------\"\"\"\n",
    "        album_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'album_collection')\n",
    "        album_data = album_data.filter(album_data['Execution_date'] == Execution_date)\n",
    "        print(\"Starting bronze preprocessing for album data...\")\n",
    "        try:\n",
    "            album_data = album_data.withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                   .withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                   .withColumn('Release_Date', to_date('Release_Date', \"MM/dd/yyyy\")) \\\n",
    "                                   .withColumn('TotalTracks', col('TotalTracks').cast('int')) \\\n",
    "                                   .withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "            #reorder columns after reading\n",
    "            album_data = album_data.select('Artist', 'Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', \n",
    "                                        'Label', 'Popularity', 'Available_Markets', 'Release_Date', \n",
    "                                        'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', \n",
    "                                        'External_URL', 'Href', 'Image', 'Uri', 'Execution_date')\n",
    "            album_data = spark.createDataFrame(album_data.rdd, schema = get_schema('album'))\n",
    "            print(\"Finished bronze preprocessing for album data.\")\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = album_data, direct = 'bronze_data/bronze_album', \n",
    "                       file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK -------------------------\"\"\"\n",
    "        track_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'track_collection', \n",
    "                                  schema = get_schema('track'))\n",
    "        track_data = track_data.filter(track_data['Execution_date'] == Execution_date)\n",
    "        track_data = track_data.withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_data, direct = 'bronze_data/bronze_track', \n",
    "                   file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK FEATURE ------------------------\"\"\"\n",
    "        track_feature_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'trackfeature_collection', \n",
    "                                          schema = get_schema('trackfeature'))\n",
    "        track_feature_data = track_feature_data.filter(track_feature_data['Execution_date'] == Execution_date)\n",
    "        track_feature_data = track_feature_data.withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "        \n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_feature_data, direct = 'bronze_data/bronze_track_feature', \n",
    "                   file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"------------------------------- Bronze task starts! -------------------------------\")\n",
    "    bronze_layer_processing(\"01-12-2004\")\n",
    "    print(\"------------------------------ Bronze task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import explode_outer, ltrim\n",
    "\n",
    "\"\"\" Create SilverLayer class to process data in the Silver layer. \"\"\"\n",
    "class SilverLayer:\n",
    "    #init \n",
    "    def __init__(self, data: pyspark.sql.DataFrame, \n",
    "                 drop_columns: list = None, \n",
    "                 drop_null_columns: list = None,\n",
    "                 fill_nulls_columns: dict = None,\n",
    "                 duplicate_columns: list = None,\n",
    "                 nested_columns: list = None,\n",
    "                 rename_columns: dict = None,\n",
    "                 ):\n",
    "        \n",
    "        #check valid params\n",
    "        if data is not None and not isinstance(data, pyspark.sql.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        if drop_columns is not None and not isinstance(drop_columns, list):\n",
    "            raise TypeError(\"drop_columns must be a list!\")\n",
    "        \n",
    "        if drop_null_columns is not None and not isinstance(drop_null_columns, list):\n",
    "            raise TypeError(\"drop_null_columns must be a list!\")\n",
    "        \n",
    "        if fill_nulls_columns is not None and not isinstance(fill_nulls_columns, dict):\n",
    "            raise TypeError(\"handle_nulls must be a dict!\")\n",
    "        \n",
    "        if duplicate_columns is not None and not isinstance(duplicate_columns, list):\n",
    "            raise TypeError(\"duplicate_columns must be a list!\")\n",
    "        \n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"handle_nested must be a list!\")\n",
    "        \n",
    "        if rename_columns is not None and not isinstance(rename_columns, dict):\n",
    "            raise TypeError(\"rename_columns must be a dict!\")\n",
    "        \"\"\"Initialize class attributes for data processing.\"\"\"\n",
    "        self._data = data\n",
    "        self._drop_columns = drop_columns\n",
    "        self._drop_null_columns = drop_null_columns\n",
    "        self._fill_nulls_columns = fill_nulls_columns\n",
    "        self._duplicate_columns = duplicate_columns\n",
    "        self._nested_columns = nested_columns\n",
    "        self._rename_columns = rename_columns\n",
    "\n",
    "\n",
    "    \"\"\" Method to drop unnecessary columns. \"\"\"\n",
    "    def drop(self):\n",
    "        self._data = self._data.drop(*self._drop_columns)\n",
    "\n",
    "    \n",
    "    \"\"\" Method to drop rows based on null values in each column. \"\"\"\n",
    "    def drop_null(self):\n",
    "        self._data = self._data.dropna(subset = self._drop_null_columns, how = \"all\")\n",
    "\n",
    "    \n",
    "    \"\"\" Method to fill null values. \"\"\"\n",
    "    def fill_null(self):\n",
    "        for column_list, value in self._fill_nulls_columns.items():\n",
    "            self._data = self._data.fillna(value = value, subset = column_list)\n",
    "\n",
    "\n",
    "    \"\"\" Method to rename columns. \"\"\"\n",
    "    def rename(self):\n",
    "        for old_name, new_name in self._rename_columns.items():\n",
    "            self._data = self._data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "\n",
    "    \"\"\" Method to handle duplicates. \"\"\"\n",
    "    def handle_duplicate(self):\n",
    "        self._data = self._data.dropDuplicates(self._duplicate_columns)\n",
    "\n",
    "\n",
    "    \"\"\" Method to handle nested. \"\"\"\n",
    "    def handle_nested(self):\n",
    "        for column in self._nested_columns:\n",
    "            self._data = self._data.withColumn(column, explode_outer(column)) \\\n",
    "                                   .withColumn(column, ltrim(column))\n",
    "    \n",
    "    \n",
    "    \"\"\" Main processing. \"\"\"\n",
    "    def process(self) -> pyspark.sql.DataFrame:\n",
    "        #drop unnecessary columns\n",
    "        if self._drop_columns:\n",
    "            self.drop() \n",
    "\n",
    "        #drop rows contain null values for each col\n",
    "        if self._drop_null_columns:\n",
    "            self.drop_null()\n",
    "\n",
    "        #fill null values\n",
    "        if self._fill_nulls_columns:\n",
    "            self.fill_null()\n",
    "        \n",
    "        #handle duplicate rows\n",
    "        if self._duplicate_columns:\n",
    "            self.handle_duplicate()\n",
    "\n",
    "        #handle nested columns \n",
    "        if self._nested_columns:\n",
    "            self.handle_nested()\n",
    "\n",
    "        #rename columns\n",
    "        if self._rename_columns:\n",
    "            self.rename()\n",
    "\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 13:25:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: Silver_task_spark and master: local!\n",
      "------------------------------- Silver task starts! -------------------------------\n",
      "Starting silver artist data processing...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n",
      "Processing for 'silver_artist' ...\n",
      "Finished processing for 'silver_artist'.\n",
      "Starting to upload 'silver_artist' into hdfs://namenode:9000/datalake/silver_data/silver_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_artist' into HDFS.\n",
      "Starting silver album data processing...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_album...\n",
      "Processing for 'silver_album' ...\n",
      "Finished processing for 'silver_album'.\n",
      "Starting to upload 'silver_album' into hdfs://namenode:9000/datalake/silver_data/silver_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_album' into HDFS.\n",
      "Starting silver track data processing...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n",
      "Processing for 'silver_track' ...\n",
      "Finished processing for 'silver_track'.\n",
      "Starting to upload 'silver_track' into hdfs://namenode:9000/datalake/silver_data/silver_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_track' into HDFS.\n",
      "Starting silver track feature data processing...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track_feature...\n",
      "Processing for 'silver_track_feature' ...\n",
      "Finished processing for 'silver_track_feature'.\n",
      "Starting to upload 'silver_track_feature' into hdfs://namenode:9000/datalake/silver_data/silver_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_track_feature' into HDFS.\n",
      "------------------------------ Silver task finished! -------------------------------\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "\"\"\" Processing silver artist data. \"\"\"\n",
    "def silver_artist_process(spark: SparkSession):\n",
    "    #read bronze artist data\n",
    "    bronze_artist = read_HDFS(spark, HDFS_dir = \"bronze_data/bronze_artist\", file_type = 'parquet')\n",
    "\n",
    "    #applying SilverLayer class \n",
    "    silver_artist = SilverLayer(data = bronze_artist, \n",
    "                                drop_columns       = ['Artist_Type', 'Href', 'Artist_Uri', 'Execution_date'],\n",
    "                                drop_null_columns  = ['Artist_ID'], \n",
    "                                fill_nulls_columns = {'Followers': 0,\n",
    "                                                      'Popularity': 0},\n",
    "                                duplicate_columns  = ['Artist_ID'],\n",
    "                                nested_columns     = ['Genres'],\n",
    "                                rename_columns     = {'Artist_ID': 'id',\n",
    "                                                      'Artist_Name': 'name',\n",
    "                                                      'Genres': 'genres',\n",
    "                                                      'Followers': 'followers',\n",
    "                                                      'Popularity': 'popularity',\n",
    "                                                      'Artist_Image': 'link_image',\n",
    "                                                      'External_Url': 'url'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_artist' ...\")\n",
    "    silver_artist = silver_artist.process()\n",
    "    print(\"Finished processing for 'silver_artist'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_artist, direct = \"silver_data/silver_artist\", file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver album data. \"\"\"\n",
    "def silver_album_process(spark: SparkSession):\n",
    "    #read bronze album data\n",
    "    bronze_album = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_album = SilverLayer(data = bronze_album,\n",
    "                               drop_columns       = ['Genres', 'Available_Markets', 'Restrictions', \n",
    "                                                     'Href','Uri', 'Execution_date'],\n",
    "                               drop_null_columns  = ['Album_ID'],\n",
    "                               fill_nulls_columns = {'Popularity': 0,\n",
    "                                                     'TotalTracks': 0},\n",
    "                               duplicate_columns  = ['Album_ID'],\n",
    "                               rename_columns     = {'Artist': 'artist',\n",
    "                                                     'Artist_ID': 'artist_id',\n",
    "                                                     'Album_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Type': 'type',\n",
    "                                                     'Label': 'label',\n",
    "                                                     'Popularity': 'popularity',\n",
    "                                                     'Release_Date': 'release_date',\n",
    "                                                     'ReleaseDatePrecision': 'release_date_precision',\n",
    "                                                     'TotalTracks': 'total_tracks',\n",
    "                                                     'Copyrights': 'copyrights',\n",
    "                                                     'External_URL': 'url',\n",
    "                                                     'Image': 'link_image'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_album' ...\")\n",
    "    silver_album = silver_album.process()\n",
    "    print(\"Finished processing for 'silver_album'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_album, direct = 'silver_data/silver_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track data. \"\"\"\n",
    "def silver_track_process(spark: SparkSession):\n",
    "    #read bronze track data\n",
    "    bronze_track = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track = SilverLayer(data               = bronze_track,\n",
    "                               drop_columns       = ['Artists', 'Type', 'AvailableMarkets', 'Href', \n",
    "                                                     'Uri', 'Is_Local', 'Execution_date'],\n",
    "                               drop_null_columns  = ['Track_ID'],\n",
    "                               fill_nulls_columns = {'Restrictions': 'None'},\n",
    "                               duplicate_columns  = ['Track_ID'],\n",
    "                               rename_columns     = {'Album_ID': 'album_id',\n",
    "                                                     'Album_Name': 'album_name',\n",
    "                                                     'Track_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Track_Number': 'track_number',\n",
    "                                                     'Disc_Number': 'disc_number',\n",
    "                                                     'Duration_ms': 'duration_ms',\n",
    "                                                     'Explicit': 'explicit',\n",
    "                                                     'External_urls': 'url',\n",
    "                                                     'Restrictions': 'restriction',\n",
    "                                                     'Preview_url': 'preview'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track' ...\")\n",
    "    silver_track = silver_track.process()\n",
    "    print(\"Finished processing for 'silver_track'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track, direct = 'silver_data/silver_track', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track feature data. \"\"\"\n",
    "def silver_track_feature_process(spark: SparkSession):\n",
    "    #read silver track feature data\n",
    "    bronze_track_feature = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track_feature', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track_feature = SilverLayer(data              = bronze_track_feature,\n",
    "                                       drop_columns      = ['Track_href', 'Type_Feature', 'Analysis_Url', 'Execution_date'],\n",
    "                                       drop_null_columns = ['Track_ID'],\n",
    "                                       duplicate_columns = ['Track_ID'],\n",
    "                                       rename_columns    = {'Track_ID': 'id',\n",
    "                                                            'Danceability': 'danceability',\n",
    "                                                            'Energy': 'energy',\n",
    "                                                            'Key': 'key',\n",
    "                                                            'Loudness': 'loudness',\n",
    "                                                            'Mode': 'mode',\n",
    "                                                            'Speechiness': 'speechiness',\n",
    "                                                            'Acousticness': 'acousticness',\n",
    "                                                            'Instrumentalness': 'instrumentalness',\n",
    "                                                            'Liveness': 'liveness',\n",
    "                                                            'Valence': 'valence',\n",
    "                                                            'Tempo': 'tempo',\n",
    "                                                            'Time_signature': 'time_signature'})\n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track_feature' ...\")\n",
    "    silver_track_feature = silver_track_feature.process()\n",
    "    print(\"Finished processing for 'silver_track_feature'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track_feature, direct = 'silver_data/silver_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "#main call\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with get_sparkSession(\"Silver_task_spark\") as spark:\n",
    "        print(\"------------------------------- Silver task starts! -------------------------------\")\n",
    "        print(\"Starting silver artist data processing...\")\n",
    "        silver_artist_process(spark)\n",
    "        print(\"Starting silver album data processing...\")\n",
    "        silver_album_process(spark)\n",
    "        print(\"Starting silver track data processing...\")\n",
    "        silver_track_process(spark)\n",
    "        print(\"Starting silver track feature data processing...\")\n",
    "        silver_track_feature_process(spark)\n",
    "        print(\"------------------------------ Silver task finished! -------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
