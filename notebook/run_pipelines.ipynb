{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkConf\n",
    "from contextlib import contextmanager\n",
    "import pyspark.sql\n",
    "\n",
    "\"\"\" Context manager for creating Spark Session. \"\"\"\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    #declare sparkconf\n",
    "    conf = SparkConf()\n",
    "\n",
    "    #set config\n",
    "    conf = conf.setAppName(appName) \\\n",
    "               .setMaster(master) \\\n",
    "               .set(\"spark.executor.memory\", \"4g\") \\\n",
    "               .set(\"spark.executor.cores\", \"2\") \\\n",
    "               .set(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "               .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "               .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\") \\\n",
    "            #    .set(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4\")\n",
    "    \n",
    "    #create Spark Session\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully created Spark Session with app name: {appName} and master: {master}!\")\n",
    "\n",
    "    #yield spark\n",
    "    try:\n",
    "        yield spark\n",
    "\n",
    "    finally:\n",
    "        #must stop Spark Session\n",
    "        spark.stop()\n",
    "        print(\"Successfully stopped Spark Session!\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from mongoDB. \"\"\"\n",
    "def read_mongoDB(spark: SparkSession, database_name: str, collection_name: str, query: dict = None,\n",
    "                 schema: StructType = None, username: str = 'huynhthuan', password: str = 'password', \n",
    "                 host: str = 'mongo', port: str = 27017) -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if query is not None and not isinstance(query, dict):\n",
    "        raise TypeError(\"query must be a dict!\")\n",
    "    \n",
    "    if schema is not None and not isinstance(schema, StructType):\n",
    "        raise TypeError(\"schema must be a StructType!\")\n",
    "    \n",
    "    #uri mongoDB \n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "\n",
    "    print(f\"Starting to read data from database '{database_name}' and collection '{collection_name}'...\")\n",
    "  \n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format('mongodb') \\\n",
    "                         .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                         .option('header', 'true')\n",
    "        \n",
    "        data = data.schema(schema).load() if schema is not None else data.load()\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from mongoDB: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from HDFS. \"\"\"\n",
    "def read_HDFS(spark: SparkSession, HDFS_dir: str, file_type: str) -> pyspark.sql.DataFrame:\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    #set HDFS path\n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{HDFS_dir}\"\n",
    "\n",
    "    print(f\"Starting to read data from {HDFS_path}...\")\n",
    "\n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format(file_type).option('header', 'true').load(HDFS_path)\n",
    "        #return data\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from HDFS: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Write data into HDFS. \"\"\"\n",
    "def write_HDFS(spark: SparkSession, data: pyspark.sql.DataFrame, direct: str, \n",
    "               file_type: str, mode: str = 'overwrite', partition: str = None):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "\n",
    "    #set HDFS path  \n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{direct}\"\n",
    "    table_name = direct.split('/')[-1]\n",
    "\n",
    "    print(f\"Starting to upload '{table_name}' into {HDFS_path}...\")\n",
    "    \n",
    "    #write data\n",
    "    try:\n",
    "        if partition is not None:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode(mode) \\\n",
    "                      .partitionBy('Execution_date') \\\n",
    "                      .save(HDFS_path)\n",
    "        else:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode(mode) \\\n",
    "                      .save(HDFS_path)\n",
    "        \n",
    "        print(f\"Successfully uploaded '{table_name}' into HDFS.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "\n",
    "\"\"\" Write data into SnowFlake Data Warehouse. \"\"\"\n",
    "def write_SnowFlake(spark: SparkSession, data: pyspark.sql.DataFrame, table_name: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    \n",
    "    snowflake_connection_options = {\n",
    "        \"sfURL\": \"https://sl70006.southeast-asia.azure.snowflakecomputing.com\",\n",
    "        \"sfUser\": \"HUYNHTHUAN\", \n",
    "        \"sfPassword\": \"Thuan123456\",\n",
    "        \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "        \"sfDatabase\": \"SPOTIFY_MUSIC_DB\" \n",
    "    }\n",
    "\n",
    "    print(f\"Starting to upload {table_name.split('.')[-1]} into SnowFlake...\")\n",
    "    try:\n",
    "        data.write.format(\"snowflake\") \\\n",
    "                .options(**snowflake_connection_options) \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .mode('overwrite') \\\n",
    "                .save()\n",
    "        print(f\"Successfully uploaded '{table_name}' into SnowFlake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Initial load task starts! -------------------------------\n",
      "Successfully created Spark Session with app name: init_load and master: local!\n",
      "Starting load csv files into MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded data into mongoDB.\n",
      "Successfully stopped Spark Session!\n",
      "------------------------------- Initial load task finished! -------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\"\"\" Load all csv files into mongoDB.\"\"\"\n",
    "def initial_load(Execution_date: str):\n",
    "    with get_sparkSession(appName = \"init_load\") as spark:\n",
    "        #uri\n",
    "        uri_artist_name = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_name_collection?authSource=admin\"\n",
    "        uri_artist = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_collection?authSource=admin\"\n",
    "        uri_album = \"mongodb://huynhthuan:password@mongo:27017/music_database.album_collection?authSource=admin\"\n",
    "        uri_track = \"mongodb://huynhthuan:password@mongo:27017/music_database.track_collection?authSource=admin\"\n",
    "        uri_trackfeature = \"mongodb://huynhthuan:password@mongo:27017/music_database.trackfeature_collection?authSource=admin\"\n",
    "\n",
    "        # read\n",
    "        df_ArtistName = spark.read.option('header', 'true').csv(\"/opt/data/ArtistName.csv\")\n",
    "        df_ArtistName = df_ArtistName.withColumn('Execution_date', lit(Execution_date))\n",
    "        \n",
    "        df_Artist = spark.read.option('header', 'true').csv(\"/opt/data/Artist.csv\")\n",
    "        df_Artist = df_Artist.withColumn('Execution_date', lit(Execution_date))\n",
    "\n",
    "        df_Album = spark.read.option('header', 'true').csv(\"/opt/data/Album.csv\")\n",
    "        df_Album = df_Album.withColumn('Execution_date', lit(Execution_date))\n",
    "\n",
    "        df_Track = spark.read.option('header', 'true').csv(\"/opt/data/Track.csv\")\n",
    "        df_Track = df_Track.withColumn('Execution_date', lit(Execution_date))\n",
    "        \n",
    "        df_TrackFeature = spark.read.option('header', 'true').csv(\"/opt/data/TrackFeature.csv\")\n",
    "        df_TrackFeature = df_TrackFeature.withColumn('Execution_date', lit(Execution_date))\n",
    "\n",
    "        #write\n",
    "        try:\n",
    "            print(\"Starting load csv files into MongoDB...\")\n",
    "            df_ArtistName.write.format('mongoDB') \\\n",
    "                            .option(\"spark.mongodb.write.connection.uri\", uri_artist_name) \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .save()\n",
    "            \n",
    "            df_Artist.write.format('mongoDB') \\\n",
    "                        .option(\"spark.mongodb.write.connection.uri\", uri_artist) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save()\n",
    "            \n",
    "            df_Album.write.format('mongoDB') \\\n",
    "                        .option(\"spark.mongodb.write.connection.uri\", uri_album) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save()\n",
    "            \n",
    "            df_Track.write.format('mongoDB') \\\n",
    "                        .option(\"spark.mongodb.write.connection.uri\", uri_track) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save()\n",
    "            \n",
    "            df_TrackFeature.write.format('mongoDB') \\\n",
    "                                .option(\"spark.mongodb.write.connection.uri\", uri_trackfeature) \\\n",
    "                                .mode(\"overwrite\") \\\n",
    "                                .save()\n",
    "            print(\"Successfully uploaded data into mongoDB.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured while loading data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"------------------------------- Initial load task starts! -------------------------------\")\n",
    "    initial_load(\"01-12-2004\")\n",
    "    print(\"------------------------------- Initial load task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DateType, FloatType\n",
    "\n",
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',      StringType(), True),\n",
    "                     StructField('Artist_Name',    StringType(), True),\n",
    "                     StructField('Genres',         ArrayType(StringType(), True), True),\n",
    "                     StructField('Followers',      IntegerType(), True),\n",
    "                     StructField('Popularity',     IntegerType(), True),\n",
    "                     StructField('Artist_Image',   StringType(), True),\n",
    "                     StructField('Artist_Type',    StringType(), True),\n",
    "                     StructField('External_Url',   StringType(), True),\n",
    "                     StructField('Href',           StringType(), True),\n",
    "                     StructField('Artist_Uri',     StringType(), True),\n",
    "                     StructField('Execution_date', DateType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)\n",
    "    \n",
    "    \"\"\" Album schema. \"\"\"\n",
    "    album_schema = [StructField('Artist',               StringType(), True),\n",
    "                    StructField('Artist_ID',            StringType(), True),\n",
    "                    StructField('Album_ID',             StringType(), True),\n",
    "                    StructField('Name',                 StringType(), True),\n",
    "                    StructField('Type',                 StringType(), True),\n",
    "                    StructField('Genres',               ArrayType(StringType(), True), True),\n",
    "                    StructField('Label',                StringType(), True),\n",
    "                    StructField('Popularity',           IntegerType(), True),\n",
    "                    StructField('Available_Markets',    StringType(), True),\n",
    "                    StructField('Release_Date',         DateType(), True),\n",
    "                    StructField('ReleaseDatePrecision', StringType(), True),\n",
    "                    StructField('TotalTracks',          IntegerType(), True),\n",
    "                    StructField('Copyrights',           StringType(), True),\n",
    "                    StructField('Restrictions',         StringType(), True),\n",
    "                    StructField('External_URL',         StringType(), True),\n",
    "                    StructField('Href',                 StringType(), True),\n",
    "                    StructField('Image',                StringType(), True),\n",
    "                    StructField('Uri',                  StringType(), True),\n",
    "                    StructField('Execution_date',       DateType(), True)]\n",
    "    #Applying struct type\n",
    "    album_schema = StructType(album_schema)\n",
    "\n",
    "    \"\"\" Track schema. \"\"\"\n",
    "    track_schema = [StructField(\"Artists\",          StringType(), True),\n",
    "                    StructField(\"Album_ID\",         StringType(), True),\n",
    "                    StructField(\"Album_Name\",       StringType(), True),\n",
    "                    StructField(\"Track_ID\",         StringType(), True),\n",
    "                    StructField(\"Name\",             StringType(), True),\n",
    "                    StructField(\"Track_Number\",     IntegerType(), True),\n",
    "                    StructField(\"Type\",             StringType(), True),\n",
    "                    StructField(\"AvailableMarkets\", StringType(), True),\n",
    "                    StructField(\"Disc_Number\",      IntegerType(), True),\n",
    "                    StructField(\"Duration_ms\",      IntegerType(), True),\n",
    "                    StructField(\"Explicit\",         StringType(), True),\n",
    "                    StructField(\"External_urls\",    StringType(), True),\n",
    "                    StructField(\"Href\",             StringType(), True),\n",
    "                    StructField(\"Restrictions\",     StringType(), True),\n",
    "                    StructField(\"Preview_url\",      StringType(), True),\n",
    "                    StructField(\"Uri\",              StringType(), True),\n",
    "                    StructField(\"Is_Local\",         StringType(), True),\n",
    "                    StructField('Execution_date',   StringType(), True)]\n",
    "    #Applying struct type\n",
    "    track_schema = StructType(track_schema)\n",
    "    \n",
    "    \"\"\" TrackFeature schema. \"\"\"\n",
    "    trackfeature_schema = [StructField(\"Track_ID\",         StringType(), True),\n",
    "                           StructField(\"Danceability\",     FloatType(), True),\n",
    "                           StructField(\"Energy\",           FloatType(), True),\n",
    "                           StructField(\"Key\",              IntegerType(), True),\n",
    "                           StructField(\"Loudness\",         FloatType(), True),\n",
    "                           StructField(\"Mode\",             IntegerType(), True),\n",
    "                           StructField(\"Speechiness\",      FloatType(), True),\n",
    "                           StructField(\"Acousticness\",     FloatType(), True),\n",
    "                           StructField(\"Instrumentalness\", FloatType(), True),\n",
    "                           StructField(\"Liveness\",         FloatType(), True),\n",
    "                           StructField(\"Valence\",          FloatType(), True),\n",
    "                           StructField(\"Tempo\",            FloatType(), True),\n",
    "                           StructField(\"Time_signature\",   IntegerType(), True),\n",
    "                           StructField(\"Track_href\",       StringType(), True),\n",
    "                           StructField(\"Type_Feature\",     StringType(), True),\n",
    "                           StructField(\"Analysis_Url\",     StringType(), True),\n",
    "                           StructField('Execution_date',   StringType(), True)]\n",
    "    #Applying struct type\n",
    "    trackfeature_schema = StructType(trackfeature_schema)\n",
    "\n",
    "    #mapping\n",
    "    mapping = {\n",
    "        'artist': artist_schema,\n",
    "        'album': album_schema,\n",
    "        'track': track_schema,\n",
    "        'trackfeature': trackfeature_schema\n",
    "    }\n",
    "    \n",
    "    #return schema\n",
    "    return mapping[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Bronze task starts! -------------------------------\n",
      "Successfully created Spark Session with app name: Bronze_task_spark and master: local!\n",
      "Starting to read data from database 'music_database' and collection 'artist_collection'...\n",
      "Starting bronze preprocessing for artist data...\n",
      "Finished bronze preprocessing for artist data.\n",
      "Starting to upload 'bronze_artist' into hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_artist' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'album_collection'...\n",
      "Starting bronze preprocessing for album data...\n",
      "Finished bronze preprocessing for album data.\n",
      "Starting to upload 'bronze_album' into hdfs://namenode:9000/datalake/bronze_data/bronze_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_album' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'track_collection'...\n",
      "Starting to upload 'bronze_track' into hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 16:39:12 WARN DataStreamer: Slow waitForAckedSeqno took 36393ms (threshold=30000ms). File being written: /datalake/bronze_data/bronze_track/_temporary/0/_temporary/attempt_2024121016371598259262031602052_0002_m_000003_7/Execution_date=__HIVE_DEFAULT_PARTITION__/part-00003-0b1124f2-9b6e-439d-94f8-2e49fa074d9c.c000.snappy.parquet, block: BP-727518114-172.18.0.7-1733812929401:blk_1073741832_1008, Write pipeline datanodes: null.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_track' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'trackfeature_collection'...\n",
      "Starting to upload 'bronze_track_feature' into hdfs://namenode:9000/datalake/bronze_data/bronze_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_track_feature' into HDFS.\n",
      "Successfully stopped Spark Session!\n",
      "------------------------------ Bronze task finished! -------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, get_json_object, to_date, regexp_replace, length\n",
    "\n",
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_layer_processing(Execution_date: str):\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task_spark') as spark:\n",
    "        \"\"\"------------------------ BRONZE ARTIST ------------------------\"\"\"\n",
    "        artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "        artist_data = artist_data.filter(artist_data['Execution_date'] == Execution_date)\n",
    "\n",
    "        print(\"Starting bronze preprocessing for artist data...\")\n",
    "        #preprocessing before loading data\n",
    "        try:\n",
    "            artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                     .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "                                     .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                     .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "                                     .withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "            #reorder columns after reading \n",
    "            artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "                                            'Followers', 'Popularity', 'Artist_Image', \n",
    "                                            'Artist_Type', 'External_Url', 'Href', 'Artist_Uri', 'Execution_date')\n",
    "            #applying schema        \n",
    "            artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "\n",
    "            print(\"Finished bronze preprocessing for artist data.\")\n",
    "\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = artist_data, direct = 'bronze_data/bronze_artist', \n",
    "                       file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "        \"\"\"------------------------ BRONE ALBUM ------------------------\"\"\"\n",
    "        album_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'album_collection')\n",
    "        album_data = album_data.filter(album_data['Execution_date'] == Execution_date)\n",
    "        print(\"Starting bronze preprocessing for album data...\")\n",
    "        try:\n",
    "            album_data = album_data.withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                   .withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                   .withColumn('Release_Date', to_date('Release_Date', \"MM/dd/yyyy\")) \\\n",
    "                                   .withColumn('TotalTracks', col('TotalTracks').cast('int')) \\\n",
    "                                   .withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "            #reorder columns after reading\n",
    "            album_data = album_data.select('Artist', 'Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', \n",
    "                                        'Label', 'Popularity', 'Available_Markets', 'Release_Date', \n",
    "                                        'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', \n",
    "                                        'External_URL', 'Href', 'Image', 'Uri', 'Execution_date')\n",
    "            album_data = spark.createDataFrame(album_data.rdd, schema = get_schema('album'))\n",
    "            print(\"Finished bronze preprocessing for album data.\")\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = album_data, direct = 'bronze_data/bronze_album', \n",
    "                       file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK -------------------------\"\"\"\n",
    "        track_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'track_collection', \n",
    "                                  schema = get_schema('track'))\n",
    "        track_data = track_data.filter(track_data['Execution_date'] == Execution_date)\n",
    "        track_data = track_data.withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_data, direct = 'bronze_data/bronze_track', \n",
    "                   file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK FEATURE ------------------------\"\"\"\n",
    "        track_feature_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'trackfeature_collection', \n",
    "                                          schema = get_schema('trackfeature'))\n",
    "        track_feature_data = track_feature_data.filter(track_feature_data['Execution_date'] == Execution_date)\n",
    "        track_feature_data = track_feature_data.withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "        \n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_feature_data, direct = 'bronze_data/bronze_track_feature', \n",
    "                   file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"------------------------------- Bronze task starts! -------------------------------\")\n",
    "    bronze_layer_processing(\"01-12-2004\")\n",
    "    print(\"------------------------------ Bronze task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import explode_outer, ltrim\n",
    "\n",
    "\"\"\" Create SilverLayer class to process data in the Silver layer. \"\"\"\n",
    "class SilverLayer:\n",
    "    #init \n",
    "    def __init__(self, data: pyspark.sql.DataFrame, \n",
    "                 drop_columns: list = None, \n",
    "                 drop_null_columns: list = None,\n",
    "                 fill_nulls_columns: dict = None,\n",
    "                 duplicate_columns: list = None,\n",
    "                 nested_columns: list = None,\n",
    "                 rename_columns: dict = None,\n",
    "                 ):\n",
    "        \n",
    "        #check valid params\n",
    "        if data is not None and not isinstance(data, pyspark.sql.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        if drop_columns is not None and not isinstance(drop_columns, list):\n",
    "            raise TypeError(\"drop_columns must be a list!\")\n",
    "        \n",
    "        if drop_null_columns is not None and not isinstance(drop_null_columns, list):\n",
    "            raise TypeError(\"drop_null_columns must be a list!\")\n",
    "        \n",
    "        if fill_nulls_columns is not None and not isinstance(fill_nulls_columns, dict):\n",
    "            raise TypeError(\"handle_nulls must be a dict!\")\n",
    "        \n",
    "        if duplicate_columns is not None and not isinstance(duplicate_columns, list):\n",
    "            raise TypeError(\"duplicate_columns must be a list!\")\n",
    "        \n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"handle_nested must be a list!\")\n",
    "        \n",
    "        if rename_columns is not None and not isinstance(rename_columns, dict):\n",
    "            raise TypeError(\"rename_columns must be a dict!\")\n",
    "        \"\"\"Initialize class attributes for data processing.\"\"\"\n",
    "        self._data = data\n",
    "        self._drop_columns = drop_columns\n",
    "        self._drop_null_columns = drop_null_columns\n",
    "        self._fill_nulls_columns = fill_nulls_columns\n",
    "        self._duplicate_columns = duplicate_columns\n",
    "        self._nested_columns = nested_columns\n",
    "        self._rename_columns = rename_columns\n",
    "\n",
    "\n",
    "    \"\"\" Method to drop unnecessary columns. \"\"\"\n",
    "    def drop(self):\n",
    "        self._data = self._data.drop(*self._drop_columns)\n",
    "\n",
    "    \n",
    "    \"\"\" Method to drop rows based on null values in each column. \"\"\"\n",
    "    def drop_null(self):\n",
    "        self._data = self._data.dropna(subset = self._drop_null_columns, how = \"all\")\n",
    "\n",
    "    \n",
    "    \"\"\" Method to fill null values. \"\"\"\n",
    "    def fill_null(self):\n",
    "        for column_list, value in self._fill_nulls_columns.items():\n",
    "            self._data = self._data.fillna(value = value, subset = column_list)\n",
    "\n",
    "\n",
    "    \"\"\" Method to rename columns. \"\"\"\n",
    "    def rename(self):\n",
    "        for old_name, new_name in self._rename_columns.items():\n",
    "            self._data = self._data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "\n",
    "    \"\"\" Method to handle duplicates. \"\"\"\n",
    "    def handle_duplicate(self):\n",
    "        self._data = self._data.dropDuplicates(self._duplicate_columns)\n",
    "\n",
    "\n",
    "    \"\"\" Method to handle nested. \"\"\"\n",
    "    def handle_nested(self):\n",
    "        for column in self._nested_columns:\n",
    "            self._data = self._data.withColumn(column, explode_outer(column)) \\\n",
    "                                   .withColumn(column, ltrim(column))\n",
    "    \n",
    "    \n",
    "    \"\"\" Main processing. \"\"\"\n",
    "    def process(self) -> pyspark.sql.DataFrame:\n",
    "        #drop unnecessary columns\n",
    "        if self._drop_columns:\n",
    "            self.drop() \n",
    "\n",
    "        #drop rows contain null values for each col\n",
    "        if self._drop_null_columns:\n",
    "            self.drop_null()\n",
    "\n",
    "        #fill null values\n",
    "        if self._fill_nulls_columns:\n",
    "            self.fill_null()\n",
    "        \n",
    "        #handle duplicate rows\n",
    "        if self._duplicate_columns:\n",
    "            self.handle_duplicate()\n",
    "\n",
    "        #handle nested columns \n",
    "        if self._nested_columns:\n",
    "            self.handle_nested()\n",
    "\n",
    "        #rename columns\n",
    "        if self._rename_columns:\n",
    "            self.rename()\n",
    "\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "\"\"\" Processing silver artist data. \"\"\"\n",
    "def silver_artist_process(spark: SparkSession):\n",
    "    #read bronze artist data\n",
    "    bronze_artist = read_HDFS(spark, HDFS_dir = \"bronze_data/bronze_artist\", file_type = 'parquet')\n",
    "\n",
    "    #applying SilverLayer class \n",
    "    silver_artist = SilverLayer(data = bronze_artist, \n",
    "                                drop_columns       = ['Artist_Type', 'Href', 'Artist_Uri', 'Execution_date'],\n",
    "                                drop_null_columns  = ['Artist_ID'], \n",
    "                                fill_nulls_columns = {'Followers': 0,\n",
    "                                                      'Popularity': 0},\n",
    "                                duplicate_columns  = ['Artist_ID'],\n",
    "                                nested_columns     = ['Genres'],\n",
    "                                rename_columns     = {'Artist_ID': 'id',\n",
    "                                                      'Artist_Name': 'name',\n",
    "                                                      'Genres': 'genres',\n",
    "                                                      'Followers': 'followers',\n",
    "                                                      'Popularity': 'popularity',\n",
    "                                                      'Artist_Image': 'link_image',\n",
    "                                                      'External_Url': 'url'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_artist' ...\")\n",
    "    silver_artist = silver_artist.process()\n",
    "    print(\"Finished processing for 'silver_artist'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_artist, direct = \"silver_data/silver_artist\", file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver album data. \"\"\"\n",
    "def silver_album_process(spark: SparkSession):\n",
    "    #read bronze album data\n",
    "    bronze_album = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_album = SilverLayer(data = bronze_album,\n",
    "                               drop_columns       = ['Genres', 'Available_Markets', 'Restrictions', \n",
    "                                                     'Href','Uri', 'Execution_date'],\n",
    "                               drop_null_columns  = ['Album_ID'],\n",
    "                               fill_nulls_columns = {'Popularity': 0,\n",
    "                                                     'TotalTracks': 0},\n",
    "                               duplicate_columns  = ['Album_ID'],\n",
    "                               rename_columns     = {'Artist': 'artist',\n",
    "                                                     'Artist_ID': 'artist_id',\n",
    "                                                     'Album_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Type': 'type',\n",
    "                                                     'Label': 'label',\n",
    "                                                     'Popularity': 'popularity',\n",
    "                                                     'Release_Date': 'release_date',\n",
    "                                                     'ReleaseDatePrecision': 'release_date_precision',\n",
    "                                                     'TotalTracks': 'total_tracks',\n",
    "                                                     'Copyrights': 'copyrights',\n",
    "                                                     'External_URL': 'url',\n",
    "                                                     'Image': 'link_image'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_album' ...\")\n",
    "    silver_album = silver_album.process()\n",
    "    print(\"Finished processing for 'silver_album'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_album, direct = 'silver_data/silver_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track data. \"\"\"\n",
    "def silver_track_process(spark: SparkSession):\n",
    "    #read bronze track data\n",
    "    bronze_track = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track = SilverLayer(data               = bronze_track,\n",
    "                               drop_columns       = ['Artists', 'Type', 'AvailableMarkets', 'Href', \n",
    "                                                     'Uri', 'Is_Local', 'Execution_date'],\n",
    "                               drop_null_columns  = ['Track_ID'],\n",
    "                               fill_nulls_columns = {'Restrictions': 'None'},\n",
    "                               duplicate_columns  = ['Track_ID'],\n",
    "                               rename_columns     = {'Album_ID': 'album_id',\n",
    "                                                     'Album_Name': 'album_name',\n",
    "                                                     'Track_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Track_Number': 'track_number',\n",
    "                                                     'Disc_Number': 'disc_number',\n",
    "                                                     'Duration_ms': 'duration_ms',\n",
    "                                                     'Explicit': 'explicit',\n",
    "                                                     'External_urls': 'url',\n",
    "                                                     'Restrictions': 'restriction',\n",
    "                                                     'Preview_url': 'preview'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track' ...\")\n",
    "    silver_track = silver_track.process()\n",
    "    print(\"Finished processing for 'silver_track'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track, direct = 'silver_data/silver_track', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track feature data. \"\"\"\n",
    "def silver_track_feature_process(spark: SparkSession):\n",
    "    #read silver track feature data\n",
    "    bronze_track_feature = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track_feature', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track_feature = SilverLayer(data              = bronze_track_feature,\n",
    "                                       drop_columns      = ['Track_href', 'Type_Feature', 'Analysis_Url', 'Execution_date'],\n",
    "                                       drop_null_columns = ['Track_ID'],\n",
    "                                       duplicate_columns = ['Track_ID'],\n",
    "                                       rename_columns    = {'Track_ID': 'id',\n",
    "                                                            'Danceability': 'danceability',\n",
    "                                                            'Energy': 'energy',\n",
    "                                                            'Key': 'key',\n",
    "                                                            'Loudness': 'loudness',\n",
    "                                                            'Mode': 'mode',\n",
    "                                                            'Speechiness': 'speechiness',\n",
    "                                                            'Acousticness': 'acousticness',\n",
    "                                                            'Instrumentalness': 'instrumentalness',\n",
    "                                                            'Liveness': 'liveness',\n",
    "                                                            'Valence': 'valence',\n",
    "                                                            'Tempo': 'tempo',\n",
    "                                                            'Time_signature': 'time_signature'})\n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track_feature' ...\")\n",
    "    silver_track_feature = silver_track_feature.process()\n",
    "    print(\"Finished processing for 'silver_track_feature'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track_feature, direct = 'silver_data/silver_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "#main call\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with get_sparkSession(\"Silver_task_spark\") as spark:\n",
    "        print(\"------------------------------- Silver task starts! -------------------------------\")\n",
    "        print(\"Starting silver artist data processing...\")\n",
    "        silver_artist_process(spark)\n",
    "        print(\"Starting silver album data processing...\")\n",
    "        silver_album_process(spark)\n",
    "        print(\"Starting silver track data processing...\")\n",
    "        silver_track_process(spark)\n",
    "        print(\"Starting silver track feature data processing...\")\n",
    "        silver_track_feature_process(spark)\n",
    "        print(\"------------------------------ Silver task finished! -------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
