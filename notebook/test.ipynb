{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient, database, collection\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionFailure, OperationFailure\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient, database, collection\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\"\"\" Context manager for mongoDB connection. \"\"\"\n",
    "@contextmanager\n",
    "def mongoDB_client(username: str, password: str, \n",
    "                    host: str = 'mongo', port: str = 27017):\n",
    "    #set path\n",
    "    path = f\"mongodb://{username}:{password}@{host}:{port}\"\n",
    "    client = None\n",
    "\n",
    "    #init\n",
    "    try:\n",
    "        print(\"Starting connect mongoDB...\")\n",
    "        client = MongoClient(path)\n",
    "        \n",
    "        print(\"Client connected successfully!\")\n",
    "        yield client\n",
    "\n",
    "    #handle error\n",
    "    except ConnectionFailure:\n",
    "        print(\"Connection to mongoDB failed!\")\n",
    "\n",
    "    except OperationFailure:\n",
    "        print(\"Operation failed!\")\n",
    "\n",
    "    #close client\n",
    "    finally:\n",
    "        client.close()\n",
    "        print(\"The connection to MongoDB has stopped!\")\n",
    "\n",
    "\"\"\" Class mongoDB for operations. \"\"\"\n",
    "class mongoDB_operations:\n",
    "    \"\"\" Init \"\"\"\n",
    "    def __init__(self, client: MongoClient):\n",
    "        #check params\n",
    "        if not isinstance(client, MongoClient):\n",
    "            raise TypeError('client must be MongoClient!')\n",
    "        \n",
    "        #set value for class attrs\n",
    "        self.client = client\n",
    "\n",
    "    \"\"\" Check whether the database exists. \"\"\"\n",
    "    def check_database_exists(self, database_name: str) -> bool:\n",
    "        #list database name\n",
    "        return database_name in self.client.list_database_names()\n",
    "\n",
    "    \"\"\" Check whether collection exists. \"\"\"\n",
    "    def check_collection_exists(self, database_obj: database.Database, collection: str) -> bool:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #list collection name\n",
    "        return collection in self.client[database_obj.name].list_collection_names()\n",
    "\n",
    "    \"\"\" Create new database. \"\"\"\n",
    "    def create_database_if_not_exists(self, database_name: str) -> database.Database:\n",
    "        #check whether database exists\n",
    "        if self.check_database_exists(database_name):\n",
    "            print(f\"Don't create the database '{database_name}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created database '{database_name}'.\")\n",
    "\n",
    "        #return database\n",
    "        return self.client[database_name]\n",
    "    \n",
    "    \"\"\" Create new collection. \"\"\"\n",
    "    def create_collection_if_not_exists(self, database_obj: database.Database, collection: str) -> collection.Collection:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #check whether collection exists\n",
    "        if self.check_collection_exists(database_obj, collection):\n",
    "            print(f\"Don't create the collection '{collection}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created collection '{collection}'.\")\n",
    "\n",
    "        #return collection\n",
    "        return self.client[database_obj.name][collection]\n",
    "    \n",
    "    \"\"\" Insert data \"\"\"\n",
    "    def insert_data(self, collection_obj: collection.Collection, data: list[dict]):\n",
    "        #check params\n",
    "        if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n",
    "            raise TypeError(\"data must be a list of dictionaries!\")\n",
    "        \n",
    "        if not isinstance(collection_obj, collection.Collection):\n",
    "            raise TypeError(\"collection_obj must be a collection.Collection!\")\n",
    "        \n",
    "        #insert data\n",
    "        collection_obj.insert_many(data)\n",
    "\n",
    "        print(f\"Successfully inserted data into collection '{collection_obj.name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "Don't create the database 'artist_database' because it already exists.\n",
      "Don't create the collection 'artist_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\" Convert data to dictionaries. \"\"\"\n",
    "def get_dict_data(csv_path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.to_dict(orient = 'records')\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_mongodb_artist(artist_path: str = '/opt/data/Artist.csv'):\n",
    "    #use mongoDB client\n",
    "    with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "        client = mongoDB_operations(client)\n",
    "        #create artist database\n",
    "        client_artist_database = client.create_database_if_not_exists(database_name= 'artist_database')\n",
    "\n",
    "        #create artist collection\n",
    "        client_artist_collection = client.create_collection_if_not_exists(database_obj = client_artist_database, \n",
    "                                                                          collection = 'artist_collection')\n",
    "\n",
    "        #get data\n",
    "        data = get_dict_data(artist_path)    \n",
    "\n",
    "        #insert artist data\n",
    "        client_artist_insert = client.insert_data(collection_obj = client_artist_collection, data = data)\n",
    "\n",
    "load_mongodb_artist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DateType, FloatType, BooleanType\n",
    "\n",
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',    StringType(), True),\n",
    "                     StructField('Artist_Name',  StringType(), True),\n",
    "                     StructField('Genres',       ArrayType(StringType(), True), True),\n",
    "                     StructField('Followers',    IntegerType(), True),\n",
    "                     StructField('Popularity',   IntegerType(), True),\n",
    "                     StructField('Artist_Image', StringType(), True),\n",
    "                     StructField('Artist_Type',  StringType(), True),\n",
    "                     StructField('External_Url', StringType(), True),\n",
    "                     StructField('Href',         StringType(), True),\n",
    "                     StructField('Artist_Uri',   StringType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)\n",
    "    \n",
    "    \"\"\" Album schema. \"\"\"\n",
    "    album_schema = [StructField('Artist',               StringType(), True),\n",
    "                    StructField('Artist_ID',            StringType(), True),\n",
    "                    StructField('Album_ID',             StringType(), True),\n",
    "                    StructField('Name',                 StringType(), True),\n",
    "                    StructField('Type',                 StringType(), True),\n",
    "                    StructField('Genres',               ArrayType(StringType(), True), True),\n",
    "                    StructField('Label',                StringType(), True),\n",
    "                    StructField('Popularity',           StringType(), True),\n",
    "                    StructField('Available_Markets',    StringType(), True),\n",
    "                    StructField('Release_Date',         DateType(), True),\n",
    "                    StructField('ReleaseDatePrecision', StringType(), True),\n",
    "                    StructField('TotalTracks',          IntegerType(), True),\n",
    "                    StructField('Copyrights',           StringType(), True),\n",
    "                    StructField('Restrictions',         StringType(), True),\n",
    "                    StructField('External_URL',         StringType(), True),\n",
    "                    StructField('Href',                 StringType(), True),\n",
    "                    StructField('Image',                StringType(), True),\n",
    "                    StructField('Uri',                  StringType(), True)]\n",
    "    #Applying struct type\n",
    "    album_schema = StructType(album_schema)\n",
    "\n",
    "    \"\"\" Track schema. \"\"\"\n",
    "    track_schema = [StructField(\"Artists\",           StringType(), True),\n",
    "                    StructField(\"Album_ID\",         StringType(), True),\n",
    "                    StructField(\"Album_Name\",       StringType(), True),\n",
    "                    StructField(\"Track_ID\",         StringType(), True),\n",
    "                    StructField(\"Name\",             StringType(), True),\n",
    "                    StructField(\"Track_Number\",     IntegerType(), True),\n",
    "                    StructField(\"Type\",             StringType(), True),\n",
    "                    StructField(\"AvailableMarkets\", StringType(), True),\n",
    "                    StructField(\"Disc_Number\",      StringType(), True),\n",
    "                    StructField(\"Duration_ms\",      IntegerType(), True),\n",
    "                    StructField(\"Explicit\",         StringType(), True),\n",
    "                    StructField(\"External_urls\",    StringType(), True),\n",
    "                    StructField(\"Href\",             StringType(), True),\n",
    "                    StructField(\"Restrictions\",     StringType(), True),\n",
    "                    StructField(\"Preview_url\",      StringType(), True),\n",
    "                    StructField(\"Uri\",              StringType(), True),\n",
    "                    StructField(\"Is_Local\",         StringType(), True)]\n",
    "    #Applying struct type\n",
    "    track_schema = StructType(track_schema)\n",
    "    \n",
    "    \"\"\" TrackFeature schema. \"\"\"\n",
    "    trackfeature_schema = [StructField(\"Track_ID\",         StringType(), True),\n",
    "                           StructField(\"Danceability\",     FloatType(), True),\n",
    "                           StructField(\"Energy\",           FloatType(), True),\n",
    "                           StructField(\"Key\",              IntegerType(), True),\n",
    "                           StructField(\"Loudness\",         FloatType(), True),\n",
    "                           StructField(\"Mode\",             BooleanType(), True),\n",
    "                           StructField(\"Speechiness\",      FloatType(), True),\n",
    "                           StructField(\"Acousticness\",     FloatType(), True),\n",
    "                           StructField(\"Instrumentalness\", FloatType(), True),\n",
    "                           StructField(\"Liveness\",         FloatType(), True),\n",
    "                           StructField(\"Valence\",          FloatType(), True),\n",
    "                           StructField(\"Tempo\",            FloatType(), True),\n",
    "                           StructField(\"Time_signature\",   IntegerType(), True),\n",
    "                           StructField(\"Track_href\",       StringType(), True),\n",
    "                           StructField(\"Type_Feature\",     StringType(), True),\n",
    "                           StructField(\"Analysis_Url\",     StringType(), True)]\n",
    "    #Applying struct type\n",
    "    trackfeature_schema = StructType(trackfeature_schema)\n",
    "\n",
    "    #mapping\n",
    "    mapping = {\n",
    "        'artist': artist_schema,\n",
    "        'album': album_schema,\n",
    "        'track': track_schema,\n",
    "        'trackfeature': trackfeature_schema\n",
    "    }\n",
    "    \n",
    "    #return schema\n",
    "    return mapping[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkConf\n",
    "from contextlib import contextmanager\n",
    "import pyspark.sql\n",
    "\n",
    "\"\"\" Context manager for creating Spark Session. \"\"\"\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local[4]'):\n",
    "    #declare sparkconf\n",
    "    conf = SparkConf()\n",
    "\n",
    "    #set config\n",
    "    conf = conf.setAppName(appName) \\\n",
    "               .setMaster(master) \\\n",
    "               .set(\"spark.executor.memory\", \"4g\") \\\n",
    "               .set(\"spark.executor.cores\", \"4\") \\\n",
    "               .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "               .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "    \n",
    "    #create Spark Session\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully created Spark Session with app name: {appName} and master: {master}!\")\n",
    "\n",
    "    #yield spark\n",
    "    try:\n",
    "        yield spark\n",
    "\n",
    "    finally:\n",
    "        #must stop Spark Session\n",
    "        spark.stop()\n",
    "        print(\"Successfully stopped Spark Session!\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from mongoDB. \"\"\"\n",
    "def read_mongoDB(spark: SparkSession, database_name: str, collection_name: str, schema: StructType = None,\n",
    "                 username: str = 'huynhthuan', password: str = 'password', \n",
    "                 host: str = 'mongo', port: str = 27017) -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if schema is not None and not isinstance(schema, StructType):\n",
    "        raise TypeError(\"schema must be a StructType!\")\n",
    "    \n",
    "    #uri mongoDB \n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "\n",
    "    print(f\"Starting to read data from database '{database_name}' and collection '{collection_name}'...\")\n",
    "  \n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format('mongodb') \\\n",
    "                         .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                         .option('header', 'true')\n",
    "        \n",
    "        data = data.schema(schema).load() if schema is not None else data.load()\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception:\n",
    "        print(\"An error occurred while reading data from mongoDB.\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from HDFS. \"\"\"\n",
    "def read_HDFS(spark: SparkSession, HDFS_dir: str, file_type: str) -> pyspark.sql.DataFrame:\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    #set HDFS path\n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{HDFS_dir}\"\n",
    "\n",
    "    print(f\"Starting to read data from {HDFS_path}...\")\n",
    "\n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format(file_type).option('header', 'true').load(HDFS_path)\n",
    "        #return data\n",
    "        return data\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"An error occurred while reading data from HDFS.\")\n",
    "\n",
    "\n",
    "\"\"\" Write data into HDFS. \"\"\"\n",
    "def write_HDFS(spark: SparkSession, data: pyspark.sql.DataFrame, direct: str, file_type: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "\n",
    "    #set HDFS path  \n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{direct}\"\n",
    "    table_name = direct.split('/')[-1]\n",
    "\n",
    "    print(f\"Starting to upload '{table_name}' into {HDFS_path}...\")\n",
    "    \n",
    "    #write data\n",
    "    try:\n",
    "        data.write.format(file_type) \\\n",
    "                  .option('header', 'true') \\\n",
    "                  .mode('append') \\\n",
    "                  .save(HDFS_path)\n",
    "        \n",
    "        print(f\"Successfully uploaded '{table_name}' into HDFS.\")\n",
    "\n",
    "    except Exception:\n",
    "        print(\"An error occurred while upload data into HDFS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: init_load and master: local[4]!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load all csv files into mongoDB.\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(appName = \"init_load\") as spark:\n",
    "        #uri\n",
    "        uri_artist_name = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_name_collection?authSource=admin\"\n",
    "        uri_artist = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_collection?authSource=admin\"\n",
    "        uri_album = \"mongodb://huynhthuan:password@mongo:27017/music_database.album_collection?authSource=admin\"\n",
    "        uri_track = \"mongodb://huynhthuan:password@mongo:27017/music_database.track_collection?authSource=admin\"\n",
    "        uri_trackfeature = \"mongodb://huynhthuan:password@mongo:27017/music_database.trackfeature_collection?authSource=admin\"\n",
    "\n",
    "        # read\n",
    "        df_ArtistName = spark.read.option('header', 'true').csv(\"/opt/data/ArtistName.csv\")\n",
    "        df_Artist = spark.read.option('header', 'true').csv(\"/opt/data/Artist.csv\")\n",
    "        df_Album = spark.read.option('header', 'true').csv(\"/opt/data/Album.csv\")\n",
    "        df_Track = spark.read.option('header', 'true').csv(\"/opt/data/Track.csv\")\n",
    "        df_TrackFeature = spark.read.option('header', 'true').csv(\"/opt/data/TrackFeature.csv\")\n",
    "\n",
    "\n",
    "        #write\n",
    "        df_ArtistName.write.format('mongoDB') \\\n",
    "                           .option(\"spark.mongodb.write.connection.uri\", uri_artist_name) \\\n",
    "                           .mode(\"overwrite\") \\\n",
    "                           .save()\n",
    "        \n",
    "        df_Artist.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_artist) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Album.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_album) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Track.write.format('mongoDB') \\\n",
    "                      .option(\"spark.mongodb.write.connection.uri\", uri_track) \\\n",
    "                      .mode(\"overwrite\") \\\n",
    "                      .save()\n",
    "        \n",
    "        df_TrackFeature.write.format('mongoDB') \\\n",
    "                             .option(\"spark.mongodb.write.connection.uri\", uri_trackfeature) \\\n",
    "                             .mode(\"overwrite\") \\\n",
    "                             .save()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: Bronze_task and master: local[4]!\n",
      "Starting to read data from database 'music_database' and collection 'artist_collection'...\n",
      "Starting to upload 'bronze_artist' into hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_artist' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'album_collection'...\n",
      "Starting to upload 'bronze_album' into hdfs://namenode:9000/datalake/bronze_data/bronze_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_album' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'track_collection'...\n",
      "Starting to upload 'bronze_track' into hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_track' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'trackfeature_collection'...\n",
      "Starting to upload 'bronze_track_feature' into hdfs://namenode:9000/datalake/bronze_data/bronze_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_track_feature' into HDFS.\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, get_json_object, to_date, regexp_replace, length\n",
    "\n",
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_task():\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task') as spark:\n",
    "        \"\"\" Artist data. \"\"\"\n",
    "        artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "        #preprocessing before loading data\n",
    "        artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                 .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "                                 .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                 .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "        #reorder columns after reading \n",
    "        artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "                                         'Followers', 'Popularity', 'Artist_Image', \n",
    "                                         'Artist_Type', 'External_Url', 'Href', 'Artist_Uri')\n",
    "        #applying schema        \n",
    "        artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = artist_data, direct = 'bronze_data/bronze_artist', file_type = 'parquet')\n",
    "\n",
    "\n",
    "        \"\"\" Album data. \"\"\"\n",
    "        album_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'album_collection')\n",
    "        album_data = album_data.withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                               .withColumn('Release_Date', to_date('Release_Date', \"MM/dd/yyyy\")) \\\n",
    "                               .withColumn('TotalTracks', col('TotalTracks').cast('int'))\n",
    "        #reorder columns after reading\n",
    "        album_data = album_data.select('Artist', 'Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', \n",
    "                                       'Label', 'Popularity', 'Available_Markets', 'Release_Date', \n",
    "                                       'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', \n",
    "                                       'External_URL', 'Href', 'Image', 'Uri')\n",
    "        album_data = spark.createDataFrame(album_data.rdd, schema = get_schema('album'))\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = album_data, direct = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "        \"\"\" Track data. \"\"\"\n",
    "        track_data = read_mongoDB(spark, database_name = 'music_database', \n",
    "                                  collection_name = 'track_collection', schema = get_schema('track'))\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_data, direct = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "\n",
    "        \"\"\" Track feature data. \"\"\"\n",
    "        track_feature_data = read_mongoDB(spark, database_name = 'music_database', \n",
    "                                          collection_name = 'trackfeature_collection', schema = get_schema('track'))\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_feature_data, direct = 'bronze_data/bronze_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "bronze_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import explode_outer, ltrim\n",
    "\n",
    "\"\"\" Create SilverLayer class to process data in the Silver layer. \"\"\"\n",
    "class SilverLayer:\n",
    "    #init \n",
    "    def __init__(self, data: pyspark.sql.DataFrame, \n",
    "                 drop_columns: list = None, \n",
    "                 drop_null_columns: list = None,\n",
    "                 fill_nulls_columns: dict = None,\n",
    "                 nested_columns: list = None,\n",
    "                 duplicate_columns: list = None,\n",
    "                 rename_columns: dict = None,\n",
    "                 ):\n",
    "        \n",
    "        #check valid params\n",
    "        if data is not None and not isinstance(data, pyspark.sql.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        if drop_columns is not None and not isinstance(drop_columns, list):\n",
    "            raise TypeError(\"drop_columns must be a list!\")\n",
    "        \n",
    "        if drop_null_columns is not None and not isinstance(drop_null_columns, list):\n",
    "            raise TypeError(\"drop_null_columns must be a list!\")\n",
    "        \n",
    "        if fill_nulls_columns is not None and not isinstance(fill_nulls_columns, dict):\n",
    "            raise TypeError(\"handle_nulls must be a dict!\")\n",
    "        \n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"handle_nested must be a list!\")\n",
    "        \n",
    "        if duplicate_columns is not None and not isinstance(duplicate_columns, list):\n",
    "            raise TypeError(\"duplicate_columns must be a list!\")\n",
    "        \n",
    "        if rename_columns is not None and not isinstance(rename_columns, dict):\n",
    "            raise TypeError(\"rename_columns must be a dict!\")\n",
    "        \"\"\"Initialize class attributes for data processing.\"\"\"\n",
    "        self._data = data\n",
    "        self._drop_columns = drop_columns\n",
    "        self._drop_null_columns = drop_null_columns\n",
    "        self._fill_nulls_columns = fill_nulls_columns\n",
    "        self._nested_columns = nested_columns\n",
    "        self._duplicate_columns = duplicate_columns\n",
    "        self._rename_columns = rename_columns\n",
    "\n",
    "\n",
    "    \"\"\" Method to drop unnecessary columns. \"\"\"\n",
    "    def drop(self):\n",
    "        self._data = self._data.drop(*self._drop_columns)\n",
    "\n",
    "    \n",
    "    \"\"\" Method to drop rows based on null values in each column. \"\"\"\n",
    "    def drop_null(self):\n",
    "        self._data = self._data.dropna(subset = self._drop_null_columns, how = \"all\")\n",
    "\n",
    "    \n",
    "    \"\"\" Method to fill null values. \"\"\"\n",
    "    def fill_null(self):\n",
    "        for column_list, value in self._fill_nulls_columns.items():\n",
    "            self._data = self._data.fillna(value = value, subset = column_list)\n",
    "\n",
    "\n",
    "    \"\"\" Method to rename columns. \"\"\"\n",
    "    def rename(self):\n",
    "        for old_name, new_name in self._rename_columns.items():\n",
    "            self._data = self._data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "\n",
    "    \"\"\" Method to handle nested. \"\"\"\n",
    "    def handle_nested(self):\n",
    "        for column in self._nested_columns:\n",
    "            self._data = self._data.withColumn(column, explode_outer(column)) \\\n",
    "                                   .withColumn(column, ltrim(column))\n",
    "    \n",
    "\n",
    "    \"\"\" Method to handle duplicates. \"\"\"\n",
    "    def handle_duplicate(self):\n",
    "        self._data = self._data.dropDuplicates(self._duplicate_columns)\n",
    "    \n",
    "\n",
    "    \"\"\" Main processing. \"\"\"\n",
    "    def process(self) -> pyspark.sql.DataFrame:\n",
    "        #drop unnecessary columns\n",
    "        if self._drop_columns:\n",
    "            self.drop()\n",
    "\n",
    "        #drop null rows\n",
    "        self.drop_null()\n",
    "\n",
    "        #fill null values\n",
    "        if self._fill_nulls_columns:\n",
    "            self.fill_null()\n",
    "        \n",
    "        #handle nested columns \n",
    "        if self._nested_columns:\n",
    "            self.handle_nested()\n",
    "        \n",
    "        #handle duplicate rows\n",
    "        self.handle_duplicate()\n",
    "\n",
    "        #rename columns\n",
    "        if self._rename_columns:\n",
    "            self.rename()\n",
    "\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: silver_task and master: local[4]!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+-----------+-----------+--------+--------------------+--------------------+------------+--------------------+--------------------+--------+\n",
      "|             Artists|            Album_ID|          Album_Name|            Track_ID|                Name|Track_Number| Type|    AvailableMarkets|Disc_Number|Duration_ms|Explicit|       External_urls|                Href|Restrictions|         Preview_url|                 Uri|Is_Local|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+-----------+-----------+--------+--------------------+--------------------+------------+--------------------+--------------------+--------+\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|3kTLvsR1WpPswVo9G...|Devocion - Extend...|           9|track|AR,AU,AT,BE,BO,BR...|          1|     235200|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:3kT...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|4siF381fut2Mm19Wr...|Prisionera - Rino...|          10|track|AR,AU,AT,BE,BO,BR...|          1|     260702|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:4si...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|4gVq0YdChR2BagJay...|Prisionera - Ian ...|          11|track|AR,AU,AT,BE,BO,BR...|          1|     168000|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:4gV...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|1GRu8RY6Cj54r8nGP...|Prisionera - Ian ...|          12|track|AR,AU,AT,BE,BO,BR...|          1|     265142|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:1GR...|   False|\n",
      "|  Otilia, Ian Burlak|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|76yCLD1BHofRMPUqX...|Sleepless Nights ...|          13|track|AR,AU,AT,BE,BO,BR...|          1|     324000|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:76y...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|3OtMA6GdDCteLCAuV...|Wine My Body - Ni...|          14|track|AR,AU,AT,BE,BO,BR...|          1|     182465|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:3Ot...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|1eVQDv13nirAQxoBR...|Wine My Body - Ni...|          15|track|AR,AU,AT,BE,BO,BR...|          1|     266939|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:1eV...|   False|\n",
      "|  Otilia, Deejay Fly|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|6SGJ6LCev0BgR3HE5...|I Don't Know - MD...|          16|track|AR,AU,AT,BE,BO,BR...|          1|     237693|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:6SG...|   False|\n",
      "|  Otilia, Alex Spite|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|69rXJS4Xm4m6qVpCx...|Deli Gibi - Alex ...|           1|track|AR,AU,AT,BE,BO,BR...|          1|     258734|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:69r...|   False|\n",
      "|  Otilia, Anas Otman|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|0xKcNnMpSwLaI6xF2...|Deli Gibi - Anas ...|           2|track|AR,AU,AT,BE,BO,BR...|          1|     187031|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:0xK...|   False|\n",
      "|Otilia, Anton Shi...|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|6PuwMTTNVLhYN7V05...|Deli Gibi - Anton...|           3|track|AR,AU,AT,BE,BO,BR...|          1|     247771|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:6Pu...|   False|\n",
      "|      Otilia, Darriz|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|5RY8YkIE7lY7qOzCZ...|Deli Gibi - Darri...|           4|track|AR,AU,AT,BE,BO,BR...|          1|     158041|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:5RY...|   False|\n",
      "|   Otilia, DJ Lidman|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|6HROHDNdTqDx0jntM...|Deli Gibi - Dj Li...|           5|track|AR,AU,AT,BE,BO,BR...|          1|     213133|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:6HR...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|2wkly9H7JsF38Ozu1...|Deli Gibi - DJ Ph...|           6|track|AR,AU,AT,BE,BO,BR...|          1|     309681|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:2wk...|   False|\n",
      "|  Otilia, Eric Deray|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|6qPpTnNNkmiYK1SLn...|Deli Gibi - Eric ...|           7|track|AR,AU,AT,BE,BO,BR...|          1|     305397|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:6qP...|   False|\n",
      "|   Otilia, FREEZONES|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|3AYjpBsTBySkWWcck...|Deli Gibi - Freez...|           8|track|AR,AU,AT,BE,BO,BR...|          1|     214935|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:3AY...|   False|\n",
      "|Otilia, Harun Yıldız|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|7lpLVLSsXlLuSmZIh...|Deli Gibi - Harun...|           9|track|AR,AU,AT,BE,BO,BR...|          1|     180705|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:7lp...|   False|\n",
      "|       Otilia, Maxun|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|5J3L7PN73zPgfrZyO...|Deli Gibi - Maxun...|          10|track|AR,AU,AT,BE,BO,BR...|          1|     165172|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:5J3...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|6itXlaee0WWfSLWxM...|Deli Gibi (OLMEGA...|          11|track|AR,AU,AT,BE,BO,BR...|          1|     113280|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:6it...|   False|\n",
      "|              Otilia|3tRo0I4L5FBZOsAXm...|Manattinile Mantr...|7MG2yZHVdeGnAkzZS...|Deli Gibi (Party ...|          12|track|AR,AU,AT,BE,BO,BR...|          1|     179002|   False|https://open.spot...|https://api.spoti...|        null|https://p.scdn.co...|spotify:track:7MG...|   False|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+-----------+-----------+--------+--------------------+--------------------+------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "with get_sparkSession(\"silver_task\") as spark:\n",
    "    \"\"\" Silver artist data. \"\"\"\n",
    "    # #read bronze artist data\n",
    "    # bronze_artist = read_HDFS(spark, HDFS_dir = \"bronze_data/bronze_artist\", file_type = 'parquet')\n",
    "    # #applying SilverLayer class \n",
    "    # silver_artist = SilverLayer(data = bronze_artist, \n",
    "    #                             drop_columns       = ['Artist_Type', 'Href', 'Artist_Uri'],\n",
    "    #                             drop_null_columns  = ['Artist_ID'], \n",
    "    #                             fill_nulls_columns = {'Followers': 0,\n",
    "    #                                                   'Popularity': 0},\n",
    "    #                             nested_columns     = ['Genres'],\n",
    "    #                             rename_columns     = {'Artist_ID': 'id',\n",
    "    #                                                   'Artist_Name': 'name',\n",
    "    #                                                   'Genres': 'genres',\n",
    "    #                                                   'Followers': 'follower',\n",
    "    #                                                   'Popularity': 'popularity',\n",
    "    #                                                   'Artist_Image': 'link_image',\n",
    "    #                                                   'External_Url': 'url'})\n",
    "    # #processing data\n",
    "    # print(\"Processing for 'silver_artist' ...\")\n",
    "    # silver_artist = silver_artist.process()\n",
    "    # print(\"Finished processing for 'silver_artist'.\")\n",
    "    # #load data into HDFS\n",
    "    # write_HDFS(spark, data = silver_artist, direct = \"silver_data/silver_artist\", file_type = 'parquet')\n",
    "    \n",
    "\n",
    "    # \"\"\" Silver album data. \"\"\"\n",
    "    # #read bronze album data\n",
    "    # bronze_album = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "    # #applying Silver Layer class\n",
    "    # silver_album = SilverLayer(data = bronze_album,\n",
    "    #                            drop_columns = ['Genres', 'Available_Markets', 'Restrictions', 'Href','Uri'],\n",
    "    #                            drop_null_columns = ['Album_ID'],\n",
    "    #                            fill_nulls_columns = {'Popularity': 0,\n",
    "    #                                                  'TotalTracks': 0},\n",
    "                               \n",
    "    #                            rename_columns = {'Artist': 'artist',\n",
    "    #                                              'Artist_ID': 'artist_id',\n",
    "    #                                              'Album_ID': 'id',\n",
    "    #                                              'Name': 'name',\n",
    "    #                                              'Type': 'type',\n",
    "    #                                              'Label': 'label',\n",
    "    #                                              'Popularity': 'popularity',\n",
    "    #                                              'Release_Date': 'release_date',\n",
    "    #                                              'ReleaseDatePrecision': 'release_date_precision',\n",
    "    #                                              'TotalTracks': 'total_tracks',\n",
    "    #                                              'Copyrights': 'copyrights',\n",
    "    #                                              'External_URL': 'url',\n",
    "    #                                              'Image': 'link_image'})\n",
    "    # #processing data\n",
    "    # print(\"Processing for 'silver_album' ...\")\n",
    "    # silver_album = silver_album.process()\n",
    "    # print(\"Finished processing for 'silver_album'.\")\n",
    "    # #load data into HDFS\n",
    "    # write_HDFS(spark, data = silver_album, direct = 'silver_data/silver_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Silver track data. \"\"\"\n",
    "    #read bronze track data\n",
    "    bronze_track = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track = SilverLayer(data = bronze_track,\n",
    "                               drop_columns = ['Artists'])\n",
    "    bronze_track.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local[4]!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_album...\n",
      "+--------------------+----------------------+----------------------+-------------------------------------------------------------------------------+-----+----------------------------+----------+------------+----------------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------+\n",
      "|artist              |artist_id             |id                    |name                                                                           |type |label                       |popularity|release_date|release_date_precision|total_tracks|copyrights                                                                                                                                                                                                                        |url                                                  |link_image                                                      |\n",
      "+--------------------+----------------------+----------------------+-------------------------------------------------------------------------------+-----+----------------------------+----------+------------+----------------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------+\n",
      "|Mi Casa             |6c7bGIcrxaMdYSn6htbHj0|6yoVZbXBz4r9naCq2khg7y|Home Sweet Home                                                                |album|34 MUSIC                    |32        |2015-10-30  |day                   |24          |© 2015 Mi Casa Music, ℗ 2015 Mi Casa Music                                                                                                                                                                                        |https://open.spotify.com/album/6yoVZbXBz4r9naCq2khg7y|https://i.scdn.co/image/ab67616d0000b2734b1fce6314357ab3f02e07f0|\n",
      "|Kevin Kern          |0I8nAaq5bLnTm3aEvr8rIB|6thsZ6tLss7dZtyE1jYEBM|Summer Daydreams                                                               |album|Real Music                  |38        |1998-06-01  |day                   |10          |1998 Real Music, 1998 Real Music                                                                                                                                                                                                  |https://open.spotify.com/album/6thsZ6tLss7dZtyE1jYEBM|https://i.scdn.co/image/ab67616d0000b2731c72abad8e127788974a4afd|\n",
      "|Kaeyra              |3LRqB4U9moDI5yO6e4NrEG|3TjibeLBWi8Lmf2zKBS4pQ|Fountains of Gold                                                              |album|KAEYRA                      |13        |2017-08-25  |day                   |7           |2017 KAEYRA, 2017 KAEYRA                                                                                                                                                                                                          |https://open.spotify.com/album/3TjibeLBWi8Lmf2zKBS4pQ|https://i.scdn.co/image/ab67616d0000b2732df49005e6f0bd4f9dac551d|\n",
      "|Jean-Jacques Goldman|2Cx19OTMqa6gpz2l60cGG2|4fVljX1Zfv9ofFbR9I8BW6|Un tour ensemble (Live)                                                        |album|JRG Productions             |41        |2003-06-02  |day                   |22          |(P) 2003 JRG                                                                                                                                                                                                                      |https://open.spotify.com/album/4fVljX1Zfv9ofFbR9I8BW6|https://i.scdn.co/image/ab67616d0000b27376046f61894a1ff479afb9cd|\n",
      "|Jérôme Rebotier     |5LDz41tmX7odOCzOzwuqGd|7tBVGy08Q7mchuRRzJqgEa|Sahara (Pierre Coré's Original Motion Picture Soundtrack)                      |album|Studiocanal                 |19        |2017-02-01  |day                   |23          |2016 Mandarin Production / La Station Animation / Studiocanal, 2016 Mandarin Production / La Station Animation                                                                                                                    |https://open.spotify.com/album/7tBVGy08Q7mchuRRzJqgEa|https://i.scdn.co/image/ab67616d0000b273d19163bd6d5635688ddd18de|\n",
      "|Philipp Kopachevsky |1aAhadIgMiwXExJGwHCVr1|7s3ss5FMRCMdodtWLfdaHW|20th Century Piano Sonatas, Vol. 1                                             |album|Brilliant Classics          |0         |2020-09-12  |day                   |8           |2020 Brilliant Classics, 2020 Brilliant Classics                                                                                                                                                                                  |https://open.spotify.com/album/7s3ss5FMRCMdodtWLfdaHW|https://i.scdn.co/image/ab67616d0000b2739a1fdbff5a7480ad90e4368f|\n",
      "|Runtown             |6mMtnxEQkYoY5FfJIQ9Rhb|5eI3LjMIOoQZ80sNurXaGL|Signs                                                                          |album|Soundgod Music              |50        |2022-12-16  |day                   |12          |2023 Soundgod Music, 2023 Soundgod Music                                                                                                                                                                                          |https://open.spotify.com/album/5eI3LjMIOoQZ80sNurXaGL|https://i.scdn.co/image/ab67616d0000b273ccdf72237e81ad78f96f8bc6|\n",
      "|La Quinta Estacion  |7FZj349hdLfD6qzXkJLuAh|2sNWP3RYvaA9jp2vYe3gr2|Primera Toma                                                                   |album|Ariola                      |51        |null        |year                  |13          |(P) 2002 BMG Entertainment Mexico, S.A. De C.V.                                                                                                                                                                                   |https://open.spotify.com/album/2sNWP3RYvaA9jp2vYe3gr2|https://i.scdn.co/image/ab67616d0000b27329995dfdab00737a7b504dac|\n",
      "|Bad Bunny           |4q3ewBCX7sLwd24euuV69X|3RQQmkQEvNCY4prGKE6oc5|Un Verano Sin Ti                                                               |album|Rimas Entertainment LLC     |93        |2022-05-06  |day                   |23          |(C) 2022 Rimas Entertainment, (P) 2022 Rimas Entertainment                                                                                                                                                                        |https://open.spotify.com/album/3RQQmkQEvNCY4prGKE6oc5|https://i.scdn.co/image/ab67616d0000b27349d694203245f241a1bcaa72|\n",
      "|Tiakola             |3vUMXQ9kPnZAQkMkZZ7Hfh|3YjNC4de1PEvhuyZjMyxJU|Mélo                                                                           |album|Wati-B                      |72        |2022-05-27  |day                   |16          |(P) 2022 Wati-B                                                                                                                                                                                                                   |https://open.spotify.com/album/3YjNC4de1PEvhuyZjMyxJU|https://i.scdn.co/image/ab67616d0000b2731bbc72ff946f55d4d3188062|\n",
      "|Huncho Jack         |6extd4B6hl8VTmnlhpl2bY|6FED8aeieEnUWwQqAO9zT1|Huncho Jack, Jack Huncho                                                       |album|Quality Control Music - Epic|66        |2017-12-21  |day                   |13          |© 2017 Quality Control Music, LLC, ℗ 2017 Quality Control Music, LLC                                                                                                                                                              |https://open.spotify.com/album/6FED8aeieEnUWwQqAO9zT1|https://i.scdn.co/image/ab67616d0000b273631973893e94d01cfcdf0e5e|\n",
      "|Post Malone         |246dkjvS1zLTtiykXe5h60|6trNtQUgC8cgbWcqoMYkOR|beerbongs & bentleys                                                           |album|Republic Records            |83        |2018-04-27  |day                   |18          |© 2018 Republic Records, a division of UMG Recordings, Inc., ℗ 2018 Republic Records, a division of UMG Recordings, Inc.                                                                                                          |https://open.spotify.com/album/6trNtQUgC8cgbWcqoMYkOR|https://i.scdn.co/image/ab67616d0000b273b1c4b76e23414c9f20242268|\n",
      "|Lazza               |0jdNdfi4vAuVi7a6cPDFBM|7qMFX6YMY6dhl4OWzve4ty|SIRIO                                                                          |album|Universal Music Italia srL. |63        |2022-04-07  |day                   |26          |© 2023 Universal Music Italia Srl, ℗ 2023 Universal Music Italia Srl                                                                                                                                                              |https://open.spotify.com/album/7qMFX6YMY6dhl4OWzve4ty|https://i.scdn.co/image/ab67616d0000b273279a772ce72afcc068718423|\n",
      "|Chris Brown         |7bXgB6jMjp9ATFy66eO08Z|2W9Vq2BMrjiK6qGmJ5va7z|Heartbreak On A Full Moon Deluxe Edition: Cuffing Season - 12 Days Of Christmas|album|RCA Records Label           |54        |2017-12-13  |day                   |57          |(P) 2017 RCA Records, a division of Sony Music Entertainment                                                                                                                                                                      |https://open.spotify.com/album/2W9Vq2BMrjiK6qGmJ5va7z|https://i.scdn.co/image/ab67616d0000b2731a450f85d9c9e9bdc9070057|\n",
      "|David Guetta        |1Cs0zKBU1kc0i8ypK3B9ai|4cm27QX0Xqgflgtj5ns470|Listen (Track by Track)                                                        |album|Parlophone (France)         |13        |2014-10-05  |day                   |15          |© 2014 What A Music Ltd, under exclusive licence to Parlophone/Warner Music France, A Warner Music Group Company, ℗ 2014 What A Music Ltd, under exclusive licence to Parlophone/Warner Music France, A Warner Music Group Company|https://open.spotify.com/album/4cm27QX0Xqgflgtj5ns470|https://i.scdn.co/image/ab67616d0000b2731f95d10977fd47e354203248|\n",
      "|Silence Wang        |0PdNEiQ3MsJGCEgE13Tz60|0FPDTYMqrZLTS9o2pGOJQY|大娱乐家                                                                       |album|大象音乐                    |37        |2020-11-20  |day                   |10          |2020 大象音乐, 2020 大象音乐                                                                                                                                                                                                      |https://open.spotify.com/album/0FPDTYMqrZLTS9o2pGOJQY|https://i.scdn.co/image/ab67616d0000b273dc5ea6c638f8eaefb628bd9a|\n",
      "|Giuseppe Vendittelli|1BM9C4ImQ0aa1Vl1FXKWqu|6UwtlnNuA9MAxLWQk2syb7|Il mito dell'opera: Giuseppe Vendittelli (Live Recordings 1974-1995)           |album|Bongiovanni                 |0         |2014-08-01  |day                   |32          |2014 Bongiovanni, 2014 Bongiovanni                                                                                                                                                                                                |https://open.spotify.com/album/6UwtlnNuA9MAxLWQk2syb7|https://i.scdn.co/image/ab67616d0000b27314afba93f1c7face792b035b|\n",
      "|G-Unit              |6evKD5JWJON3qPBJtUEmtY|4S0fxvOfiXJXz12EoUiYo2|T.O.S. (Terminate On Sight)                                                    |album|G-Unit Records              |49        |2008-01-01  |day                   |16          |© 2008 G-Unit Records, LLC, ℗ 2008 G-Unit Records, LLC                                                                                                                                                                            |https://open.spotify.com/album/4S0fxvOfiXJXz12EoUiYo2|https://i.scdn.co/image/ab67616d0000b27371634ea01494838ab462e9bf|\n",
      "|R. D. Burman        |2JSYASbWU5Y0fVpts3Eq7g|1YURDULKvbyKtbxp3AjOxJ|Birthday Celebration of R.D. Burman                                            |album|UME - Global Clearing House |3         |2023-06-13  |day                   |29          |© 2023 UMG Recordings, Inc., ℗ 2023 UMG Recordings, Inc. FP                                                                                                                                                                       |https://open.spotify.com/album/1YURDULKvbyKtbxp3AjOxJ|https://i.scdn.co/image/ab67616d0000b2733d13000925fbec4a472da1a7|\n",
      "|Sergei Prokofiev    |4kHtgiRnpmFIV5Tm4BIs8l|0oCSDXgy6iUJVZnTAOUW7A|Barcarolle                                                                     |album|UME - Global Clearing House |23        |2024-09-28  |day                   |96          |© 2024 UMG Recordings, Inc., ℗ 2024 UMG Recordings, Inc. FP                                                                                                                                                                       |https://open.spotify.com/album/0oCSDXgy6iUJVZnTAOUW7A|https://i.scdn.co/image/ab67616d0000b2736626460c0e630294eb48f0cc|\n",
      "+--------------------+----------------------+----------------------+-------------------------------------------------------------------------------+-----+----------------------------+----------+------------+----------------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "with get_sparkSession('test') as spark:\n",
    "    data = read_HDFS(spark, HDFS_dir = 'silver_data/silver_album', file_type = 'parquet')\n",
    "    data.show(truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
