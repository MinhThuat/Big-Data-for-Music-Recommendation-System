{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, database, collection\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "\"\"\" Context manager for mongoDB connection. \"\"\"\n",
    "@contextmanager\n",
    "def mongoDB_client(username: str, password: str, \n",
    "                    host: str = 'mongo', port: str = 27017):\n",
    "    #set path\n",
    "    path = f\"mongodb://{username}:{password}@{host}:{port}\"\n",
    "    client = None\n",
    "\n",
    "    #init\n",
    "    try:\n",
    "        print(\"Starting connect mongoDB...\")\n",
    "        client = MongoClient(path)\n",
    "        \n",
    "        print(\"Client connected successfully!\")\n",
    "        yield client\n",
    "\n",
    "    #handle error\n",
    "    except ConnectionFailure:\n",
    "        print(\"Connection to mongoDB failed!\")\n",
    "\n",
    "    except OperationFailure:\n",
    "        print(\"Operation failed!\")\n",
    "\n",
    "    #close client\n",
    "    finally:\n",
    "        client.close()\n",
    "        print(\"The connection to MongoDB has stopped!\")\n",
    "\n",
    "\"\"\" Class mongoDB for operations. \"\"\"\n",
    "class mongoDB_operations:\n",
    "    \"\"\" Init \"\"\"\n",
    "    def __init__(self, client: MongoClient):\n",
    "        #check params\n",
    "        if not isinstance(client, MongoClient):\n",
    "            raise TypeError('client must be MongoClient!')\n",
    "        \n",
    "        #set value for class attrs\n",
    "        self.client = client\n",
    "\n",
    "    \"\"\" Check whether the database exists. \"\"\"\n",
    "    def check_database_exists(self, database_name: str) -> bool:\n",
    "        #list database name\n",
    "        return database_name in self.client.list_database_names()\n",
    "\n",
    "    \"\"\" Check whether collection exists. \"\"\"\n",
    "    def check_collection_exists(self, database_obj: database.Database, collection: str) -> bool:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #list collection name\n",
    "        return collection in self.client[database_obj.name].list_collection_names()\n",
    "\n",
    "    \"\"\" Create new database. \"\"\"\n",
    "    def create_database_if_not_exists(self, database_name: str) -> database.Database:\n",
    "        #check whether database exists\n",
    "        if self.check_database_exists(database_name):\n",
    "            print(f\"Don't create the database '{database_name}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created database '{database_name}'.\")\n",
    "\n",
    "        #return database\n",
    "        return self.client[database_name]\n",
    "    \n",
    "    \"\"\" Create new collection. \"\"\"\n",
    "    def create_collection_if_not_exists(self, database_obj: database.Database, collection: str) -> collection.Collection:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #check whether collection exists\n",
    "        if self.check_collection_exists(database_obj, collection):\n",
    "            print(f\"Don't create the collection '{collection}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created collection '{collection}'.\")\n",
    "\n",
    "        #return collection\n",
    "        return self.client[database_obj.name][collection]\n",
    "    \n",
    "    \"\"\" Insert data. \"\"\"\n",
    "    def insert_data(self, database_name: str, collection_name: str, data = pd.DataFrame):\n",
    "        #check params\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        database_obj = self.create_database_if_not_exists(database_name)\n",
    "        collection_obj = self.create_collection_if_not_exists(database_obj, collection_name)\n",
    "        #insert data\n",
    "        data = data.to_dict(orient = 'records')\n",
    "        collection_obj.insert_many(data)\n",
    "\n",
    "        print(f\"Successfully inserted data into collection '{collection_obj.name}'.\")\n",
    "    \n",
    "    \"\"\" Read data. \"\"\"\n",
    "    def read_data(self, database_name: str, collection_name:str, query: dict = None) -> pd.DataFrame:\n",
    "        #check params\n",
    "        if query is not None and not isinstance(query, dict):\n",
    "            raise TypeError(\"query must be a dict!\")\n",
    "        \n",
    "        #check database and collection exist\n",
    "        if not self.check_database_exists(database_name):\n",
    "            raise Exception(f\"Database '{database_name}' does not exist!\")\n",
    "        if not self.check_collection_exists(database_obj = self.client[database_name], collection = collection_name):\n",
    "            raise Exception(f\"Collection '{collection_name}' does not exist!\")\n",
    "        \n",
    "\n",
    "        data = self.client[database_name][collection_name].find(query)\n",
    "        data = pd.DataFrame(list(data))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "Don't create the database 'artist_database' because it already exists.\n",
      "Don't create the collection 'artist_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\" Convert data to dictionaries. \"\"\"\n",
    "def get_dict_data(csv_path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.to_dict(orient = 'records')\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_mongodb_artist(artist_path: str = '/opt/data/Artist.csv'):\n",
    "    #use mongoDB client\n",
    "    with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "        client = mongoDB_operations(client)\n",
    "        #create artist database\n",
    "        client_artist_database = client.create_database_if_not_exists(database_name= 'artist_database')\n",
    "\n",
    "        #create artist collection\n",
    "        client_artist_collection = client.create_collection_if_not_exists(database_obj = client_artist_database, \n",
    "                                                                          collection = 'artist_collection')\n",
    "\n",
    "        #get data\n",
    "        data = get_dict_data(artist_path)    \n",
    "\n",
    "        #insert artist data\n",
    "        client_artist_insert = client.insert_data(collection_obj = client_artist_collection, data = data)\n",
    "\n",
    "load_mongodb_artist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DateType, FloatType\n",
    "\n",
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',      StringType(), True),\n",
    "                     StructField('Artist_Name',    StringType(), True),\n",
    "                     StructField('Genres',         ArrayType(StringType(), True), True),\n",
    "                     StructField('Followers',      IntegerType(), True),\n",
    "                     StructField('Popularity',     IntegerType(), True),\n",
    "                     StructField('Artist_Image',   StringType(), True),\n",
    "                     StructField('Artist_Type',    StringType(), True),\n",
    "                     StructField('External_Url',   StringType(), True),\n",
    "                     StructField('Href',           StringType(), True),\n",
    "                     StructField('Artist_Uri',     StringType(), True),\n",
    "                     StructField('Execution_date',  DateType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)\n",
    "    \n",
    "    \"\"\" Album schema. \"\"\"\n",
    "    album_schema = [StructField('Artist',               StringType(), True),\n",
    "                    StructField('Artist_ID',            StringType(), True),\n",
    "                    StructField('Album_ID',             StringType(), True),\n",
    "                    StructField('Name',                 StringType(), True),\n",
    "                    StructField('Type',                 StringType(), True),\n",
    "                    StructField('Genres',               ArrayType(StringType(), True), True),\n",
    "                    StructField('Label',                StringType(), True),\n",
    "                    StructField('Popularity',           StringType(), True),\n",
    "                    StructField('Available_Markets',    StringType(), True),\n",
    "                    StructField('Release_Date',         DateType(), True),\n",
    "                    StructField('ReleaseDatePrecision', StringType(), True),\n",
    "                    StructField('TotalTracks',          IntegerType(), True),\n",
    "                    StructField('Copyrights',           StringType(), True),\n",
    "                    StructField('Restrictions',         StringType(), True),\n",
    "                    StructField('External_URL',         StringType(), True),\n",
    "                    StructField('Href',                 StringType(), True),\n",
    "                    StructField('Image',                StringType(), True),\n",
    "                    StructField('Uri',                  StringType(), True),\n",
    "                    StructField('Execution_date',        DateType(), True)]\n",
    "    #Applying struct type\n",
    "    album_schema = StructType(album_schema)\n",
    "\n",
    "    \"\"\" Track schema. \"\"\"\n",
    "    track_schema = [StructField(\"Artists\",          StringType(), True),\n",
    "                    StructField(\"Album_ID\",         StringType(), True),\n",
    "                    StructField(\"Album_Name\",       StringType(), True),\n",
    "                    StructField(\"Track_ID\",         StringType(), True),\n",
    "                    StructField(\"Name\",             StringType(), True),\n",
    "                    StructField(\"Track_Number\",     IntegerType(), True),\n",
    "                    StructField(\"Type\",             StringType(), True),\n",
    "                    StructField(\"AvailableMarkets\", StringType(), True),\n",
    "                    StructField(\"Disc_Number\",      StringType(), True),\n",
    "                    StructField(\"Duration_ms\",      IntegerType(), True),\n",
    "                    StructField(\"Explicit\",         StringType(), True),\n",
    "                    StructField(\"External_urls\",    StringType(), True),\n",
    "                    StructField(\"Href\",             StringType(), True),\n",
    "                    StructField(\"Restrictions\",     StringType(), True),\n",
    "                    StructField(\"Preview_url\",      StringType(), True),\n",
    "                    StructField(\"Uri\",              StringType(), True),\n",
    "                    StructField(\"Is_Local\",         StringType(), True),\n",
    "                    StructField('Execution_date',   DateType(), True)]\n",
    "    #Applying struct type\n",
    "    track_schema = StructType(track_schema)\n",
    "    \n",
    "    \"\"\" TrackFeature schema. \"\"\"\n",
    "    trackfeature_schema = [StructField(\"Track_ID\",         StringType(), True),\n",
    "                           StructField(\"Danceability\",     FloatType(), True),\n",
    "                           StructField(\"Energy\",           FloatType(), True),\n",
    "                           StructField(\"Key\",              IntegerType(), True),\n",
    "                           StructField(\"Loudness\",         FloatType(), True),\n",
    "                           StructField(\"Mode\",             IntegerType(), True),\n",
    "                           StructField(\"Speechiness\",      FloatType(), True),\n",
    "                           StructField(\"Acousticness\",     FloatType(), True),\n",
    "                           StructField(\"Instrumentalness\", FloatType(), True),\n",
    "                           StructField(\"Liveness\",         FloatType(), True),\n",
    "                           StructField(\"Valence\",          FloatType(), True),\n",
    "                           StructField(\"Tempo\",            FloatType(), True),\n",
    "                           StructField(\"Time_signature\",   IntegerType(), True),\n",
    "                           StructField(\"Track_href\",       StringType(), True),\n",
    "                           StructField(\"Type_Feature\",     StringType(), True),\n",
    "                           StructField(\"Analysis_Url\",     StringType(), True),\n",
    "                           StructField('Execution_date',    DateType(), True)]\n",
    "    #Applying struct type\n",
    "    trackfeature_schema = StructType(trackfeature_schema)\n",
    "\n",
    "    #mapping\n",
    "    mapping = {\n",
    "        'artist': artist_schema,\n",
    "        'album': album_schema,\n",
    "        'track': track_schema,\n",
    "        'trackfeature': trackfeature_schema\n",
    "    }\n",
    "    \n",
    "    #return schema\n",
    "    return mapping[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkConf\n",
    "from contextlib import contextmanager\n",
    "import pyspark.sql\n",
    "\n",
    "\"\"\" Context manager for creating Spark Session. \"\"\"\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    #declare sparkconf         \n",
    "    conf = SparkConf()\n",
    "\n",
    "    #set config\n",
    "    conf = conf.setAppName(appName) \\\n",
    "               .setMaster(master) \\\n",
    "               .set(\"spark.executor.memory\", \"4g\") \\\n",
    "               .set(\"spark.executor.cores\", \"2\") \\\n",
    "               .set(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "               .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "               .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\") \n",
    "               #.set(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4\")\n",
    "    \n",
    "    #               .set(\"spark.executor.instances\", \"2\") \\\n",
    "    #create Spark Session\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully created Spark Session with app name: {appName} and master: {master}!\")\n",
    "\n",
    "    #yield spark\n",
    "    try:\n",
    "        yield spark\n",
    "\n",
    "    finally:\n",
    "        #must stop Spark Session\n",
    "        spark.stop()\n",
    "        print(\"Successfully stopped Spark Session!\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from mongoDB. \"\"\"\n",
    "def read_mongoDB(spark: SparkSession, database_name: str, collection_name: str, chunk_params: list = None,\n",
    "                 schema: StructType = None, username: str = 'huynhthuan', password: str = 'password', \n",
    "                 host: str = 'mongo', port: str = 27017) -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if chunk_params is not None and not isinstance(chunk_params, list):\n",
    "        raise TypeError(\"chunk_params must be a dict!\")\n",
    "    \n",
    "    if schema is not None and not isinstance(schema, StructType):\n",
    "        raise TypeError(\"schema must be a StructType!\")\n",
    "    \n",
    "    #uri mongoDB \n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "\n",
    "    print(f\"Starting to read data from database '{database_name}' and collection '{collection_name}'...\")\n",
    "  \n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format('mongodb') \\\n",
    "                         .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                         .option('header', 'true')\n",
    "        \n",
    "        data = data.schema(schema).load() if schema is not None else data.load()\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from mongoDB: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from HDFS. \"\"\"\n",
    "def read_HDFS(spark: SparkSession, HDFS_dir: str, file_type: str) -> pyspark.sql.DataFrame:\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    #set HDFS path\n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{HDFS_dir}\"\n",
    "\n",
    "    print(f\"Starting to read data from {HDFS_path}...\")\n",
    "\n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format(file_type).option('header', 'true').load(HDFS_path)\n",
    "        #return data\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from HDFS: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Write data into HDFS. \"\"\"\n",
    "def write_HDFS(spark: SparkSession, data: pyspark.sql.DataFrame, direct: str, file_type: str, partition_key: str = None):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "\n",
    "    #set HDFS path  \n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{direct}\"\n",
    "    table_name = direct.split('/')[-1]\n",
    "\n",
    "    print(f\"Starting to upload '{table_name}' into {HDFS_path}...\")\n",
    "    \n",
    "    #write data\n",
    "    try:\n",
    "        if partition_key is not None:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode('append') \\\n",
    "                      .partitionBy(partition_key) \\\n",
    "                      .save(HDFS_path)\n",
    "        else:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode('append') \\\n",
    "                      .save(HDFS_path)\n",
    "        print(f\"Successfully uploaded '{table_name}' into HDFS.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "\n",
    "\"\"\" Write data into SnowFlake Data Warehouse. \"\"\"\n",
    "def write_SnowFlake(spark: SparkSession, data: pyspark.sql.DataFrame, table_name: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    \n",
    "    snowflake_connection_options = {\n",
    "        \"sfURL\": \"https://sl70006.southeast-asia.azure.snowflakecomputing.com\",\n",
    "        \"sfUser\": \"HUYNHTHUAN\", \n",
    "        \"sfPassword\": \"Thuan123456\",\n",
    "        \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "        \"sfDatabase\": \"SPOTIFY_MUSIC_DB\" \n",
    "    }\n",
    "\n",
    "    print(f\"Starting to upload {table_name.split('.')[-1]} into SnowFlake...\")\n",
    "    try:\n",
    "        data.write.format(\"snowflake\") \\\n",
    "                .options(**snowflake_connection_options) \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .mode('append') \\\n",
    "                .save()\n",
    "        print(f\"Successfully uploaded '{table_name}' into SnowFlake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e0967f68-534b-4c7b-8942-d315d202c7ec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 7380ms :: artifacts dl 43ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e0967f68-534b-4c7b-8942-d315d202c7ec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/21ms)\n",
      "24/11/21 06:18:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: init_load and master: local!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load all csv files into mongoDB.\"\"\"\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(appName = \"init_load\") as spark:\n",
    "        execution_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        #uri\n",
    "        uri_artist_name = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_name_collection?authSource=admin\"\n",
    "        uri_artist = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_collection?authSource=admin\"\n",
    "        uri_album = \"mongodb://huynhthuan:password@mongo:27017/music_database.album_collection?authSource=admin\"\n",
    "        uri_track = \"mongodb://huynhthuan:password@mongo:27017/music_database.track_collection?authSource=admin\"\n",
    "        uri_trackfeature = \"mongodb://huynhthuan:password@mongo:27017/music_database.trackfeature_collection?authSource=admin\"\n",
    "\n",
    "        # read\n",
    "        df_ArtistName = spark.read.option('header', 'true').csv(\"/opt/data/ArtistName.csv\")\n",
    "        df_ArtistName = df_ArtistName.withColumn('Execution_date', lit(execution_date))\n",
    "        \n",
    "        df_Artist = spark.read.option('header', 'true').csv(\"/opt/data/Artist.csv\")\n",
    "        df_Artist = df_Artist.withColumn('Execution_date', lit(execution_date))\n",
    "\n",
    "        df_Album = spark.read.option('header', 'true').csv(\"/opt/data/Album.csv\")\n",
    "        df_Album = df_Album.withColumn('Execution_date', lit(execution_date))\n",
    "\n",
    "        df_Track = spark.read.option('header', 'true').csv(\"/opt/data/Track.csv\")\n",
    "        df_Track = df_Track.withColumn('Execution_date', lit(execution_date))\n",
    "        \n",
    "        df_TrackFeature = spark.read.option('header', 'true').csv(\"/opt/data/TrackFeature.csv\")\n",
    "        df_TrackFeature = df_TrackFeature.withColumn('Execution_date', lit(execution_date))\n",
    "\n",
    "        #write\n",
    "        df_ArtistName.write.format('mongoDB') \\\n",
    "                           .option(\"spark.mongodb.write.connection.uri\", uri_artist_name) \\\n",
    "                           .mode(\"overwrite\") \\\n",
    "                           .save()\n",
    "        \n",
    "        df_Artist.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_artist) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Album.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_album) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Track.write.format('mongoDB') \\\n",
    "                      .option(\"spark.mongodb.write.connection.uri\", uri_track) \\\n",
    "                      .mode(\"overwrite\") \\\n",
    "                      .save()\n",
    "        \n",
    "        df_TrackFeature.write.format('mongoDB') \\\n",
    "                             .option(\"spark.mongodb.write.connection.uri\", uri_trackfeature) \\\n",
    "                             .mode(\"overwrite\") \\\n",
    "                             .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Bronze task starts! -------------------------------\n",
      "Successfully created Spark Session with app name: Bronze_task and master: local!\n",
      "Starting to read data from database 'music_database' and collection 'artist_collection'...\n",
      "Starting bronze preprocessing for artist data...\n",
      "Finished bronze preprocessing for artist data.\n",
      "Starting to upload 'bronze_artist' into hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_artist' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'album_collection'...\n",
      "Starting bronze preprocessing for album data...\n",
      "Finished bronze preprocessing for album data.\n",
      "Starting to upload 'bronze_album' into hdfs://namenode:9000/datalake/bronze_data/bronze_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_album' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'track_collection'...\n",
      "Starting to upload 'bronze_track' into hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_track' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'trackfeature_collection'...\n",
      "Starting to upload 'bronze_track_feature' into hdfs://namenode:9000/datalake/bronze_data/bronze_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_track_feature' into HDFS.\n",
      "Successfully stopped Spark Session!\n",
      "------------------------------ Bronze task finished! -------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, get_json_object, to_date, regexp_replace, length, current_date\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_task():\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task') as spark:\n",
    "\n",
    "        \"\"\"------------------------ BRONZE ARTIST ------------------------\"\"\"\n",
    "        artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "        print(\"Starting bronze preprocessing for artist data...\")\n",
    "        #preprocessing before loading data\n",
    "        try:\n",
    "            artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                    .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "                                    .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                    .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "            #reorder columns after reading \n",
    "            artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "                                            'Followers', 'Popularity', 'Artist_Image', \n",
    "                                            'Artist_Type', 'External_Url', 'Href', 'Artist_Uri')\n",
    "            #applying schema        \n",
    "            artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "            artist_data = artist_data.withColumn('execution_date', current_date())\n",
    "\n",
    "            print(\"Finished bronze preprocessing for artist data.\")\n",
    "\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = artist_data, direct = 'bronze_data/bronze_artist', file_type = 'parquet', partition_key = 'execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONE ALBUM ------------------------\"\"\"\n",
    "        album_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'album_collection')\n",
    "        print(\"Starting bronze preprocessing for album data...\")\n",
    "        try:\n",
    "            album_data = album_data.withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                .withColumn('Release_Date', to_date('Release_Date', \"MM/dd/yyyy\")) \\\n",
    "                                .withColumn('TotalTracks', col('TotalTracks').cast('int'))\n",
    "            #reorder columns after reading\n",
    "            album_data = album_data.select('Artist', 'Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', \n",
    "                                        'Label', 'Popularity', 'Available_Markets', 'Release_Date', \n",
    "                                        'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', \n",
    "                                        'External_URL', 'Href', 'Image', 'Uri')\n",
    "            album_data = spark.createDataFrame(album_data.rdd, schema = get_schema('album'))\n",
    "            album_data = album_data.withColumn('execution_date', current_date())\n",
    "            print(\"Finished bronze preprocessing for album data.\")\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = album_data, direct = 'bronze_data/bronze_album', file_type = 'parquet', partition_key = 'execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK -------------------------\"\"\"\n",
    "        track_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'track_collection', \n",
    "                                  schema = get_schema('track'))\n",
    "        track_data = track_data.withColumn('execution_date', current_date())\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_data, direct = 'bronze_data/bronze_track', file_type = 'parquet', partition_key = 'execution_date')\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK FEATURE ------------------------\"\"\"\n",
    "        track_feature_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'trackfeature_collection', \n",
    "                                          schema = get_schema('trackfeature'))\n",
    "        track_feature_data = track_feature_data.withColumn('execution_date', current_date())\n",
    "        #upload data into HDFS\n",
    "        write_HDFS(spark, data = track_feature_data, direct = 'bronze_data/bronze_track_feature', file_type = 'parquet', partition_key = 'execution_date')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"------------------------------- Bronze task starts! -------------------------------\")\n",
    "    bronze_task()\n",
    "    print(\"------------------------------ Bronze task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import explode_outer, ltrim\n",
    "\n",
    "\"\"\" Create SilverLayer class to process data in the Silver layer. \"\"\"\n",
    "class SilverLayer:\n",
    "    #init \n",
    "    def __init__(self, data: pyspark.sql.DataFrame, \n",
    "                 drop_columns: list = None, \n",
    "                 drop_null_columns: list = None,\n",
    "                 fill_nulls_columns: dict = None,\n",
    "                 duplicate_columns: list = None,\n",
    "                 nested_columns: list = None,\n",
    "                 rename_columns: dict = None,\n",
    "                 ):\n",
    "        \n",
    "        #check valid params\n",
    "        if data is not None and not isinstance(data, pyspark.sql.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        if drop_columns is not None and not isinstance(drop_columns, list):\n",
    "            raise TypeError(\"drop_columns must be a list!\")\n",
    "        \n",
    "        if drop_null_columns is not None and not isinstance(drop_null_columns, list):\n",
    "            raise TypeError(\"drop_null_columns must be a list!\")\n",
    "        \n",
    "        if fill_nulls_columns is not None and not isinstance(fill_nulls_columns, dict):\n",
    "            raise TypeError(\"handle_nulls must be a dict!\")\n",
    "        \n",
    "        if duplicate_columns is not None and not isinstance(duplicate_columns, list):\n",
    "            raise TypeError(\"duplicate_columns must be a list!\")\n",
    "        \n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"handle_nested must be a list!\")\n",
    "        \n",
    "        if rename_columns is not None and not isinstance(rename_columns, dict):\n",
    "            raise TypeError(\"rename_columns must be a dict!\")\n",
    "        \"\"\"Initialize class attributes for data processing.\"\"\"\n",
    "        self._data = data\n",
    "        self._drop_columns = drop_columns\n",
    "        self._drop_null_columns = drop_null_columns\n",
    "        self._fill_nulls_columns = fill_nulls_columns\n",
    "        self._duplicate_columns = duplicate_columns\n",
    "        self._nested_columns = nested_columns\n",
    "        self._rename_columns = rename_columns\n",
    "\n",
    "\n",
    "    \"\"\" Method to drop unnecessary columns. \"\"\"\n",
    "    def drop(self):\n",
    "        self._data = self._data.drop(*self._drop_columns)\n",
    "\n",
    "    \n",
    "    \"\"\" Method to drop rows based on null values in each column. \"\"\"\n",
    "    def drop_null(self):\n",
    "        self._data = self._data.dropna(subset = self._drop_null_columns, how = \"all\")\n",
    "\n",
    "    \n",
    "    \"\"\" Method to fill null values. \"\"\"\n",
    "    def fill_null(self):\n",
    "        for column_list, value in self._fill_nulls_columns.items():\n",
    "            self._data = self._data.fillna(value = value, subset = column_list)\n",
    "\n",
    "\n",
    "    \"\"\" Method to rename columns. \"\"\"\n",
    "    def rename(self):\n",
    "        for old_name, new_name in self._rename_columns.items():\n",
    "            self._data = self._data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    \"\"\" Method to handle duplicates. \"\"\"\n",
    "    def handle_duplicate(self):\n",
    "        self._data = self._data.dropDuplicates(self._duplicate_columns)\n",
    "\n",
    "    \"\"\" Method to handle nested. \"\"\"\n",
    "    def handle_nested(self):\n",
    "        for column in self._nested_columns:\n",
    "            self._data = self._data.withColumn(column, explode_outer(column)) \\\n",
    "                                   .withColumn(column, ltrim(column))\n",
    "    \n",
    "    \"\"\" Main processing. \"\"\"\n",
    "    def process(self) -> pyspark.sql.DataFrame:\n",
    "        #drop unnecessary columns\n",
    "        if self._drop_columns:\n",
    "            self.drop() \n",
    "\n",
    "        #drop rows contain null values for each col\n",
    "        if self._drop_null_columns:\n",
    "            self.drop_null()\n",
    "\n",
    "        #fill null values\n",
    "        if self._fill_nulls_columns:\n",
    "            self.fill_null()\n",
    "        \n",
    "        #handle duplicate rows\n",
    "        if self._duplicate_columns:\n",
    "            self.handle_duplicate()\n",
    "\n",
    "        #handle nested columns \n",
    "        if self._nested_columns:\n",
    "            self.handle_nested()\n",
    "\n",
    "        #rename columns\n",
    "        if self._rename_columns:\n",
    "            self.rename()\n",
    "\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: silver_task and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n",
      "Processing for 'silver_artist' ...\n",
      "Finished processing for 'silver_artist'.\n",
      "Starting to upload 'silver_artist' into hdfs://namenode:9000/datalake/silver_data/silver_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_artist' into HDFS.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_album...\n",
      "Processing for 'silver_album' ...\n",
      "Finished processing for 'silver_album'.\n",
      "Starting to upload 'silver_album' into hdfs://namenode:9000/datalake/silver_data/silver_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 06:51:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_album' into HDFS.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n",
      "Processing for 'silver_track' ...\n",
      "Finished processing for 'silver_track'.\n",
      "Starting to upload 'silver_track' into hdfs://namenode:9000/datalake/silver_data/silver_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_track' into HDFS.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track_feature...\n",
      "Processing for 'silver_track_feature' ...\n",
      "Finished processing for 'silver_track_feature'.\n",
      "Starting to upload 'silver_track_feature' into hdfs://namenode:9000/datalake/silver_data/silver_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_track_feature' into HDFS.\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "\"\"\" Processing silver artist data. \"\"\"\n",
    "def silver_artist_process(spark):\n",
    "    #read bronze artist data\n",
    "    bronze_artist = read_HDFS(spark, HDFS_dir = \"bronze_data/bronze_artist\", file_type = 'parquet')\n",
    "    #applying SilverLayer class \n",
    "    silver_artist = SilverLayer(data = bronze_artist, \n",
    "                                drop_columns       = ['Artist_Type', 'Href', 'Artist_Uri'],\n",
    "                                drop_null_columns  = ['Artist_ID'], \n",
    "                                fill_nulls_columns = {'Followers': 0,\n",
    "                                                      'Popularity': 0},\n",
    "                                duplicate_columns  = ['Artist_ID'],\n",
    "                                nested_columns     = ['Genres'],\n",
    "                                rename_columns     = {'Artist_ID': 'id',\n",
    "                                                      'Artist_Name': 'name',\n",
    "                                                      'Genres': 'genres',\n",
    "                                                      'Followers': 'followers',\n",
    "                                                      'Popularity': 'popularity',\n",
    "                                                      'Artist_Image': 'link_image',\n",
    "                                                      'External_Url': 'url'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_artist' ...\")\n",
    "    silver_artist = silver_artist.process()\n",
    "    print(\"Finished processing for 'silver_artist'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_artist, direct = \"silver_data/silver_artist\", file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver album data. \"\"\"\n",
    "def silver_album_process(spark):\n",
    "    #read bronze album data\n",
    "    bronze_album = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_album = SilverLayer(data = bronze_album,\n",
    "                               drop_columns       = ['Genres', 'Available_Markets', 'Restrictions', 'Href','Uri'],\n",
    "                               drop_null_columns  = ['Album_ID'],\n",
    "                               fill_nulls_columns = {'Popularity': 0,\n",
    "                                                     'TotalTracks': 0},\n",
    "                               duplicate_columns  = ['Album_ID'],\n",
    "                               rename_columns     = {'Artist': 'artist',\n",
    "                                                     'Artist_ID': 'artist_id',\n",
    "                                                     'Album_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Type': 'type',\n",
    "                                                     'Label': 'label',\n",
    "                                                     'Popularity': 'popularity',\n",
    "                                                     'Release_Date': 'release_date',\n",
    "                                                     'ReleaseDatePrecision': 'release_date_precision',\n",
    "                                                     'TotalTracks': 'total_tracks',\n",
    "                                                     'Copyrights': 'copyrights',\n",
    "                                                     'External_URL': 'url',\n",
    "                                                     'Image': 'link_image'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_album' ...\")\n",
    "    silver_album = silver_album.process()\n",
    "    print(\"Finished processing for 'silver_album'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_album, direct = 'silver_data/silver_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track data. \"\"\"\n",
    "def silver_track_process(spark):\n",
    "    #read bronze track data\n",
    "    bronze_track = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track = SilverLayer(data               = bronze_track,\n",
    "                               drop_columns       = ['Artists', 'Type', 'AvailableMarkets', 'Href', 'Uri', 'Is_Local'],\n",
    "                               drop_null_columns  = ['Track_ID'],\n",
    "                               fill_nulls_columns = {'Restrictions': 'None'},\n",
    "                               duplicate_columns  = ['Track_ID'],\n",
    "                               rename_columns     = {'Album_ID': 'album_id',\n",
    "                                                     'Album_Name': 'album_name',\n",
    "                                                     'Track_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Track_Number': 'track_number',\n",
    "                                                     'Disc_Number': 'disc_number',\n",
    "                                                     'Duration_ms': 'duration_ms',\n",
    "                                                     'Explicit': 'explicit',\n",
    "                                                     'External_urls': 'url',\n",
    "                                                     'Restrictions': 'restriction',\n",
    "                                                     'Preview_url': 'preview'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track' ...\")\n",
    "    silver_track = silver_track.process()\n",
    "    print(\"Finished processing for 'silver_track'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track, direct = 'silver_data/silver_track', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track feature data. \"\"\"\n",
    "def silver_track_feature_process(spark):\n",
    "    #read silver track feature data\n",
    "    bronze_track_feature = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track_feature', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track_feature = SilverLayer(data              = bronze_track_feature,\n",
    "                                       drop_columns      = ['Track_href', 'Type_Feature', 'Analysis_Url'],\n",
    "                                       drop_null_columns = ['Track_ID'],\n",
    "                                       duplicate_columns = ['Track_ID'],\n",
    "                                       rename_columns    = {'Track_ID': 'id',\n",
    "                                                            'Danceability': 'danceability',\n",
    "                                                            'Energy': 'energy',\n",
    "                                                            'Key': 'key',\n",
    "                                                            'Loudness': 'loudness',\n",
    "                                                            'Mode': 'mode',\n",
    "                                                            'Speechiness': 'speechiness',\n",
    "                                                            'Acousticness': 'acousticness',\n",
    "                                                            'Instrumentalness': 'instrumentalness',\n",
    "                                                            'Liveness': 'liveness',\n",
    "                                                            'Valence': 'valence',\n",
    "                                                            'Tempo': 'tempo',\n",
    "                                                            'Time_signature': 'time_signature'})\n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track_feature' ...\")\n",
    "    silver_track_feature = silver_track_feature.process()\n",
    "    print(\"Finished processing for 'silver_track_feature'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track_feature, direct = 'silver_data/silver_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "#main call\n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(\"silver_task_spark\") as spark:\n",
    "        print(\"------------------------------- Silver task starts! -------------------------------\")\n",
    "        print(\"Starting silver artist data processing...\")\n",
    "        silver_artist_process(spark)\n",
    "        print(\"Starting silver album data processing...\")\n",
    "        silver_album_process(spark)\n",
    "        print(\"Starting silver track data processing...\")\n",
    "        silver_track_process(spark)\n",
    "        print(\"Starting silver track feature data processing...\")\n",
    "        silver_track_feature_process(spark)\n",
    "        print(\"------------------------------ Silver task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19c48c5c-0338-4ba2-8822-8ecbb226dfb8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.0/mongo-spark-connector_2.12-10.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.4.0!mongo-spark-connector_2.12.jar (4411ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/5.1.4/mongodb-driver-sync-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;5.1.4!mongodb-driver-sync.jar (4319ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/5.1.4/bson-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;5.1.4!bson.jar (12316ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/5.1.4/mongodb-driver-core-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;5.1.4!mongodb-driver-core.jar (53738ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/5.1.4/bson-record-codec-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;5.1.4!bson-record-codec.jar (731ms)\n",
      ":: resolution report :: resolve 20803ms :: artifacts dl 75540ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-19c48c5c-0338-4ba2-8822-8ecbb226dfb8\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (2522kB/36ms)\n",
      "24/11/21 08:52:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/fact_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=60>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/context.py\", line 377, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/context.py\", line 2255, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o27.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o42.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_sparkSession(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m spark:\n\u001b[1;32m      3\u001b[0m     d \u001b[38;5;241m=\u001b[39m read_HDFS(spark, HDFS_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgold_data/fact_track\u001b[39m\u001b[38;5;124m'\u001b[39m, file_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:901\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    896\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    897\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o42.showString"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "with get_sparkSession('test') as spark:\n",
    "    d = read_HDFS(spark, HDFS_dir = 'gold_data/fact_track', file_type = 'parquet')\n",
    "    d.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: spark_for_gold_task and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_artist...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_album...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_track...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_track_feature...\n",
      "Starting to upload 'dim_genres' into hdfs://namenode:9000/datalake/gold_data/dim_genres...\n",
      "Successfully uploaded 'dim_genres' into HDFS.\n",
      "Starting to upload 'dim_artist' into hdfs://namenode:9000/datalake/gold_data/dim_artist...\n",
      "Successfully uploaded 'dim_artist' into HDFS.\n",
      "Starting to upload 'dim_artist_genres' into hdfs://namenode:9000/datalake/gold_data/dim_artist_genres...\n",
      "Successfully uploaded 'dim_artist_genres' into HDFS.\n",
      "Starting to upload 'dim_album' into hdfs://namenode:9000/datalake/gold_data/dim_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'dim_album' into HDFS.\n",
      "Starting to upload 'dim_track_feature' into hdfs://namenode:9000/datalake/gold_data/dim_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'dim_track_feature' into HDFS.\n",
      "Starting to upload 'fact_track' into hdfs://namenode:9000/datalake/gold_data/fact_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'fact_track' into HDFS.\n",
      "Successfully stopped Spark Session!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" Gold layer. \"\"\"\n",
    "#Handle table individually\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat, col, lit, count\n",
    "with get_sparkSession('spark_for_gold_task') as spark:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #Read data from HDFS\n",
    "    silver_artist = read_HDFS(spark, HDFS_dir = 'silver_data/silver_artist', file_type = 'parquet')\n",
    "    silver_album = read_HDFS(spark, HDFS_dir = 'silver_data/silver_album', file_type = 'parquet')\n",
    "    silver_track = read_HDFS(spark, HDFS_dir = 'silver_data/silver_track', file_type = 'parquet')\n",
    "    silver_track_feature = read_HDFS(spark, HDFS_dir = 'silver_data/silver_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_genres table. \"\"\"\n",
    "    #list all distinct genres in artist table\n",
    "    dim_genres = silver_artist.select('genres', 'execution_date').distinct()\n",
    "    dim_genres = dim_genres.filter(col('genres').isNotNull())\n",
    "    #add primary key\n",
    "    dim_genres = dim_genres.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "                           .withColumn(\"id\", concat(lit(current_day.replace(\"-\", \"\")), col('id')))\n",
    "    #reorder columns\n",
    "    dim_genres = dim_genres.select(\"id\", \"genres\", \"execution_date\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_genres, direct = 'gold_data/dim_genres', file_type = 'parquet')\n",
    "    \n",
    "\n",
    "    \"\"\" Create dim_artist table. \"\"\"\n",
    "    #just drop genres column and distinct row\n",
    "    dim_artist = silver_artist.drop('genres').distinct()\n",
    "    write_HDFS(spark, data = dim_artist, direct = 'gold_data/dim_artist', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_artist_genres table. \"\"\"\n",
    "    #select necessary columns in artist table\n",
    "    dim_artist_genres = silver_artist.select('id', 'genres') \\\n",
    "                                     .withColumnRenamed('id', 'artist_id')\n",
    "    #joining tables to map artist IDs and genre IDs\n",
    "    dim_artist_genres = dim_artist_genres.join(dim_genres, on = 'genres', how = 'inner') \\\n",
    "                                         .withColumnRenamed('id', 'genres_id')\n",
    "    #drop genres column\n",
    "    dim_artist_genres = dim_artist_genres.drop('genres')\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_artist_genres, direct = 'gold_data/dim_artist_genres', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_album table. \"\"\"\n",
    "    #just drop unnecessary columns \n",
    "    dim_album = silver_album.drop('artist', 'artist_id', 'total_tracks', 'release_date_precision')\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_album, direct = 'gold_data/dim_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_track_feature table. \"\"\"\n",
    "    #we don't need to do anything since the dim_track_feature table is complete\n",
    "    dim_track_feature = silver_track_feature\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_track_feature, direct = 'gold_data/dim_track_feature', file_type = 'parquet')\n",
    "\n",
    "    \n",
    "    \"\"\" Create fact_track table. \"\"\"\n",
    "    #drop album name and rename track id column\n",
    "    fact_track = silver_track.drop('album_name') \\\n",
    "                             .withColumnRenamed('id', 'track_id')\n",
    "    #get artist ID from silver album table to create a foreign key for the fact_track table\n",
    "    silver_album = silver_album.select('id', 'artist_id') \\\n",
    "                               .withColumnRenamed('id', 'album_id')\n",
    "    fact_track = fact_track.join(silver_album, on = 'album_id', how = 'inner')\n",
    "    #reorder columns\n",
    "    fact_track = fact_track.select('track_id', 'artist_id', 'album_id', 'name', 'track_number', \n",
    "                                   'disc_number', 'duration_ms', 'explicit', 'url', 'restriction', 'preview', 'execution_date')\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = fact_track, direct = 'gold_data/fact_track', file_type = 'parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0ff282fc-a9fe-49ce-95eb-43570646f644;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 13539ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0ff282fc-a9fe-49ce-95eb-43570646f644\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/28ms)\n",
      "24/11/21 13:21:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------+------------+----------------------+------------+--------------------+--------------------+--------------------+--------------+\n",
      "|              artist|           artist_id|                  id|                name| type|               label|popularity|release_date|release_date_precision|total_tracks|          copyrights|                 url|          link_image|execution_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------+------------+----------------------+------------+--------------------+--------------------+--------------------+--------------+\n",
      "|          Sonu Nigam|1dVygo6tRFXC8CSWU...|000LYwpRfBAlh4aFM...|Sonu Nigam Kannad...|album|        Lahari Music|        25|  2017-07-30|                   day|           7|2017 Lahari Recor...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|              AmaLee|4sf4DrAOkheqktxTy...|000fJX0XXUpWDpgit...|               Unity|album|    Leegion Creative|        42|  2020-08-05|                   day|          12|2020 Leegion Crea...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|     Mega Shinnosuke|72owWXEwmyfKq3ajr...|000hrcnG9LpofStj7...|         Culture Dog|album|     Mega Shinnosuke|        19|  2021-09-08|                   day|          11|2021 Mega Shinnos...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|    Dayang Nurfaizah|1E5aZPein8p4Jf9zk...|000iB8Z8L2XmqQiW6...|                2007|album|New Southern Reco...|        14|  2007-06-18|                   day|          12|New Southern Reco...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|          The Motans|05qpk4JDcLSFNJSsP...|000mwAZN63acYRoma...|  Great Expectations|album|      Global Records|        44|  2022-05-13|                   day|          12|2022 Global Recor...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|     Robbie Williams|2HcwFjNelS49kFbfv...|002H8sA77XFikjH4k...|The Heavy Enterta...|album|            Columbia|        52|  2016-11-04|                   day|          16|(P) 2016 Robert W...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|             CAKEBOY|6qEwx66sEhRl98CXP...|002f2iCucCNnMwVgL...|             LUVANDA|album|         Koala Music|        23|  2020-03-27|                   day|          11|2020 GLAM GO GANG...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|Sons de la Nature...|5n65kD8k3MMdHUTIz...|003W4yNNPuYiI9c0Q...|Bruit de la pluie...|album|      Integral Music|        37|  2023-02-27|                   day|         100|2023 Integral Mus...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|  Paloma San Basilio|1QcwtcwAClkmeaVof...|0040c1kULS1ykbMFk...|            Escorpio|album|    Orosound Records|         6|  2013-11-18|                   day|          11|(C) 2013 Orosound...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|               Don Q|5TM9R6dNoJSMq23yZ...|0046G1euhbWOAtnwK...|            Corleone|album|Highbridge the La...|        30|  2022-06-24|                   day|          11|2022 Highbridge t...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|      Camila Cabello|4nDoRrQiYLoBzwC5B...|004ywPlW72Hgn1Bo9...|              C,XOXO|album|Geffen/Interscope...|        73|  2024-06-28|                   day|          14|© 2024 Camila Cab...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|Newborn Sleep Mus...|2vpce9EltQyoH9lVn...|005E8jJyWA1aKhMIe...|Moonlit Meadows: ...|album|Merry Measures St...|         0|  2024-09-23|                   day|          30|2024 Merry Measur...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|  Brass Construction|5jWuVnPLmjABrgCGi...|005moGTm1TocUHCTN...|Conquest (Expande...|album|CAPITOL CATALOG M...|         4|  1985-01-01|                   day|          12|© 1985 Capitol Re...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|   Roberto Vecchioni|3TVifQ5FPcIzzcYSU...|005w0J5X9b2u9bwtv...|  Il Cielo Capovolto|album|       EMI Marketing|        28|  1995-01-01|                   day|          10|© 1995 EMI Music ...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|           21 Savage|1URnnhqYAYcrqrcwq...|007DWn799UWvfY1ww...|        i am > i was|album|Slaughter Gang, L...|        79|  2018-12-21|                   day|          15|(P) 2018 Slaughte...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|           Nightwish|2NPduAUeLVsfIauhR...|007KuGgJRyLlPvIkG...|Decades: Live in ...|album|       Nuclear Blast|        41|  2019-12-06|                   day|          21|2019 Nuclear Blas...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|         David Broza|3BCJyAgxvYyeIjQyo...|007ZwjOEezTooZWqo...|החומרים שמהם עשוי...|album| David Broza Records|        16|  1994-01-01|                   day|          13|(C) 2014 David Br...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|         Bjarni Ómar|30NBEBMzDmnnw5lvs...|008WvCl6UF4JMiDyf...|           Fyrirheit|album|Bjarni Ómar Haral...|        29|  2016-05-21|                   day|          12|2016 Bjarni Ómar ...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|Konstantin Klasht...|6OzDh0bCqcwWZfGNV...|0097npz4ZQXbUcMDo...|      Smooth Jazz IV|album|           Kvk Music|        14|  2017-06-02|                   day|          10|2017 Konstantin K...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|          VDJ WIZKEL|08L39ZVoopuNLrODG...|009ULF23b9DugiC3c...| Yan Yan Always Hard|album|          VDJ WIZKEL|        17|  2024-02-28|                   day|           4|2024 VDJ WIZKEL, ...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------+------------+----------------------+------------+--------------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "67544\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "with get_sparkSession('test') as spark:\n",
    "    data = read_HDFS(spark, HDFS_dir = \"silver_data/silver_album\", file_type = 'parquet')dât.\n",
    "    data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowFlake loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "net.snowflake#spark-snowflake_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-12153588-065c-41bc-ac39-e3b008a09fdc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound net.snowflake#spark-snowflake_2.12;2.12.0-spark_3.4 in central\n",
      "\tfound net.snowflake#snowflake-ingest-sdk;0.10.8 in central\n",
      "\tfound net.snowflake#snowflake-jdbc;3.13.30 in central\n",
      ":: resolution report :: resolve 485ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tnet.snowflake#snowflake-ingest-sdk;0.10.8 from central in [default]\n",
      "\tnet.snowflake#snowflake-jdbc;3.13.30 from central in [default]\n",
      "\tnet.snowflake#spark-snowflake_2.12;2.12.0-spark_3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-12153588-065c-41bc-ac39-e3b008a09fdc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "24/11/19 13:22:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: snowflake_load_data_spark and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to upload dim_artist into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_genres...\n",
      "Starting to upload dim_genres into SnowFlake...\n",
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_genres' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_artist_genres...\n",
      "Starting to upload dim_artist_genres into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist_genres' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_album...\n",
      "Starting to upload dim_album into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_album' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to upload dim_track_feature into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_track_feature' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/fact_track...\n",
      "Starting to upload fact_track into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.fact_track' into SnowFlake.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 13:24:41 WARN SparkConnectorContext$: Finish cancelling all queries for local-1732022544370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "def load_data_Snowflake(spark):\n",
    "    dim_artist = read_HDFS(spark, HDFS_dir = 'gold_data/dim_artist', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_artist, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist')\n",
    "    dim_genres = read_HDFS(spark, HDFS_dir = 'gold_data/dim_genres', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_genres, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_genres')\n",
    "    dim_artist_genres = read_HDFS(spark, HDFS_dir = 'gold_data/dim_artist_genres', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_artist_genres, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist_genres')\n",
    "    dim_album = read_HDFS(spark, HDFS_dir = 'gold_data/dim_album', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data= dim_album, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_album')\n",
    "    dim_track_feature = read_HDFS(spark, HDFS_dir = 'gold_data/dim_track_feature', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_track_feature, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_track_feature')\n",
    "    fact_track = read_HDFS(spark, HDFS_dir = 'gold_data/fact_track', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = fact_track, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.fact_track')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(\"snowflake_load_data_spark\") as spark:\n",
    "        load_data_Snowflake(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_sparkSession\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m spark:\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m read_HDFS(spark, HDFS_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilver_data/silver_artist\u001b[39m\u001b[38;5;124m'\u001b[39m, file_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     data\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_sparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "with get_sparkSession(\"test\") as spark:\n",
    "    data = read_HDFS(spark, HDFS_dir = 'silver_data/silver_artist', file_type = 'parquet')\n",
    "    data.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "     Pos            Artist Execution_date\n",
      "126  127  Ovy On the Drums     2024-11-22\n",
      "Don't create the database 'music_database' because it already exists.\n",
      "Don't create the collection 'artist_name_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_name_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # add pandas\n",
    "from datetime import datetime\n",
    "def crawl_new_artist_name():\n",
    "    # The URL of the website containing the art listing\n",
    "    url = 'https://kworb.net/itunes/extended.html'\n",
    "    # Read all tables from the website into a list\n",
    "    table = pd.read_html(url)\n",
    "    # Get the first table from the site data (The table includes the names of the artists)\n",
    "    ArtistName = table[0][['Pos','Artist']]\n",
    "    ArtistName['Execution_date'] = \"2024-11-22\"\n",
    "    return ArtistName.astype(str)\n",
    "    # Convert to csv file\n",
    "    # filePath = 'D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\artistName.csv'\n",
    "    # ArtistName.to_csv(filePath,index=False)\n",
    "    print(\"Completed\")\n",
    "\n",
    "\n",
    "# crawl_new_artist_name()\n",
    "\n",
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    old_artist_name_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_name_collection')    \n",
    "    old_artist_name_data = old_artist_name_data[['Artist']]\n",
    "    old_artist_name_data.rename(columns = {'Artist': 'Old_Artist'}, inplace = True)\n",
    "    new_artist_name_data = crawl_new_artist_name()\n",
    "\n",
    "    daily_artist_name_data = pd.merge(old_artist_name_data, new_artist_name_data, left_on = 'Old_Artist', right_on = 'Artist', how = 'right')\n",
    "    daily_artist_name_data = daily_artist_name_data[daily_artist_name_data['Old_Artist'].isnull()][['Pos', 'Artist', 'Execution_date']]\n",
    "\n",
    "    daily_artist_name_data = daily_artist_name_data.head(1)\n",
    "\n",
    "    print(daily_artist_name_data)\n",
    "    \n",
    "    client_operations.insert_data(database_name = 'music_database', collection_name = 'artist_name_collection', data = daily_artist_name_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "                        _id  Pos            Artist Execution_date\n",
      "0  673ed65b1b1f107a33c932e2   27        Charli xcx     2024-11-22\n",
      "1  673ed6871b1f107a33c932e4  127  Ovy On the Drums     2024-11-22\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "#dfArtists = pd.read_csv('D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\mydata.csv')\n",
    "#artist_file = 'D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\artistInfo.csv'\n",
    "artist_columns=['Artist_ID', 'Artist_Name', 'Genres', 'Followers', 'Popularity', 'Artist_Image',\n",
    "                'Artist_Type', 'External_Url', 'Href', 'Artist_Uri']\n",
    "sp =spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='48359fbc41d14367bb76ad67e4508f8e'\n",
    "                                                          ,client_secret='17b9ae9f31f44b82a7ce223e15c863bf'))\n",
    "def write_to_csv(file_path, data, columns):\n",
    "    # Chuyển data thành dataframe với các cột đã chỉ định\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    # Mở file ở chế độ append ('a') và ghi dữ liệu vào\n",
    "    with open(file_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        # Ghi dữ liệu với cờ `header=False` nếu file đã tồn tại, chỉ ghi header khi file trống\n",
    "        df.to_csv(f, header=f.tell() == 0, index=False)\n",
    "\n",
    "def getArtistData(artistName):\n",
    "    result = sp.search(q='artist:' + artistName, type='artist') #Sử dụng biến api_call để lưu lại hàm lambda với lời gọi api tương ứng\n",
    "    if result and result['artists']['items']:  # Kiểm tra nếu tìm thấy nghệ sĩ\n",
    "        artist = result['artists']['items'][0]  # Lấy nghệ sĩ đầu tiên\n",
    "        artistId = artist['id']  # ID nghệ sĩ\n",
    "        artistInfo = { #Thêm các thông tin của nghệ sĩ vào list \n",
    "            'name': artist['name'],\n",
    "            'genres': ', '.join(artist['genres']),  # Nối các thể loại lại thành chuỗi\n",
    "            'followers': artist['followers']['total'],\n",
    "            'popularity':artist['popularity'],\n",
    "            'image':artist['images'][0]['url'] if artist['images'] else None,\n",
    "            'type':artist['type'],\n",
    "            'externalURL':artist['external_urls'],\n",
    "            'href':artist['href'],\n",
    "            'uri':artist['uri']\n",
    "    }\n",
    "        return artistId, artistInfo #Trả về ID và thông tin của nghệ sĩ\n",
    "    print(f\"Can't find artist: {artistName}\")\n",
    "    return None,None #Trả về None nếu không có thông tin \n",
    "\n",
    "def start_to_crawl_artist(dfArtists: pd.DataFrame, Execution_date: str):\n",
    "    Artist_Data = [] \n",
    "    i=1\n",
    "    for artistName in dfArtists['Artist']: #Lặp từng nghệ sĩ trong danh sách\n",
    "        print(str(i)+\")Loading Artist...\"+ artistName)\n",
    "        artistId,artistInfo = getArtistData(artistName) #Lấy thông tin từ hàm đã cài đặt\n",
    "        if artistId and artistInfo:\n",
    "            Artist_Data.append({ #Thêm thông tin vào List lưu trữ\n",
    "                            'Artist_ID':artistId,\n",
    "                            'Artist_Name':artistInfo['name'],\n",
    "                            'Genres':artistInfo['genres'],\n",
    "                            'Followers':artistInfo['followers'],\n",
    "                            'Popularity':artistInfo['popularity'],\n",
    "                            'Artist_Image':artistInfo['image'],\n",
    "                            'Artist_Type':artistInfo['type'],\n",
    "                            'External_Url':artistInfo['externalURL'],\n",
    "                            'Href':artistInfo['href'],\n",
    "                            'Artist_Uri':artistInfo['uri']\n",
    "                    })\n",
    "            #write_to_csv(artist_file,Artist_Data,columns=artist_columns)\n",
    "        i+=1\n",
    "    Artist_Data = pd.DataFrame(Artist_Data)\n",
    "    Artist_Data['Execution_date'] = Execution_date\n",
    "    return Artist_Data\n",
    "    print(\"Successfully\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "1)Loading Artist...Charli xcx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't write token to cache at: .cache\n",
      "Couldn't read cache at: .cache\n",
      "Couldn't write token to cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2)Loading Artist...Ovy On the Drums\n",
      "Don't create the database 'music_database' because it already exists.\n",
      "Don't create the collection 'artist_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    daily_artist_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_name_collection', query = {'Execution_date': \"2024-11-22\"})\n",
    "    data = start_to_crawl_artist(daily_artist_data, Execution_date = \"2024-11-22\")\n",
    "    \n",
    "    client_operations.insert_data(database_name = 'music_database', collection_name = 'artist_collection', data = data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "# Import necessary libraries\n",
    "# dfAlbumID = pd.read_csv('D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\Project_Music_recommend\\\\mydata.csv')\n",
    "# album_file = 'D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\Project_Music_recommend\\\\AlbumDatatmp.csv'\n",
    "# track_file = 'D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\Project_Music_recommend\\\\TrackData.csv'\n",
    "# album_columns=['Artist','Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', 'Label', 'Popularity', 'Available_Markets',\n",
    "#                'Release_Date', 'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', 'External_URL', 'Href', 'Image', 'Uri']\n",
    "# track_columns=['Artists','Album_ID','Album_Name','Track_ID', 'Name', 'Track_Number', 'Type', 'AvailableMarkets', 'Disc_Number', \n",
    "#                'Duration_ms', 'Explicit', 'External_urls', 'Href', 'Restrictions', 'Preview_url', \n",
    "#                'Uri', 'Is_Local']\n",
    "\n",
    "# def write_to_csv(file_path, data, columns):\n",
    "#     # Convert data into dataframe with specified columns\n",
    "#     df = pd.DataFrame(data, columns=columns)\n",
    "#     # Open the file in append mode ('a') and write data to it\n",
    "#     with open(file_path, 'a', newline='', encoding='utf-8') as f:\n",
    "#         # Write data with `header=False` flag if file already exists, write header only if file is empty\n",
    "#         df.to_csv(f, header=f.tell() == 0, index=False)\n",
    "\n",
    "# Function to divide album list into small groups (chunks)\n",
    "def chunk_album_ids(album_ids,chunk_size=20):\n",
    "    for i in range(0,len(album_ids),chunk_size):\n",
    "        yield album_ids[i:i+chunk_size]\n",
    "\n",
    "# Function to get album data from Spotify API\n",
    "def crawl_album_track(dfAlbum: pd.DataFrame): \n",
    "    sp =spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='05e2ff0a21954615b11878a9eb038e7f'\n",
    "                                                          ,client_secret='7f2e7dc0bd0e41caa3665b5dea9ab8e0'))\n",
    "    \n",
    "    albumIds = dfAlbum['Album_ID'].tolist()\n",
    "\n",
    "    album_info =[]\n",
    "    track_info =[]\n",
    "    i=1\n",
    "    #Split album list into chunks\n",
    "    for chunk in chunk_album_ids(albumIds):\n",
    "        print(str(i)+f\" )Calling API for {len(chunk)} albums\")\n",
    "        albums = sp.albums(chunk) # Get information about the albums from the API\n",
    "        for album in albums['albums']:\n",
    "            copyrights = album.get('copyrights', []) # Get information about the copyrights\n",
    "            copyrights_info = ', '.join([c['text'] for c in copyrights]) if copyrights else \"No copyrights information\" #If there is information, get the opposite information and return No information\n",
    "            album_info.append({ # Add album information to the list\n",
    "                'Artist':album['artists'][0]['name'],\n",
    "                'Artist_ID':album['artists'][0]['id'],\n",
    "                'Album_ID':album['id'],\n",
    "                'Name':album['name'],\n",
    "                'Type':album['album_type'],\n",
    "                'Genres': ','.join(album.get('genres', [])),\n",
    "                'Label':album.get('label','Unknown'),\n",
    "                'Popularity':album.get('popularity',None),\n",
    "                'Available_Markets':','.join(album.get('available_markets',[])),\n",
    "                'Release_Date':album.get('release_date','Unknow'),\n",
    "                'ReleaseDatePrecision':album.get('release_date_precision','Unknow'),\n",
    "                'TotalTracks':album.get('total_tracks',None),\n",
    "                'Copyrights': copyrights_info,\n",
    "                'Restrictions': album.get('restrictions', {}).get('reason', None),\n",
    "                'External_URL': album.get('external_urls', {}).get('spotify',None),\n",
    "                'Href': album.get('href',None),\n",
    "                'Image': album['images'][0]['url'] if album.get('images') and len(album['images']) > 0 else None,\n",
    "                'Uri': album.get('uri',None)\n",
    "            })\n",
    "            for track in album['tracks']['items']:\n",
    "                track_info.append({\n",
    "                    'Artists': ', '.join(artist['name'] for artist in track['artists']),  # Join the artists' names into a string\n",
    "                    'Album_Name':album['name'],\n",
    "                    'Album_ID':album['id'],\n",
    "                    'Track_ID': track['id'],\n",
    "                    'Name': track['name'],\n",
    "                    'Track_Number': track['track_number'],\n",
    "                    'Type': track['type'],\n",
    "                    'AvailableMarkets': ','.join(track.get('available_markets', [])),\n",
    "                    'Disc_Number': track['disc_number'],\n",
    "                    'Duration_ms': track['duration_ms'],\n",
    "                    'Explicit': track['explicit'],\n",
    "                    'External_urls': track['external_urls'].get('spotify') if track.get('external_urls') else None,  # check externalURL\n",
    "                    'Href': track['href'],\n",
    "                    'Restrictions': track.get('restrictions', {}).get('reason', None),\n",
    "                    'Preview_url': track.get('preview_url',None),\n",
    "                    'Uri': track['uri'],\n",
    "                    'Is_Local': track['is_local']\n",
    "                })\n",
    "        i+=1\n",
    "    album_info, track_info = pd.DataFrame(album_info), pd.DataFrame(track_info)\n",
    "    return album_info,track_info\n",
    "\n",
    "#album_info, track_info = crawl_album_track(albumIds)\n",
    "\n",
    "#print(\"Successful\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "The connection to MongoDB has stopped!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "crawl_album_track() missing 1 required positional argument: 'dfAlbum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m client_operations \u001b[38;5;241m=\u001b[39m mongoDB_operations(client)\n\u001b[1;32m      3\u001b[0m daily_artist_data \u001b[38;5;241m=\u001b[39m client_operations\u001b[38;5;241m.\u001b[39mread_data(database_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic_database\u001b[39m\u001b[38;5;124m'\u001b[39m, collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist_collection\u001b[39m\u001b[38;5;124m'\u001b[39m, query \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution_date\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-11-23\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 4\u001b[0m daily_album_data, daily_track_data \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_album_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(daily_album_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(daily_track_data)\n",
      "\u001b[0;31mTypeError\u001b[0m: crawl_album_track() missing 1 required positional argument: 'dfAlbum'"
     ]
    }
   ],
   "source": [
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    daily_artist_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_collection', query = {'Execution_date': \"2024-11-23\"})\n",
    "    daily_album_data, daily_track_data = crawl_album_track(daily_artist_data)\n",
    "    print(daily_album_data)\n",
    "    print(daily_track_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
