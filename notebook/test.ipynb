{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, database, collection\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\"\"\" Context manager for mongoDB connection. \"\"\"\n",
    "@contextmanager\n",
    "def mongoDB_client(username: str, password: str, \n",
    "                    host: str = 'mongo', port: str = 27017):\n",
    "    #set path\n",
    "    path = f\"mongodb://{username}:{password}@{host}:{port}\"\n",
    "    client = None\n",
    "\n",
    "    #init\n",
    "    try:\n",
    "        print(\"Starting connect mongoDB...\")\n",
    "        client = MongoClient(path)\n",
    "        \n",
    "        print(\"Client connected successfully!\")\n",
    "        yield client\n",
    "\n",
    "    #handle error\n",
    "    except ConnectionFailure:\n",
    "        print(\"Connection to mongoDB failed!\")\n",
    "\n",
    "    except OperationFailure:\n",
    "        print(\"Operation failed!\")\n",
    "\n",
    "    #close client\n",
    "    finally:\n",
    "        client.close()\n",
    "        print(\"The connection to MongoDB has stopped!\")\n",
    "\n",
    "\"\"\" Class mongoDB for operations. \"\"\"\n",
    "class mongoDB_operations:\n",
    "    \"\"\" Init \"\"\"\n",
    "    def __init__(self, client: MongoClient):\n",
    "        #check params\n",
    "        if not isinstance(client, MongoClient):\n",
    "            raise TypeError('client must be MongoClient!')\n",
    "        \n",
    "        #set value for class attrs\n",
    "        self.client = client\n",
    "\n",
    "    \"\"\" Check whether the database exists. \"\"\"\n",
    "    def check_database_exists(self, database_name: str) -> bool:\n",
    "        #list database name\n",
    "        return database_name in self.client.list_database_names()\n",
    "\n",
    "    \"\"\" Check whether collection exists. \"\"\"\n",
    "    def check_collection_exists(self, database_obj: database.Database, collection: str) -> bool:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #list collection name\n",
    "        return collection in self.client[database_obj.name].list_collection_names()\n",
    "\n",
    "    \"\"\" Create new database. \"\"\"\n",
    "    def create_database_if_not_exists(self, database_name: str) -> database.Database:\n",
    "        #check whether database exists\n",
    "        if self.check_database_exists(database_name):\n",
    "            print(f\"Don't create the database '{database_name}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created database '{database_name}'.\")\n",
    "\n",
    "        #return database\n",
    "        return self.client[database_name]\n",
    "    \n",
    "    \"\"\" Create new collection. \"\"\"\n",
    "    def create_collection_if_not_exists(self, database_obj: database.Database, collection: str) -> collection.Collection:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #check whether collection exists\n",
    "        if self.check_collection_exists(database_obj, collection):\n",
    "            print(f\"Don't create the collection '{collection}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created collection '{collection}'.\")\n",
    "\n",
    "        #return collection\n",
    "        return self.client[database_obj.name][collection]\n",
    "    \n",
    "    \"\"\" Insert data \"\"\"\n",
    "    def insert_data(self, collection_obj: collection.Collection, data: list[dict]):\n",
    "        #check params\n",
    "        if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n",
    "            raise TypeError(\"data must be a list of dictionaries!\")\n",
    "        \n",
    "        if not isinstance(collection_obj, collection.Collection):\n",
    "            raise TypeError(\"collection_obj must be a collection.Collection!\")\n",
    "        \n",
    "        #insert data\n",
    "        collection_obj.insert_many(data)\n",
    "\n",
    "        print(f\"Successfully inserted data into collection '{collection_obj.name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "Don't create the database 'artist_database' because it already exists.\n",
      "Don't create the collection 'artist_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\" Convert data to dictionaries. \"\"\"\n",
    "def get_dict_data(csv_path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.to_dict(orient = 'records')\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_mongodb_artist(artist_path: str = '/opt/data/Artist.csv'):\n",
    "    #use mongoDB client\n",
    "    with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "        client = mongoDB_operations(client)\n",
    "        #create artist database\n",
    "        client_artist_database = client.create_database_if_not_exists(database_name= 'artist_database')\n",
    "\n",
    "        #create artist collection\n",
    "        client_artist_collection = client.create_collection_if_not_exists(database_obj = client_artist_database, \n",
    "                                                                          collection = 'artist_collection')\n",
    "\n",
    "        #get data\n",
    "        data = get_dict_data(artist_path)    \n",
    "\n",
    "        #insert artist data\n",
    "        client_artist_insert = client.insert_data(collection_obj = client_artist_collection, data = data)\n",
    "\n",
    "load_mongodb_artist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DateType, FloatType, BooleanType\n",
    "\n",
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',    StringType(), True),\n",
    "                     StructField('Artist_Name',  StringType(), True),\n",
    "                     StructField('Genres',       ArrayType(StringType(), True), True),\n",
    "                     StructField('Followers',    IntegerType(), True),\n",
    "                     StructField('Popularity',   IntegerType(), True),\n",
    "                     StructField('Artist_Image', StringType(), True),\n",
    "                     StructField('Artist_Type',  StringType(), True),\n",
    "                     StructField('External_Url', StringType(), True),\n",
    "                     StructField('Href',         StringType(), True),\n",
    "                     StructField('Artist_Uri',   StringType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)\n",
    "    \n",
    "    \"\"\" Album schema. \"\"\"\n",
    "    album_schema = [StructField('Artist',               StringType(), True),\n",
    "                    StructField('Artist_ID',            StringType(), True),\n",
    "                    StructField('Album_ID',             StringType(), True),\n",
    "                    StructField('Name',                 StringType(), True),\n",
    "                    StructField('Type',                 StringType(), True),\n",
    "                    StructField('Genres',               ArrayType(StringType(), True), True),\n",
    "                    StructField('Label',                StringType(), True),\n",
    "                    StructField('Popularity',           StringType(), True),\n",
    "                    StructField('Available_Markets',    StringType(), True),\n",
    "                    StructField('Release_Date',         DateType(), True),\n",
    "                    StructField('ReleaseDatePrecision', StringType(), True),\n",
    "                    StructField('TotalTracks',          IntegerType(), True),\n",
    "                    StructField('Copyrights',           StringType(), True),\n",
    "                    StructField('Restrictions',         StringType(), True),\n",
    "                    StructField('External_URL',         StringType(), True),\n",
    "                    StructField('Href',                 StringType(), True),\n",
    "                    StructField('Image',                StringType(), True),\n",
    "                    StructField('Uri',                  StringType(), True)]\n",
    "    #Applying struct type\n",
    "    album_schema = StructType(album_schema)\n",
    "\n",
    "    \"\"\" Track schema. \"\"\"\n",
    "    track_schema = [StructField(\"Artists\",           StringType(), True),\n",
    "                    StructField(\"Album_ID\",         StringType(), True),\n",
    "                    StructField(\"Album_Name\",       StringType(), True),\n",
    "                    StructField(\"Track_ID\",         StringType(), True),\n",
    "                    StructField(\"Name\",             StringType(), True),\n",
    "                    StructField(\"Track_Number\",     IntegerType(), True),\n",
    "                    StructField(\"Type\",             StringType(), True),\n",
    "                    StructField(\"AvailableMarkets\", StringType(), True),\n",
    "                    StructField(\"Disc_Number\",      StringType(), True),\n",
    "                    StructField(\"Duration_ms\",      IntegerType(), True),\n",
    "                    StructField(\"Explicit\",         StringType(), True),\n",
    "                    StructField(\"External_urls\",    StringType(), True),\n",
    "                    StructField(\"Href\",             StringType(), True),\n",
    "                    StructField(\"Restrictions\",     StringType(), True),\n",
    "                    StructField(\"Preview_url\",      StringType(), True),\n",
    "                    StructField(\"Uri\",              StringType(), True),\n",
    "                    StructField(\"Is_Local\",         StringType(), True)]\n",
    "    #Applying struct type\n",
    "    track_schema = StructType(track_schema)\n",
    "    \n",
    "    \"\"\" TrackFeature schema. \"\"\"\n",
    "    trackfeature_schema = [StructField(\"Track_ID\",         StringType(), True),\n",
    "                           StructField(\"Danceability\",     FloatType(), True),\n",
    "                           StructField(\"Energy\",           FloatType(), True),\n",
    "                           StructField(\"Key\",              IntegerType(), True),\n",
    "                           StructField(\"Loudness\",         FloatType(), True),\n",
    "                           StructField(\"Mode\",             BooleanType(), True),\n",
    "                           StructField(\"Speechiness\",      FloatType(), True),\n",
    "                           StructField(\"Acousticness\",     FloatType(), True),\n",
    "                           StructField(\"Instrumentalness\", FloatType(), True),\n",
    "                           StructField(\"Liveness\",         FloatType(), True),\n",
    "                           StructField(\"Valence\",          FloatType(), True),\n",
    "                           StructField(\"Tempo\",            FloatType(), True),\n",
    "                           StructField(\"Time_signature\",   IntegerType(), True),\n",
    "                           StructField(\"Track_href\",       StringType(), True),\n",
    "                           StructField(\"Type_Feature\",     StringType(), True),\n",
    "                           StructField(\"Analysis_Url\",     StringType(), True)]\n",
    "    #Applying struct type\n",
    "    trackfeature_schema = StructType(trackfeature_schema)\n",
    "\n",
    "    #mapping\n",
    "    mapping = {\n",
    "        'artist': artist_schema,\n",
    "        'album': album_schema,\n",
    "        'track': track_schema,\n",
    "        'trackfeature': trackfeature_schema\n",
    "    }\n",
    "    \n",
    "    #return schema\n",
    "    return mapping[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkConf\n",
    "from contextlib import contextmanager\n",
    "import pyspark.sql\n",
    "\n",
    "\"\"\" Context manager for creating Spark Session. \"\"\"\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    #declare sparkconf\n",
    "    conf = SparkConf()\n",
    "\n",
    "    #set config\n",
    "    conf = conf.setAppName(appName) \\\n",
    "               .setMaster(master) \\\n",
    "               .set(\"spark.executor.memory\", \"4g\") \\\n",
    "               .set(\"spark.executor.cores\", \"4\") \\\n",
    "               .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "               .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "    \n",
    "    #create Spark Session\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully created Spark Session with app name: {appName} and master: {master}!\")\n",
    "\n",
    "    #yield spark\n",
    "    try:\n",
    "        yield spark\n",
    "\n",
    "    finally:\n",
    "        #must stop Spark Session\n",
    "        spark.stop()\n",
    "        print(\"Successfully stopped Spark Session!\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from mongoDB. \"\"\"\n",
    "def read_mongoDB(spark: SparkSession, database_name: str, collection_name: str, schema: StructType = None,\n",
    "                 username: str = 'huynhthuan', password: str = 'password', \n",
    "                 host: str = 'mongo', port: str = 27017) -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if schema is not None and not isinstance(schema, StructType):\n",
    "        raise TypeError(\"schema must be a StructType!\")\n",
    "    \n",
    "    #uri mongoDB \n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "\n",
    "    print(f\"Starting to read data from database '{database_name}' and collection '{collection_name}'...\")\n",
    "  \n",
    "    #read data\n",
    "    try:\n",
    "        if schema is not None:\n",
    "            data = spark.read.format(\"mongodb\") \\\n",
    "                            .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                            .option('header', 'true') \\\n",
    "                            .schema(schema) \\\n",
    "                            .load()\n",
    "        else:\n",
    "            data = spark.read.format(\"mongodb\") \\\n",
    "                .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                .option('header', 'true') \\\n",
    "                .load()\n",
    "        #retun data \n",
    "        return data \n",
    "    \n",
    "    except Exception:\n",
    "        print(\"An error occurred while reading data from mongoDB.\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from HDFS. \"\"\"\n",
    "def read_HDFS(spark: SparkSession, HDFS_dir: str, file_type: str) -> pyspark.sql.DataFrame:\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    #set HDFS path\n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{HDFS_dir}\"\n",
    "\n",
    "    print(f\"Starting to read data from {HDFS_path}...\")\n",
    "\n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format(file_type).option('header', 'true').load(HDFS_path)\n",
    "        #return data\n",
    "        return data\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"An error occurred while reading data from HDFS.\")\n",
    "\n",
    "\n",
    "\"\"\" Write data into HDFS. \"\"\"\n",
    "def write_HDFS(spark: SparkSession, data: pyspark.sql.DataFrame, table_name: str, file_type: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "\n",
    "    #set HDFS path  \n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{table_name}\"\n",
    "\n",
    "    print(f\"Starting to upload '{table_name}' into {HDFS_path}...\")\n",
    "    \n",
    "    #write data\n",
    "    try:\n",
    "        data.write.format(file_type) \\\n",
    "                  .option('header', 'true') \\\n",
    "                  .mode('append') \\\n",
    "                  .save(HDFS_path)\n",
    "        \n",
    "        print(f\"Successfully uploaded '{table_name}' into HDFS.\")\n",
    "\n",
    "    except Exception:\n",
    "        print(\"An error occurred while upload data into HDFS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: init_load and master: local!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load all csv files into mongoDB.\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(appName = \"init_load\") as spark:\n",
    "        #uri\n",
    "        uri_artist_name = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_name_collection?authSource=admin\"\n",
    "        uri_artist = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_collection?authSource=admin\"\n",
    "        uri_album = \"mongodb://huynhthuan:password@mongo:27017/music_database.album_collection?authSource=admin\"\n",
    "        uri_track = \"mongodb://huynhthuan:password@mongo:27017/music_database.track_collection?authSource=admin\"\n",
    "        uri_trackfeature = \"mongodb://huynhthuan:password@mongo:27017/music_database.trackfeature_collection?authSource=admin\"\n",
    "\n",
    "        # read\n",
    "        df_ArtistName = spark.read.option('header', 'true').csv(\"/opt/data/ArtistName.csv\")\n",
    "        df_Artist = spark.read.option('header', 'true').csv(\"/opt/data/Artist.csv\")\n",
    "        df_Album = spark.read.option('header', 'true').csv(\"/opt/data/Album.csv\")\n",
    "        df_Track = spark.read.option('header', 'true').csv(\"/opt/data/Track.csv\")\n",
    "        df_TrackFeature = spark.read.option('header', 'true').csv(\"/opt/data/TrackFeature.csv\")\n",
    "\n",
    "\n",
    "        #write\n",
    "        df_ArtistName.write.format('mongoDB') \\\n",
    "                           .option(\"spark.mongodb.write.connection.uri\", uri_artist_name) \\\n",
    "                           .mode(\"overwrite\") \\\n",
    "                           .save()\n",
    "        \n",
    "        df_Artist.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_artist) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Album.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_album) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Track.write.format('mongoDB') \\\n",
    "                      .option(\"spark.mongodb.write.connection.uri\", uri_track) \\\n",
    "                      .mode(\"overwrite\") \\\n",
    "                      .save()\n",
    "        \n",
    "        df_TrackFeature.write.format('mongoDB') \\\n",
    "                             .option(\"spark.mongodb.write.connection.uri\", uri_trackfeature) \\\n",
    "                             .mode(\"overwrite\") \\\n",
    "                             .save()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: Bronze_task and master: local!\n",
      "Starting to read data from database 'music_database' and collection 'track_collection'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=61>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/context.py\", line 377, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/context.py\", line 2255, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o4458.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o4498.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 75\u001b[0m\n\u001b[1;32m     48\u001b[0m         track_data\u001b[38;5;241m.\u001b[39mfilter(length(track_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrack_Number\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(truncate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;66;03m#upload data into HDFS\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m#write_HDFS(spark, data = track_data, table_name = 'bronze_track', file_type = 'parquet')\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# track_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'track_collection')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m# #load data into mongoDB\u001b[39;00m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# write_HDFS(spark, track_feature_data, 'trackfeature', 'parquet')\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mbronze_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 48\u001b[0m, in \u001b[0;36mbronze_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m track_data \u001b[38;5;241m=\u001b[39m read_mongoDB(spark, database_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic_database\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     41\u001b[0m                           collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_collection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m track_data \u001b[38;5;241m=\u001b[39m track_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtists\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlbum_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlbum_Name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrack_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     44\u001b[0m                                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrack_Number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvailableMarkets\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     45\u001b[0m                                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisc_Number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration_ms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplicit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExternal_urls\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     46\u001b[0m                                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRestrictions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreview_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUri\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIs_Local\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrack_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrack_Number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:914\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    907\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    908\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m         },\n\u001b[1;32m    912\u001b[0m     )\n\u001b[0;32m--> 914\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o4498.showString"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, get_json_object, to_date, regexp_replace, length\n",
    "\n",
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_task():\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task') as spark:\n",
    "        # \"\"\" Artist data. \"\"\"\n",
    "        # artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "        # #preprocessing before loading data\n",
    "        # artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "        #                          .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "        #                          .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "        #                          .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "        # #reorder columns after reading \n",
    "        # artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "        #                                  'Followers', 'Popularity', 'Artist_Image', \n",
    "        #                                  'Artist_Type', 'External_Url', 'Href', 'Artist_Uri')\n",
    "        # #applying schema        \n",
    "        # artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "        # #upload data into HDFS\n",
    "        # write_HDFS(spark, data = artist_data, table_name = 'bronze_artist', file_type = 'parquet')\n",
    "\n",
    "\n",
    "        # \"\"\" Album data. \"\"\"\n",
    "        # album_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'album_collection')\n",
    "        # album_data = album_data.withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "        #                        .withColumn('Release_Date', to_date('Release_Date', \"MM/dd/yyyy\")) \\\n",
    "        #                        .withColumn('TotalTracks', col('TotalTracks').cast('int'))\n",
    "        # #reorder columns after reading\n",
    "        # album_data = album_data.select('Artist', 'Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', \n",
    "        #                                'Label', 'Popularity', 'Available_Markets', 'Release_Date', \n",
    "        #                                'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', \n",
    "        #                                'External_URL', 'Href', 'Image', 'Uri')\n",
    "        # album_data = spark.createDataFrame(album_data.rdd, schema = get_schema('album'))\n",
    "        # #upload data into HDFS\n",
    "        # write_HDFS(spark, data = album_data, table_name = 'bronze_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "        # \"\"\" Track data. \"\"\"\n",
    "        track_data = read_mongoDB(spark, database_name = 'music_database', \n",
    "                                  collection_name = 'track_collection')\n",
    "        \n",
    "        track_data = track_data.select('Artists', 'Album_ID', 'Album_Name', 'Track_ID', \n",
    "                                       'Name', 'Track_Number', 'Type', \n",
    "                                       'Disc_Number', 'Duration_ms', 'Explicit', 'External_urls', \n",
    "                                       'Href', 'Restrictions', 'Preview_url', 'Uri', 'Is_Local')\n",
    "        \n",
    "        track_data.filter(length(track_data['Track_Number']) > 3).show(truncate = False)\n",
    "        #upload data into HDFS\n",
    "        #write_HDFS(spark, data = track_data, table_name = 'bronze_track', file_type = 'parquet')\n",
    "        # track_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'track_collection')\n",
    "        # #reorder columns after reading\n",
    "        # track_data = track_data.select('Artist', 'Album_ID', 'Album_Name', 'Track_ID', \n",
    "        #                                'Name', 'Track_Number', 'Type', 'AvailableMarkets', \n",
    "        #                                'Disc_Number', 'Duration_ms', 'Explicit', 'External_urls', \n",
    "        #                                'Href', 'Restrictions', 'Preview_url', 'Uri', 'Is_Local')\n",
    "        # #applying schema\n",
    "        # track_data = spark.createDataFrame(track_data.rdd, schema = get_schema('track'))\n",
    "        # #load data into HDFS\n",
    "        # write_HDFS(spark, track_data, 'track', 'parquet')\n",
    "\n",
    "        # \"\"\" Track Feature data. \"\"\"\n",
    "        # track_feature_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'trackfeature_collection')\n",
    "        # #drop _id\n",
    "        # track_feature_data = track_feature_data.select('Track_ID', 'Danceability', 'Energy', 'Key', 'Loudness', \n",
    "        #                                                'Mode', 'Speechiness', 'Acousticness', 'Instrumentalness', \n",
    "        #                                                'Liveness', 'Valence', 'Valence', 'Time_signature', 'Track_href', \n",
    "        #                                                'Type_Feature', 'Analysis_Url')\n",
    "        # #applying schema\n",
    "        # track_feature_data = spark.createDataFrame(track_feature_data.rdd, schema = get_schema('trackfeature'))\n",
    "        # #load data into mongoDB\n",
    "        # write_HDFS(spark, track_feature_data, 'trackfeature', 'parquet')\n",
    "\n",
    "\n",
    "bronze_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local!\n",
      "Starting to read data from database 'music_database' and collection 'track_collection'...\n",
      "+--------------------+--------------------+-------------+--------------------+--------------------+------------+-----+--------------------+-----------+-----------+--------+--------------------+--------------------+------------+-----------+--------------------+--------+\n",
      "|             Artists|            Album_ID|   Album_Name|            Track_ID|                Name|Track_Number| Type|    AvailableMarkets|Disc_Number|Duration_ms|Explicit|       External_urls|                Href|Restrictions|Preview_url|                 Uri|Is_Local|\n",
      "+--------------------+--------------------+-------------+--------------------+--------------------+------------+-----+--------------------+-----------+-----------+--------+--------------------+--------------------+------------+-----------+--------------------+--------+\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|2gVhfX2Gy1T9kDuS9...|              willow|           1|track|AR,AU,AT,BE,BO,BR...|          1|     214706|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:2gV...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|1gcyHQpBQ1lfXGdhZ...|  champagne problems|           2|track|AR,AU,AT,BE,BO,BR...|          1|     244000|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:1gc...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|3Dby3p1m6IOZn2gII...|           gold rush|           3|track|AR,AU,AT,BE,BO,BR...|          1|     185320|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:3Db...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|6sQckd3Z8NPxVVKUn...|‘tis the damn season|           4|track|AR,AU,AT,BE,BO,BR...|          1|     229840|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:6sQ...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|6lCvK2AR2uOKkVFCV...|         tolerate it|           5|track|AR,AU,AT,BE,BO,BR...|          1|     245440|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:6lC...|   False|\n",
      "|  Taylor Swift, HAIM|4XHIjbhjRmqWlosjj...|LIVING THINGS|6uwfVkaOM1mcMkFmS...|no body, no crime...|           6|track|AR,AU,AT,BE,BO,BR...|          1|     215626|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:6uw...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|55Vf4bimc1Rtfg0PA...|           happiness|           7|track|AR,AU,AT,BE,BO,BR...|          1|     315146|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:55V...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|66tOfHVH3aUrscg8v...|            dorothea|           8|track|AR,AU,AT,BE,BO,BR...|          1|     225880|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:66t...|   False|\n",
      "|Taylor Swift, The...|4XHIjbhjRmqWlosjj...|LIVING THINGS|2awNGIJHodfLZSClB...|coney island (fea...|           9|track|AR,AU,AT,BE,BO,BR...|          1|     275320|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:2aw...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|43Ykum9T72UOPhBN3...|                 ivy|          10|track|AR,AU,AT,BE,BO,BR...|          1|     260440|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:43Y...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|52OkpDsU6MmPx1AwG...|      cowboy like me|          11|track|AR,AU,AT,BE,BO,BR...|          1|     275040|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:52O...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|5VYWxXUpxuxEmCqML...|    long story short|          12|track|AR,AU,AT,BE,BO,BR...|          1|     215920|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:5VY...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|5uICWmZTLkpEVbK22...|            marjorie|          13|track|AR,AU,AT,BE,BO,BR...|          1|     257773|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:5uI...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|6a8aUhYbaQBUI8PcJ...|             closure|          14|track|AR,AU,AT,BE,BO,BR...|          1|     180653|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:6a8...|   False|\n",
      "|Taylor Swift, Bon...|4XHIjbhjRmqWlosjj...|LIVING THINGS|6Wlq9rqkxrqj5Kls4...|evermore (feat. B...|          15|track|AR,AU,AT,BE,BO,BR...|          1|     304106|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:6Wl...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|3zwMVvkBe2qIKDObW...|right where you l...|          16|track|AR,AU,AT,BE,BO,BR...|          1|     245026|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:3zw...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|1kdWw77ZpYOkhxeuh...|it’s time to go -...|          17|track|AR,AU,AT,BE,BO,BR...|          1|     254640|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:1kd...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|0lx2cLdOt3piJbcaX...|              willow|           1|track|AR,AU,AT,BE,BO,BR...|          1|     214706|   False|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:0lx...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|0sY6ZUTh4yoctD8VI...|  champagne problems|           2|track|AR,AU,AT,BE,BO,BR...|          1|     244000|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:0sY...|   False|\n",
      "|        Taylor Swift|4XHIjbhjRmqWlosjj...|LIVING THINGS|5BK0uqwY9DNfZ630S...|           gold rush|           3|track|AR,AU,AT,BE,BO,BR...|          1|     185320|    True|https://open.spot...|https://api.spoti...|        null|       null|spotify:track:5BK...|   False|\n",
      "+--------------------+--------------------+-------------+--------------------+--------------------+------------+-----+--------------------+-----------+-----------+--------+--------------------+--------------------+------------+-----------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "with get_sparkSession('test') as spark:\n",
    "    #data = spark.read.parquet('hdfs://namenode:9000/datalake/bronze_artist', header = True)\n",
    "    # data.filter(col('Artist_ID').isNotNull()).show()\n",
    "    # username = 'huynhthuan'\n",
    "    # password = 'password'\n",
    "    # host = 'mongo'\n",
    "    # port = '27017'\n",
    "    # database_name = 'music_database'\n",
    "    # collection_name = 'artist_collection'\n",
    "    # uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "    # data = spark.read.format(\"mongodb\") \\\n",
    "    #             .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "    #             .option('header', 'true') \\\n",
    "    #             .load()\n",
    "    #             #.schema(get_schema('album')) \\\n",
    "\n",
    "    # data = read_HDFS(spark, 'bronze_album', 'parquet')\n",
    "    # data.printSchema()\n",
    "    # print(data.count())\n",
    "    track_data = read_mongoDB(spark, database_name = 'music_database', \n",
    "                                collection_name = 'track_collection', schema = get_schema('track'))\n",
    "    track_data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
