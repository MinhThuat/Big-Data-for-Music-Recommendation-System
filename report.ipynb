{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report\n",
    "## Big data for Music Recommendation System\n",
    "### Thành viên:  \n",
    "- Huỳnh Minh Thuận - 22110217  \n",
    "- Trương Minh Thuật - 22110218  \n",
    "- Nguyễn Phạm Anh Trí - 22110236  \n",
    "- Nguyễn Minh Trí - 22110235  \n",
    "- Nguyễn Đình Tiến - 22110230 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Introduction](#1.-introduction)  \n",
    "2. [Data Collection and Ingestion](#2.-data-collection-and-ingestion)  \n",
    "    2.1 [Data Retrieval Functions and Execution Process](#2.1-data-retrieval-functions-and-execution-process)  \n",
    "    2.2 [Daily Data Scraping and Storing Strategy](#2.2-daily-data-scraping-and-storing-strategy)\n",
    "3. [Three-Layer Data Lake Processing](#3.-three-layer-data-lake-processing)  \n",
    "    3.1 [Bronze Layer Procesing](#3.1-bronze-layer-processing)  \n",
    "    3.2 [Silver Layer Procesing](#3.2-silver-layer-processing)  \n",
    "    3.3 [Gold Layer Procesing](#3.3-gold-layer-processing)  \n",
    "4. [Data Warehouse Storing](#4.-data-warehouse-storing)\n",
    "5. [Exploratory Data Analysis](#5.-exploratory-data-analysis)\n",
    "6. [Machine Learning for Recommendation System](#6.-machine-learning-for-recommendation-system)\n",
    "7. [Application with Streamlit](#7.-application-with-streamlit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction \n",
    "- In this day and age, music is an essential part of life, offering both entertainment and emotional connection. Our team aims to create an end-to-end date pipeline architecture that covers data collection, processing, storage, analysis, reporting, and building a recommendation system for music based on user input.\n",
    "\n",
    "- **Data sources**: The data source is initially collected from https://kworb.net/itunes/extended.html, which includes top 15000 artist names that will change daily. After that, we use Spotify API to retrieve data about artist's information, albums, tracks and track features based on the list of artist names from Kworb website.\n",
    "\n",
    "- **Tools**:\n",
    "    - **Python**: Main programming language.\n",
    "    - **Docker**: Run containers, ensuring consistent and scalable environments.\n",
    "    - **MongoDB**: Used for data storage as Database\n",
    "    - **HDFS**: A part of Hadoop architecture, used for data storage as Data Lake.\n",
    "    - **Snowflake**: Cloud-Based Data Warehouse.\n",
    "    - **PowerBI**: A tool for displaying data and providing comprehensive overview.\n",
    "    - **Airflow**: A framework that uses Python to schedule and run tasks.\n",
    "\n",
    "- **Architecture**:    \n",
    "![My Image](./images/Architecture.png)\n",
    "- **Link**: To explore the full source code, feel free to check out our GitHub repository:  \n",
    "*https://github.com/mjngxwnj/Big-Data-for-Music-Recommendation-System*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Collection and Ingestion  \n",
    "We start by collecting data from **Kworb.net**, which includes **15,000** artist name, then use the **Spotify API** to fetch more music-related details. This data is stored in **MongoDB** for further processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Retrieval Functions and Execution Process\n",
    "#####   2.1.1 Get data from **Kworb.net**  \n",
    "- **Step 1**: We get the link from the **Kworb.net** and use pandas to get data from the site and use the read html function (**pandas.read_html(url)**) to read all tables from the returned web page. Function pandas.read_html will return a list of tables available on the web page.  \n",
    "\n",
    "- **Step 2**: Because the information we need to get is from the first table, we proceed to get the first table with 2 columns: **Pos** corresponding to the artist's position on the rankings table and  **Artist** corresponding to the artist's name.  \n",
    "\n",
    "- **Step 3**: We save the 2 columns we have retrieved to **MongoDB** to proceed with the next steps.  \n",
    "\n",
    "##### 2.1.2 Get Information of **Artist**:  \n",
    "- **Step 1**: We get the artist's name from the **MongoDB** database after filtering out artists with no information. We use the spotipy library to connect the api to spotify and get that artist's information. To connect the api with spotify we need 2 things: client id and client secret to connect  \n",
    "\n",
    "- **Step 2**: We use the function **sp.search** to get the artist's information. This function returns a dictionary with the artist's information.  \n",
    "\n",
    "- **Step 3**: We put the information into the corresponding columns and transfer that information back to MongoDB.  \n",
    "    \n",
    "##### 2.1.3 Get Information of **Artist's Album** and **Track**:  \n",
    "\n",
    "- **Step 1**: After obtaining the artist's data, including artist id, we use artist id to get data about the album id and save that data into a list for continued use.  \n",
    "\n",
    "- **Step 2**: To optimize the number of api calls (avoid overload), we divide the album id list into smaller lists to call the api for each sublist. By using the **spotipy.album** function we can get data for 20 albums and tracks contained in that album in one api call.  \n",
    "\n",
    "- **Step 3**: We save all the data to the **MongoDB** database.  \n",
    "\n",
    "##### 2.1.4 Get Information of **Track Feature**:  \n",
    "- **Step 1**: Using the data from the previous step, we have obtained the track id to get more features of each track.  \n",
    "\n",
    "- **Step 2**: Also to optimize the number of api calls, we divide the list of tracks into sublists to call the api from spotify. By using the **spotipy.audio_features** function we can get the audio features of 100 tracks in one api call.  \n",
    "\n",
    "- **Step 3**: We save all the data to the **MongoDB** database.  \n",
    "\n",
    "![Image](./images/crawl_api.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Daily Data Scraping and Storing Strategy\n",
    "##### 2.2.1 Initial Data Scraping and Storing   \n",
    "- Sau lần thử scraping data ở ngày đầu, ta thấy được việc crawl 15,000 tên nghệ sĩ và phải mất rất nhiều lượt call **Spotify API** để lấy thông tin (có thể bị block API).  \n",
    "\n",
    "- Để giải quyết vấn đề này, ta sẽ chia 15,000 tên nghệ sĩ trong 3 ngày, mỗi ngày ta sẽ chỉ lấy 5,000 tên nghệ sĩ để call **Spotify API** và lấy thông tin **artist**, **albums**, **tracks**, **track features** và lưu vào CSV file. Khi toàn bộ 15,000 tên nghệ sĩ được sử dụng để lấy thông tin, ta sẽ load toàn bộ csv file lên MongoDB, và gọi bộ data này là **initial data**.  \n",
    "\n",
    "- Các hàm call để lấy data sẽ tương tự như trên, và ta sẽ lưu bằng csv trước thay vì MongoDB.  \n",
    " \n",
    "##### 2.2.2 Subsequent Data Scraping and Storing  \n",
    "- Ở những ngày tiếp theo (sau khi đã có được toàn bộ dữ liệu của 15,000 nghệ sĩ), nếu chúng ta tiếp tục crawl 15,000 tên nghệ sĩ mới ở Kworb sau đó dùng **Spotify API** để lấy thông tin **album**, **track**, chúng ta sẽ gặp vấn đề data duplication, vì các nghệ sĩ nổi tiếng như Taylor Swift thường xuyên nằm trong bảng xếp hạng, dẫn đến việc lặp lại dữ liệu của họ nhiều lần khi crawl hằng ngày.  \n",
    "- Để tránh khỏi vấn đề này, ta sẽ crawl 15,000 tên nghệ sĩ mới (**new artist names**), sau đó thực hiện **_Left Anti Join_** với 15,000 tên nghệ sĩ cũ (**old artist names**) trong MongoDB để tìm ra được những nghệ sĩ mới (chưa nằm trong bộ dữ liệu), gọi là **new artist names**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_daily_artist_name_mongoDB(Execution_date: str):\n",
    "    with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "        client_operations = mongoDB_operations(client)\n",
    "\n",
    "        \"\"\" Đọc dữ liệu về danh sách tên nghệ sĩ cũ từ MongoDB\"\"\"\n",
    "        old_artist_name_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_name_collection')    \n",
    "        old_artist_name_data = old_artist_name_data[['Artist']]\n",
    "        old_artist_name_data.rename(columns = {'Artist': 'Old_Artist'}, inplace = True)\n",
    "\n",
    "        \"\"\" Bắt đầu crawl tên nghệ sĩ mới \"\"\"\n",
    "        new_artist_name_data = crawl_artist_name(Execution_date)\n",
    "\n",
    "        \"\"\" Thực hiện Left Anti Join để tìm ra các nghệ sĩ có ở new artist names nhưng k có trong \n",
    "            old arist names -> daily artist names \"\"\"\n",
    "        daily_artist_name_data = pd.merge(old_artist_name_data, new_artist_name_data, left_on = 'Old_Artist', right_on = 'Artist', how = 'right')\n",
    "        daily_artist_name_data = daily_artist_name_data[daily_artist_name_data['Old_Artist'].isnull()][['Pos', 'Artist', 'Execution_date']]\n",
    "        daily_artist_name_data = daily_artist_name_data.head(3000)\n",
    "        \n",
    "        \"\"\" Sau đó ta sẽ load daily arist names vào MongoDB \"\"\"\n",
    "        client_operations.insert_data(database_name = 'music_database', collection_name = 'artist_name_collection', data = daily_artist_name_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"./images/leftanti_join_artistname.png\" alt=\"Image\">\n",
    "</p>  \n",
    "\n",
    "- Chiến lược này giúp chúng ta sẽ chỉ lấy các artist chưa có trong bộ dữ liệu để dùng **Spotify API** và lấy thông tin album track -> Hạn chế duplicates, và giảm thiểu số lượng request API. (Sau khi perform left anti join thì số lượng artist names hằng ngày ~3,000).  \n",
    "\n",
    "- Tên nghệ sĩ mới sẽ được lưu vào MongoDB, sau đó lấy từ MongoDB để gọi Spotify API và lưu dữ liệu nghệ sĩ, album, track và tiếp tục lưu vào MongoDB. Ta sẽ thêm cột **execute_date** để theo dõi ngày chạy dữ liệu và lấy dữ liệu tương ứng để gọi API (tránh lấy data của những ngày trước đó để gọi **Spotify API**).  \n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"./images/daily_crawl_data.png\" alt=\"Image\">\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Three-Layer Data Lake Processing  \n",
    "Ta sẽ dùng **HDFS** (**Hadoop Distributed File System**) để lưu các bộ dữ liệu được process và transform, và ta sẽ gọi kho dữ liệu này là **Data Lake**.  \n",
    "\n",
    "Cấu trúc data lake processing system của chúng ta sẽ thiết kế với 3 lớp chính: **Bronze**, **Silver** and **Gold**. Mỗi lớp đóng vai trò quan trọng khác nhau để lưu và xử lý dữ liệu theo từng cấp cho các bước analysis, reporting và xây dựng mô hình Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Bronze Layer Processing\n",
    "Ở giai đoạn này, dữ liệu được sẽ được lấy từ **MongoDB** sau khi gọi **Spotify API** để thu thập thông tin về **artists**, **albums**, **tracks**, và **track features**. Sau đó, ta sẽ apply các schemas được định nghĩa trước vào các bộ dữ liệu để đảm bảo đúng kiểu dữ liệu của từng cột."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các schema được định nghĩa (Pyspark Schema) sẽ có dạng như sau: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',     StringType(), True),\n",
    "                    StructField('Artist_Name',    StringType(), True),\n",
    "                    StructField('Genres',         ArrayType(StringType(), True), True),\n",
    "                    StructField('Followers',      IntegerType(), True),\n",
    "                    StructField('Popularity',     IntegerType(), True),\n",
    "                    StructField('Artist_Image',   StringType(), True),\n",
    "                    StructField('Artist_Type',    StringType(), True),\n",
    "                    StructField('External_Url',   StringType(), True),\n",
    "                    StructField('Href',           StringType(), True),\n",
    "                    StructField('Artist_Uri',     StringType(), True),\n",
    "                    StructField('Execution_date', DateType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương tự cho các bảng **Album**, **Track** và **Track Feature**.  \n",
    "\n",
    "Bên cạnh đó, ở quá trình này, ta cũng sẽ áp dụng chiến lược **incremental load** để nạp dữ liệu theo từng ngày dựa theo cột **Execution_date** -> Giảm thiểu lượng data cần xử lý, cần transform và load.  \n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"./images/incremental_load.png\" alt=\"Image\">\n",
    "</p>  \n",
    "\n",
    "Ta có thể thấy chỉ những dữ liệu được crawl từ một ngày cụ thể mới được đọc - xử lý - lưu vào **Data Lake** (Không đọc - xử lý - lưu các dữ liệu crawl ở những ngày trước)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là hàm đọc dữ liệu từ **MongoDB** (sau khi scrape), xử lý đơn giản, áp dụng schema, lọc dữ liệu theo chiến lược **incremental load** bằng tham số đầu vào **Execution_date** và load vào **Data Lake** - **Bronze Layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_layer_processing(Execution_date: str):\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task_spark') as spark:\n",
    "        \"\"\"------------------------ BRONZE ARTIST ------------------------\"\"\"\n",
    "        \"\"\" Đọc dữ liệu từ MongoDB \"\"\"\n",
    "        artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "\n",
    "        \"\"\" Lọc ra các dữ liệu đã crawl ở ngày hiện tại \"\"\"\n",
    "        artist_data = artist_data.filter(artist_data['Execution_date'] == Execution_date)\n",
    "\n",
    "        \"\"\" Bắt đầu xử lý \"\"\"\n",
    "        print(\"Starting bronze preprocessing for artist data...\")\n",
    "        #preprocessing before loading data\n",
    "        try:\n",
    "            artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                     .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "                                     .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                     .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "                                     .withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "            #reorder columns after reading \n",
    "            artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "                                            'Followers', 'Popularity', 'Artist_Image', \n",
    "                                            'Artist_Type', 'External_Url', 'Href', 'Artist_Uri', 'Execution_date')\n",
    "            #applying schema        \n",
    "            artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "\n",
    "            print(\"Finished bronze preprocessing for artist data.\")\n",
    "\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = artist_data, direct = 'bronze_data/bronze_artist', \n",
    "                       file_type = 'parquet', mode = \"append\", partition = 'Execution_date')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Silver Layer Processing  \n",
    "Ở giai đoạn này, ta sẽ đọc dữ liệu từ **Bronze Layer Data Storage**(dữ liệu được xử lý đơn giản và áp dụng schema để định chuẩn kiểu dữ liệu các cột), sau đó thực hiện các quá trình xử lý dữ liệu, bao gồm:  \n",
    "- **Drop columns**: drop các cột không cần thiết.  \n",
    "- **Drop null columns**: drop các dòng chứa giá trị null dựa vào subset column được chọn.  \n",
    "- **Fill null**: thay thế giá trị null bằng các giá trị cụ thể.  \n",
    "- **Drop duplicate**: drop các dòng bị trùng lắp dữ liệu dựa vào subset column được chọn.  \n",
    "- **Handle nested**: xử lý các dòng có cấu trúc nested data.  \n",
    "- **Rename column**: Đổi tên cột cần thiết.  \n",
    "\n",
    "Để dễ dàng quản lý và tránh các thao tác xử lý dữ liệu lặp đi lặp lại cho từng bảng, ta sẽ tạo 1 **SilverLayer Class**, đây là lớp sẽ thực hiện các quá trình xử lý dữ liệu nêu trên cho từng bảng dữ liệu.  \n",
    "\n",
    "Với mỗi bảng cần xử lý, ta chỉ cần áp dụng lớp này và truyền các tham số cần thiết như bảng dữ liệu, danh sách cột cần xóa, cần rename, subset column để drop null,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create SilverLayer class to process data in the Silver layer. \"\"\"\n",
    "class SilverLayer:\n",
    "    #init \n",
    "    def __init__(self, data: pyspark.sql.DataFrame, \n",
    "                 drop_columns: list = None, \n",
    "                 drop_null_columns: list = None,\n",
    "                 fill_nulls_columns: dict = None,\n",
    "                 duplicate_columns: list = None,\n",
    "                 nested_columns: list = None,\n",
    "                 rename_columns: dict = None,\n",
    "                 ):\n",
    "        #check valid params\n",
    "        if data is not None and not isinstance(data, pyspark.sql.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        if drop_columns is not None and not isinstance(drop_columns, list):\n",
    "            raise TypeError(\"drop_columns must be a list!\")\n",
    "        if drop_null_columns is not None and not isinstance(drop_null_columns, list):\n",
    "            raise TypeError(\"drop_null_columns must be a list!\")\n",
    "        if fill_nulls_columns is not None and not isinstance(fill_nulls_columns, dict):\n",
    "            raise TypeError(\"handle_nulls must be a dict!\")\n",
    "        if duplicate_columns is not None and not isinstance(duplicate_columns, list):\n",
    "            raise TypeError(\"duplicate_columns must be a list!\")\n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"handle_nested must be a list!\")\n",
    "        if rename_columns is not None and not isinstance(rename_columns, dict):\n",
    "            raise TypeError(\"rename_columns must be a dict!\")\n",
    "        \"\"\"Initialize class attributes for data processing.\"\"\"\n",
    "        self._data = data\n",
    "        self._drop_columns = drop_columns\n",
    "        self._drop_null_columns = drop_null_columns\n",
    "        self._fill_nulls_columns = fill_nulls_columns\n",
    "        self._duplicate_columns = duplicate_columns\n",
    "        self._nested_columns = nested_columns\n",
    "        self._rename_columns = rename_columns\n",
    "\n",
    "\n",
    "    \"\"\" Method to drop unnecessary columns. \"\"\"\n",
    "    def drop(self):\n",
    "        self._data = self._data.drop(*self._drop_columns)\n",
    "\n",
    "    \"\"\" Method to drop rows based on null values in each column. \"\"\"\n",
    "    def drop_null(self):\n",
    "        self._data = self._data.dropna(subset = self._drop_null_columns, how = \"all\")\n",
    "\n",
    "    \"\"\" Method to fill null values. \"\"\"\n",
    "    def fill_null(self):\n",
    "        for column_list, value in self._fill_nulls_columns.items():\n",
    "            self._data = self._data.fillna(value = value, subset = column_list)\n",
    "    \"\"\" Method to rename columns. \"\"\"\n",
    "    def rename(self):\n",
    "        for old_name, new_name in self._rename_columns.items():\n",
    "            self._data = self._data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    \"\"\" Method to handle duplicates. \"\"\"\n",
    "    def handle_duplicate(self):\n",
    "        self._data = self._data.dropDuplicates(self._duplicate_columns)\n",
    "\n",
    "    \"\"\" Method to handle nested. \"\"\"\n",
    "    def handle_nested(self):\n",
    "        for column in self._nested_columns:\n",
    "            self._data = self._data.withColumn(column, explode_outer(column)) \\\n",
    "                                   .withColumn(column, ltrim(column))\n",
    "    \n",
    "    \"\"\" Main processing. \"\"\"\n",
    "    def process(self) -> pyspark.sql.DataFrame:\n",
    "        #drop unnecessary columns\n",
    "        if self._drop_columns:\n",
    "            self.drop() \n",
    "\n",
    "        #drop rows contain null values for each col\n",
    "        if self._drop_null_columns:\n",
    "            self.drop_null()\n",
    "\n",
    "        #fill null values\n",
    "        if self._fill_nulls_columns:\n",
    "            self.fill_null()\n",
    "        \n",
    "        #handle duplicate rows\n",
    "        if self._duplicate_columns:\n",
    "            self.handle_duplicate()\n",
    "\n",
    "        #handle nested columns \n",
    "        if self._nested_columns:\n",
    "            self.handle_nested()\n",
    "\n",
    "        #rename columns\n",
    "        if self._rename_columns:\n",
    "            self.rename()\n",
    "\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi có được lớp **SilverLayer Class**, ta sẽ tiến hành đọc dữ liệu từ lớp **Bronze Layer**, áp dụng lớp này để xử lý dữ liệu, và nạp dữ liệu vào **Silver Layer Storage**.  \n",
    "\n",
    "Đây là hàm xử lý dữ liệu ở lớp Silver Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Processing silver artist data. \"\"\"\n",
    "def silver_artist_process(spark: SparkSession):\n",
    "    \"\"\" Đọc dữ liệu từ Bronze Layer Storage \"\"\"\n",
    "    bronze_artist = read_HDFS(spark, HDFS_dir = \"bronze_data/bronze_artist\", file_type = 'parquet')\n",
    "\n",
    "    \"\"\" Sử dụng lớp SilverLayer \"\"\"\n",
    "    silver_artist = SilverLayer(data = bronze_artist, \n",
    "                                drop_columns       = ['Artist_Type', 'Href', 'Artist_Uri', 'Execution_date'],\n",
    "                                drop_null_columns  = ['Artist_ID'], \n",
    "                                fill_nulls_columns = {'Followers': 0,\n",
    "                                                      'Popularity': 0},\n",
    "                                duplicate_columns  = ['Artist_ID'],\n",
    "                                nested_columns     = ['Genres'],\n",
    "                                rename_columns     = {'Artist_ID': 'id',\n",
    "                                                      'Artist_Name': 'name',\n",
    "                                                      'Genres': 'genres',\n",
    "                                                      'Followers': 'followers',\n",
    "                                                      'Popularity': 'popularity',\n",
    "                                                      'Artist_Image': 'link_image',\n",
    "                                                      'External_Url': 'url'})\n",
    "    \n",
    "    \"\"\" Gọi phương thức process của SilverLayer class để tiến hành xử lý dữ liệu \"\"\"\n",
    "    print(\"Processing for 'silver_artist' ...\")\n",
    "    silver_artist = silver_artist.process()\n",
    "    print(\"Finished processing for 'silver_artist'.\")\n",
    "    \"\"\" Upload data (silver data) vào Silver Layer Storage \"\"\"\n",
    "    write_HDFS(spark, data = silver_artist, direct = \"silver_data/silver_artist\", file_type = 'parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xử lý tương tự cho **bronze_album**, **bronze_track** và **bronze_track_feature**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Gold Layer Processing  \n",
    "Ở bước này, sau khi bộ dữ liệu được xử lý ở giai đoạn **Silver Layer** stage, ta sẽ thực hiện quá trình kết hợp các bảng để tạo ra một lược đồ tuân theo cấu trúc **Snowflake schema**, lược đồ này sẽ được áp dụng để tổ chức dữ liệu trong **Data Warehouse**.  \n",
    "\n",
    "Đây là lược đồ mà chúng ta mong muốn:  \n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"./images/schema.jpg\" alt=\"Image\">\n",
    "</p>  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
